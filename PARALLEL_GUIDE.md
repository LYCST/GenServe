# GenServe å¹¶è¡Œå¤„ç†æŒ‡å—

## ğŸ‰ æ­å–œï¼æ‚¨çš„å¹¶è¡Œç³»ç»Ÿå·²ç»æˆåŠŸè¿è¡Œ

æ ¹æ®æ‚¨çš„æµ‹è¯•ç»“æœï¼Œç³»ç»Ÿå·²ç»å®ç°äº†çœŸæ­£çš„å¹¶è¡Œå¤„ç†ï¼š

### âœ… æˆåŠŸæŒ‡æ ‡
- **å¹¶å‘åº¦**: 10ä¸ªè¯·æ±‚åŒæ—¶å¤„ç†
- **æˆåŠŸç‡**: 100%
- **GPUåˆ©ç”¨ç‡**: 4ä¸ªGPUåŒæ—¶å·¥ä½œ
- **è´Ÿè½½å‡è¡¡**: 0.67åˆ†ï¼ˆè‰¯å¥½æ°´å¹³ï¼‰

## ğŸš€ å¦‚ä½•ä½¿ç”¨å¹¶è¡Œç³»ç»Ÿ

### 1. å¯åŠ¨æœåŠ¡
```bash
# ä½¿ç”¨ä¼˜åŒ–åçš„å¯åŠ¨è„šæœ¬
chmod +x start_optimized.sh
./start_optimized.sh

# æˆ–ä½¿ç”¨åŸå§‹å¯åŠ¨è„šæœ¬
./start.sh
```

### 2. å‘é€å¹¶å‘è¯·æ±‚
```python
import asyncio
import aiohttp
import json

async def send_concurrent_requests():
    async with aiohttp.ClientSession() as session:
        tasks = []
        
        # åˆ›å»ºå¤šä¸ªå¹¶å‘è¯·æ±‚
        for i in range(10):
            data = {
                "prompt": f"Beautiful landscape {i}",
                "model_id": "flux1-dev",
                "height": 512,
                "width": 512,
                "num_inference_steps": 20,
                "priority": i % 3  # ä¸åŒä¼˜å…ˆçº§
            }
            
            task = session.post(
                "http://localhost:12411/generate",
                json=data
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        responses = await asyncio.gather(*tasks)
        
        for i, response in enumerate(responses):
            result = await response.json()
            print(f"è¯·æ±‚ {i+1}: GPU {result.get('gpu_id')}, è€—æ—¶ {result.get('elapsed_time')}s")

# è¿è¡Œ
asyncio.run(send_concurrent_requests())
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–å·¥å…·

### 1. æ€§èƒ½æµ‹è¯•è„šæœ¬
```bash
python optimize_parallel_performance.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- æµ‹è¯•ä¸åŒè§„æ¨¡çš„å¹¶å‘è¯·æ±‚
- åˆ†æGPUä½¿ç”¨æƒ…å†µ
- è¯„ä¼°è´Ÿè½½å‡è¡¡æ•ˆæœ
- æä¾›ä¼˜åŒ–å»ºè®®

### 2. é…ç½®ä¼˜åŒ–è„šæœ¬
```bash
python optimize_config.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- åˆ†ææµ‹è¯•ç»“æœ
- è‡ªåŠ¨è°ƒæ•´é…ç½®å‚æ•°
- ç”Ÿæˆä¼˜åŒ–çš„å¯åŠ¨è„šæœ¬

## ğŸ“Š ç›‘æ§å’Œè°ƒè¯•

### 1. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
```bash
curl http://localhost:12411/status
```

### 2. æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
```bash
curl http://localhost:12411/models
```

### 3. å¥åº·æ£€æŸ¥
```bash
curl http://localhost:12411/health
```

## âš™ï¸ å…³é”®é…ç½®å‚æ•°

### å¹¶å‘é…ç½®
```python
# è°ƒåº¦å™¨ç¡çœ æ—¶é—´ï¼ˆç§’ï¼‰
SCHEDULER_SLEEP_TIME = 0.05  # æ›´å¿«çš„å“åº”

# å…¨å±€é˜Ÿåˆ—å¤§å°
MAX_GLOBAL_QUEUE_SIZE = 150  # æ›´å¤šç¼“å†²

# æ¯ä¸ªGPUé˜Ÿåˆ—å¤§å°
MAX_GPU_QUEUE_SIZE = 3  # é¿å…å•ä¸ªGPUè¿‡è½½

# ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TASK_TIMEOUT = 240  # æ›´é•¿çš„è¶…æ—¶
```

### æ€§èƒ½ä¼˜åŒ–
```python
# GPUå†…å­˜ç®¡ç†
PYTORCH_CUDA_ALLOC_CONF = "expandable_segments:True,max_split_size_mb:64"

# å¯ç”¨ä¼˜åŒ–
ENABLE_OPTIMIZATION = "true"
MEMORY_EFFICIENT_ATTENTION = "true"
ENABLE_CPU_OFFLOAD = "true"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è¯·æ±‚ä¼˜åŒ–
- ä½¿ç”¨åˆé€‚çš„å›¾ç‰‡å°ºå¯¸ï¼ˆ512x512ç”¨äºæµ‹è¯•ï¼‰
- å‡å°‘æ¨ç†æ­¥æ•°ï¼ˆ20æ­¥ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
- è®¾ç½®åˆç†çš„ä¼˜å…ˆçº§

### 2. è´Ÿè½½å‡è¡¡
- ç³»ç»Ÿè‡ªåŠ¨ä½¿ç”¨è½®è¯¢è´Ÿè½½å‡è¡¡
- ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
- é¿å…å•ä¸ªGPUè¿‡è½½

### 3. é”™è¯¯å¤„ç†
- è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
- å¤„ç†é˜Ÿåˆ—æ»¡çš„æƒ…å†µ
- ç›‘æ§è¿›ç¨‹çŠ¶æ€

## ğŸ” æ•…éšœæ’é™¤

### 1. æœåŠ¡æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep 12411

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/genserve.log
```

### 2. è¯·æ±‚å¤±è´¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:12411/status

# æ£€æŸ¥é˜Ÿåˆ—å¤§å°
curl http://localhost:12411/status | jq '.concurrent_manager.global_queue_size'
```

### 3. GPUè¿›ç¨‹æ­»äº¡
```bash
# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep gpu-worker

# é‡å¯æœåŠ¡
pkill -f "python main.py"
./start_optimized.sh
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜å»ºè®®

### åŸºäºæ‚¨çš„æµ‹è¯•ç»“æœ

1. **å“åº”æ—¶é—´ä¼˜åŒ–**
   - å½“å‰å¹³å‡å“åº”æ—¶é—´ï¼š49.75ç§’
   - å»ºè®®ï¼šå‡å°‘æ¨ç†æ­¥æ•°æˆ–ä½¿ç”¨æ›´å¿«çš„GPU

2. **è´Ÿè½½å‡è¡¡ä¼˜åŒ–**
   - å½“å‰å‡è¡¡åº¦ï¼š0.67
   - å»ºè®®ï¼šæ£€æŸ¥GPUæ€§èƒ½å·®å¼‚ï¼Œè°ƒæ•´åˆ†é…ç­–ç•¥

3. **å¹¶å‘åº¦ä¼˜åŒ–**
   - å½“å‰å¹¶å‘æ•ˆç‡ï¼š0.8
   - å»ºè®®ï¼šå¢åŠ GPUæ•°é‡æˆ–ä¼˜åŒ–é˜Ÿåˆ—ç®¡ç†

## ğŸ® é«˜çº§åŠŸèƒ½

### 1. ä¼˜å…ˆçº§é˜Ÿåˆ—
```python
# é«˜ä¼˜å…ˆçº§è¯·æ±‚
data = {
    "prompt": "urgent request",
    "priority": 0  # æœ€é«˜ä¼˜å…ˆçº§
}

# æ™®é€šè¯·æ±‚
data = {
    "prompt": "normal request", 
    "priority": 1  # æ™®é€šä¼˜å…ˆçº§
}
```

### 2. ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢
```python
# æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
task_id = "your-task-id"
response = requests.get(f"http://localhost:12411/task/{task_id}")
```

### 3. æ‰¹é‡å¤„ç†
```python
# æ‰¹é‡å‘é€è¯·æ±‚
async def batch_process(prompts):
    tasks = []
    for prompt in prompts:
        task = send_request(prompt)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

## ğŸ‰ æ€»ç»“

æ‚¨çš„å¹¶è¡Œç³»ç»Ÿå·²ç»æˆåŠŸè¿è¡Œï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

âœ… **çœŸæ­£çš„å¹¶è¡Œå¤„ç†** - å¤šä¸ªGPUåŒæ—¶å·¥ä½œ  
âœ… **æ™ºèƒ½è´Ÿè½½å‡è¡¡** - è‡ªåŠ¨åˆ†é…ä»»åŠ¡åˆ°å¯ç”¨GPU  
âœ… **ä¼˜å…ˆçº§æ”¯æŒ** - æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§è°ƒåº¦  
âœ… **å®¹é”™æœºåˆ¶** - è‡ªåŠ¨é‡å¯æ­»äº¡è¿›ç¨‹  
âœ… **æ€§èƒ½ç›‘æ§** - è¯¦ç»†çš„ç»Ÿè®¡å’Œç›‘æ§  

ç»§ç»­ä½¿ç”¨è¿™äº›å·¥å…·å’Œæœ€ä½³å®è·µï¼Œæ‚¨çš„å¹¶è¡Œå›¾ç‰‡ç”ŸæˆæœåŠ¡å°†èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§é‡å¹¶å‘è¯·æ±‚ï¼ 
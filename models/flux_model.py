import base64
import io
import time
import torch
from typing import Dict, Any, Optional
from PIL import Image
from diffusers import FluxPipeline
from .base import BaseModel
from config import Config
import logging
import os

logger = logging.getLogger(__name__)

class FluxModel(BaseModel):
    """Fluxæ¨¡å‹å®ç°"""
    
    def __init__(self, gpu_device: Optional[str] = None):
        super().__init__(
            model_id="flux1-dev",
            model_name="FLUX.1-dev",
            description="Black Forest Labs FLUX.1-dev model for high-quality image generation",
            gpu_device=gpu_device
        )
        self.model_path = Config.get_model_path("flux1-dev")
    
    def load(self) -> bool:
        """åŠ è½½Fluxæ¨¡å‹"""
        try:
            # é€‰æ‹©æœ€ä½³GPUè¿›è¡ŒåŠ è½½
            best_gpu = self._select_best_gpu()
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name} åˆ°è®¾å¤‡: {best_gpu}")
            logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False
            
            # å°è¯•å¤šç§åŠ è½½æ–¹å¼
            load_success = False
            
            # æ–¹æ³•1ï¼šä½¿ç”¨FluxPipeline with CPU offload
            try:
                logger.info("å°è¯•ä½¿ç”¨FluxPipelineåŠ è½½æ¨¡å‹...")
                self.pipe = FluxPipeline.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True,
                    local_files_only=True
                )
                # å¯ç”¨CPUå¸è½½ä»¥èŠ‚çœGPUå†…å­˜ - ä¸æŒ‡å®šgpu_idè®©å…¶è‡ªåŠ¨é€‰æ‹©
                self.pipe.enable_model_cpu_offload()
                load_success = True
                logger.info("FluxPipeline with CPU offloadåŠ è½½æˆåŠŸ")
                
            except Exception as e:
                logger.warning(f"FluxPipelineåŠ è½½å¤±è´¥: {e}")
                
                # æ–¹æ³•2ï¼šä½¿ç”¨DiffusionPipeline with CPU offload
                try:
                    logger.info("å°è¯•ä½¿ç”¨DiffusionPipelineåŠ è½½æ¨¡å‹...")
                    from diffusers import DiffusionPipeline
                    self.pipe = DiffusionPipeline.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.bfloat16,
                        use_safetensors=True,
                        local_files_only=True
                    )
                    # å¯ç”¨CPUå¸è½½ - ä¸æŒ‡å®šgpu_id
                    self.pipe.enable_model_cpu_offload()
                    load_success = True
                    logger.info("DiffusionPipeline with CPU offloadåŠ è½½æˆåŠŸ")
                    
                except Exception as e2:
                    logger.warning(f"DiffusionPipelineåŠ è½½å¤±è´¥: {e2}")
                    
                    # æ–¹æ³•3ï¼šä½¿ç”¨æ›´å®½æ¾çš„å‚æ•°
                    try:
                        logger.info("å°è¯•ä½¿ç”¨å®½æ¾å‚æ•°åŠ è½½æ¨¡å‹...")
                        from diffusers import DiffusionPipeline
                        self.pipe = DiffusionPipeline.from_pretrained(
                            self.model_path,
                            torch_dtype=torch.float16,  # æ”¹ç”¨float16
                            local_files_only=True,
                            trust_remote_code=True
                        )
                        # å¯ç”¨CPUå¸è½½ - ä¸æŒ‡å®šgpu_id
                        self.pipe.enable_model_cpu_offload()
                        load_success = True
                        logger.info("å®½æ¾å‚æ•°with CPU offloadåŠ è½½æˆåŠŸ")
                        
                    except Exception as e3:
                        logger.error(f"æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†: {e3}")
                        return False
            
            if not load_success:
                return False
            
            # ä¸æ‰‹åŠ¨ç§»åŠ¨åˆ°GPUï¼Œè®©CPU offloadè‡ªåŠ¨ç®¡ç†
            self.gpu_device = best_gpu
            logger.info(f"æ¨¡å‹ {self.model_name} å·²å¯ç”¨CPU offloadï¼Œç›®æ ‡GPU: {best_gpu.upper()}")
            
            self.is_loaded = True
            logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} åŠ è½½å¤±è´¥: {e}")
            self.is_loaded = False
            return False
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾ç‰‡"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        # éªŒè¯æç¤ºè¯
        if not self.validate_prompt(prompt):
            raise ValueError("æç¤ºè¯éªŒè¯å¤±è´¥")
        
        # è·å–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±çš„å‚æ•°
        params = self.get_default_params()
        params.update(kwargs)
        
        # éªŒè¯å‚æ•°
        if not self.validate_params(**params):
            raise ValueError("å‚æ•°éªŒè¯å¤±è´¥")
        
        # ä½¿ç”¨æ¨¡å‹åŠ è½½æ—¶é€‰æ‹©çš„GPUè®¾å¤‡
        device = self.gpu_device
        logger.debug(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡è¿›è¡Œç”Ÿæˆ: {device}")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®éšæœºç§å­ - ä½¿ç”¨CPU generatorå¦‚ç¤ºä¾‹æ‰€ç¤º
            generator = torch.Generator("cpu").manual_seed(params['seed'])
            
            logger.info(f"å¼€å§‹ç”Ÿæˆå›¾ç‰‡ï¼Œæç¤ºè¯: {prompt}ï¼Œè®¾å¤‡: {device}")
            
            with torch.no_grad():
                # ä½¿ç”¨ä¸å·¥ä½œç¤ºä¾‹ç›¸åŒçš„å‚æ•°
                result = self.pipe(
                    prompt=prompt,
                    height=params['height'],
                    width=params['width'],
                    guidance_scale=params['cfg'],
                    num_inference_steps=params['num_inference_steps'],
                    max_sequence_length=512,  # æ·»åŠ è¿™ä¸ªå…³é”®å‚æ•°
                    generator=generator
                )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸ºbase64
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            elapsed_time = time.time() - start_time
            
            # æ¸…ç†GPUå†…å­˜
            if device.startswith("cuda"):
                torch.cuda.empty_cache()
            
            # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜å›¾ç‰‡
            save_to_disk = False
            if params.get('save_disk_path'):
                try:
                    image.save(params['save_disk_path'])
                    save_to_disk = True
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {params['save_disk_path']}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            
            logger.info(f"å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œè®¾å¤‡: {device}")
            
            return {
                "success": True,
                "image": image,
                "base64": img_base64,
                "elapsed_time": elapsed_time,
                "save_to_disk": save_to_disk,
                "params": params,
                "device": device
            }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            # æ¸…ç†GPUå†…å­˜
            if device.startswith("cuda"):
                torch.cuda.empty_cache()
            return {
                "success": False,
                "error": str(e),
                "elapsed_time": time.time() - start_time,
                "device": device
            }
    
    def get_default_params(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            "num_inference_steps": 50,  # Fluxæ¨èä½¿ç”¨50æ­¥
            "seed": 42,
            "cfg": 3.5,  # Fluxæ¨èä½¿ç”¨3.5
            "height": 1024,  # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "width": 1024,   # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "save_disk_path": None
        }
    
    def validate_params(self, **kwargs) -> bool:
        """éªŒè¯å‚æ•°"""
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        required_params = ['num_inference_steps', 'seed', 'cfg', 'height', 'width']
        for param in required_params:
            if param not in kwargs:
                return False
        
        # éªŒè¯å‚æ•°èŒƒå›´
        if kwargs['num_inference_steps'] < 1 or kwargs['num_inference_steps'] > 100:
            return False
        
        if kwargs['cfg'] < 0.1 or kwargs['cfg'] > 20:
            return False
        
        # éªŒè¯å›¾ç‰‡å°ºå¯¸
        if not self.validate_image_size(kwargs['height'], kwargs['width']):
            return False
        
        return True
    
    def get_supported_features(self) -> list:
        """è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨"""
        return ["text-to-image"]
    
    def get_optimization_kwargs(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å‚æ•° - Fluxä½¿ç”¨bfloat16"""
        kwargs = {}
        kwargs["torch_dtype"] = torch.bfloat16
        return kwargs 
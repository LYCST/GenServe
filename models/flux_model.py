import base64
import io
import time
import torch
from typing import Dict, Any, Optional
from PIL import Image
from diffusers import FluxPipeline
from .base import BaseModel
from config import Config
import logging
import os
import threading
import gc

logger = logging.getLogger(__name__)

class FluxModel(BaseModel):
    """Fluxæ¨¡å‹å®ç°"""
    
    def __init__(self, gpu_device: Optional[str] = None):
        super().__init__(
            model_id="flux1-dev",
            model_name="FLUX.1-dev",
            description="Black Forest Labs FLUX.1-dev model for high-quality image generation",
            gpu_device=gpu_device
        )
        self.model_path = Config.get_model_path("flux1-dev")
        # æ·»åŠ çº¿ç¨‹é”ä¿æŠ¤
        self._generation_lock = threading.Lock()
        # ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºå”¯ä¸€æ ‡è¯†
        self._instance_id = f"flux_{gpu_device}_{id(self)}"
        logger.info(f"åˆ›å»ºFluxModelå®ä¾‹: {self._instance_id}")

        # è·å–GPU IDç”¨äºç¯å¢ƒå˜é‡è®¾ç½®
        self.gpu_id = self._get_gpu_id_from_device(self.gpu_device)

    def _get_gpu_id_from_device(self, device: str) -> str:
        """ä»è®¾å¤‡åç§°æå–GPU ID"""
        if device == "cpu":
            return "cpu"
        elif device.startswith("cuda:"):
            return device.split(":")[1]
        else:
            return "0"  # é»˜è®¤ä½¿ç”¨GPU 0
    
    def _deep_gpu_cleanup(self, target_device: Optional[str] = None):
        """æ·±åº¦GPUæ˜¾å­˜æ¸…ç† - å¢å¼ºç‰ˆæœ¬"""
        try:
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡è®¾å¤‡ï¼Œåªæ¸…ç†è¯¥è®¾å¤‡
            if target_device and target_device.startswith("cuda:"):
                gpu_id = int(target_device.split(":")[1])
                with torch.cuda.device(gpu_id):
                    torch.cuda.empty_cache()
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.synchronize()
                logger.debug(f"å·²æ¸…ç†æŒ‡å®šGPU {target_device} æ˜¾å­˜")
                return
            
            # æ¸…ç†å½“å‰GPU
            if self.gpu_device.startswith("cuda:"):
                gpu_id = int(self.gpu_device.split(":")[1])
                with torch.cuda.device(gpu_id):
                    # å¼ºåˆ¶åŒæ­¥
                    torch.cuda.synchronize()
                    # æ¸…ç†ç¼“å­˜
                    torch.cuda.empty_cache()
                    # é‡ç½®ç»Ÿè®¡
                    torch.cuda.reset_peak_memory_stats()
                    # å†æ¬¡åŒæ­¥
                    torch.cuda.synchronize()
                logger.debug(f"å·²æ¸…ç†GPU {self.gpu_device} æ˜¾å­˜")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # å†æ¬¡æ¸…ç†æ‰€æœ‰GPUç¼“å­˜
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
            
        except Exception as e:
            logger.warning(f"æ·±åº¦GPUæ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _safe_pipeline_cleanup(self):
        """å®‰å…¨æ¸…ç†pipelineåŠå…¶ç»„ä»¶"""
        try:
            if hasattr(self, 'pipe') and self.pipe is not None:
                logger.info(f"å¼€å§‹æ¸…ç†pipelineç»„ä»¶ (å®ä¾‹: {self._instance_id})")
                
                # è·å–pipelineä¸­çš„æ‰€æœ‰ç»„ä»¶
                components_to_cleanup = []
                
                # æ£€æŸ¥å¹¶æ”¶é›†éœ€è¦æ¸…ç†çš„ç»„ä»¶
                for attr_name in ['transformer', 'vae', 'text_encoder', 'text_encoder_2', 
                                'scheduler', 'tokenizer', 'tokenizer_2']:
                    if hasattr(self.pipe, attr_name):
                        component = getattr(self.pipe, attr_name)
                        if component is not None:
                            components_to_cleanup.append((attr_name, component))
                
                # é€ä¸ªæ¸…ç†ç»„ä»¶
                for name, component in components_to_cleanup:
                    try:
                        # å¦‚æœç»„ä»¶æœ‰å‚æ•°ï¼Œå°è¯•ç§»åŠ¨åˆ°CPU
                        if hasattr(component, 'to') and hasattr(component, 'parameters'):
                            component.to('cpu')
                            logger.debug(f"å·²å°† {name} ç§»åŠ¨åˆ°CPU")
                        
                        # å¦‚æœç»„ä»¶æœ‰cuda()æ–¹æ³•ï¼Œè¯´æ˜å¯èƒ½åœ¨GPUä¸Š
                        if hasattr(component, 'cuda'):
                            try:
                                component.cpu()
                                logger.debug(f"å·²å°† {name} ç§»åŠ¨åˆ°CPU")
                            except:
                                pass
                                
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ç»„ä»¶ {name} æ—¶å‡ºé”™: {e}")
                
                # æ¸…ç†pipelineæœ¬èº«
                try:
                    # ç¦ç”¨CPU offload
                    if hasattr(self.pipe, 'disable_model_cpu_offload'):
                        self.pipe.disable_model_cpu_offload()
                        logger.debug("å·²ç¦ç”¨æ¨¡å‹CPU offload")
                except Exception as e:
                    logger.warning(f"ç¦ç”¨CPU offloadæ—¶å‡ºé”™: {e}")
                
                # å°è¯•å°†æ•´ä¸ªpipelineç§»åŠ¨åˆ°CPU
                try:
                    self.pipe.to('cpu')
                    logger.debug("å·²å°†pipelineç§»åŠ¨åˆ°CPU")
                except Exception as e:
                    logger.warning(f"ç§»åŠ¨pipelineåˆ°CPUæ—¶å‡ºé”™: {e}")
                
                # åˆ é™¤pipelineå¼•ç”¨
                del self.pipe
                self.pipe = None
                logger.info(f"Pipelineå·²åˆ é™¤ (å®ä¾‹: {self._instance_id})")
                
        except Exception as e:
            logger.error(f"æ¸…ç†pipelineæ—¶å‡ºé”™: {e}")
    
    def load(self) -> bool:
        """åŠ è½½Fluxæ¨¡å‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False
            
            # åªæœ‰åœ¨ä½¿ç”¨GPUæ—¶æ‰è®¾ç½®ç¯å¢ƒå˜é‡
            if self.gpu_id != "cpu":
                logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name} åˆ°è®¾å¤‡: {self.gpu_device} (å®ä¾‹: {self._instance_id})")
                logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
            
            try:
                # ä½¿ç”¨FluxPipeline with CPU offload
                logger.info("å°è¯•ä½¿ç”¨FluxPipelineåŠ è½½æ¨¡å‹...")
                self.pipe = FluxPipeline.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True,
                    local_files_only=True
                )
                
                # æ ¹æ®è®¾å¤‡ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                if self.gpu_device.startswith("cuda:"):
                    # å¯¹äºGPUè®¾å¤‡ï¼Œä½¿ç”¨CPU offloadå¹¶æŒ‡å®šè®¾å¤‡
                    self.pipe.enable_model_cpu_offload(device=self.gpu_device)
                    logger.info(f"FluxPipeline with CPU offloadåŠ è½½æˆåŠŸï¼Œç›®æ ‡è®¾å¤‡: {self.gpu_device} (å®ä¾‹: {self._instance_id})")
                else:
                    # å¯¹äºCPUè®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨CPU
                    self.pipe = self.pipe.to("cpu")
                    logger.info(f"FluxPipelineåŠ è½½åˆ°CPUæˆåŠŸ (å®ä¾‹: {self._instance_id})")
                
                self.is_loaded = True
                logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆ (å®ä¾‹: {self._instance_id})")
                return True
                
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False
                
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} åŠ è½½å¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            self.is_loaded = False
            return False
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾ç‰‡ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªç”Ÿæˆä»»åŠ¡
        with self._generation_lock:
            return self._generate_internal(prompt, **kwargs)
    
    def _generate_internal(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """å†…éƒ¨ç”Ÿæˆæ–¹æ³•"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        # éªŒè¯æç¤ºè¯
        if not self.validate_prompt(prompt):
            raise ValueError("æç¤ºè¯éªŒè¯å¤±è´¥")
        
        # è·å–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±çš„å‚æ•°
        params = self.get_default_params()
        params.update(kwargs)
        
        # éªŒè¯å‚æ•°
        if not self.validate_params(**params):
            raise ValueError("å‚æ•°éªŒè¯å¤±è´¥")
        
        # ä½¿ç”¨æ¨¡å‹åŠ è½½æ—¶é€‰æ‹©çš„GPUè®¾å¤‡
        device = self.gpu_device
        logger.debug(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡è¿›è¡Œç”Ÿæˆ: {device} (å®ä¾‹: {self._instance_id})")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®éšæœºç§å­ - ä½¿ç”¨CPU generatorå¦‚ç¤ºä¾‹æ‰€ç¤º
            generator = torch.Generator("cpu").manual_seed(params['seed'])
            
            logger.info(f"å¼€å§‹ç”Ÿæˆå›¾ç‰‡ï¼Œæç¤ºè¯: {prompt}ï¼Œè®¾å¤‡: {device} (å®ä¾‹: {self._instance_id})")
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if device.startswith("cuda:"):
                # å¯¹äºGPUè®¾å¤‡ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„CUDAè®¾å¤‡
                torch.cuda.set_device(device)
                logger.debug(f"è®¾ç½®CUDAè®¾å¤‡ä¸º: {device}")
            
            with torch.no_grad():
                # ä½¿ç”¨ä¸å·¥ä½œç¤ºä¾‹ç›¸åŒçš„å‚æ•°
                result = self.pipe(
                    prompt=prompt,
                    height=params['height'],
                    width=params['width'],
                    guidance_scale=params['cfg'],
                    num_inference_steps=params['num_inference_steps'],
                    max_sequence_length=512,  # æ·»åŠ è¿™ä¸ªå…³é”®å‚æ•°
                    generator=generator
                )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸ºbase64
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            elapsed_time = time.time() - start_time
            
            # æˆåŠŸåçš„è½»é‡çº§æ¸…ç†
            self._deep_gpu_cleanup()
            
            # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜å›¾ç‰‡
            save_to_disk = False
            if params.get('save_disk_path'):
                try:
                    image.save(params['save_disk_path'])
                    save_to_disk = True
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {params['save_disk_path']}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            
            logger.info(f"å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œè®¾å¤‡: {device} (å®ä¾‹: {self._instance_id})")
            
            return {
                "success": True,
                "image": image,
                "base64": img_base64,
                "elapsed_time": elapsed_time,
                "save_to_disk": save_to_disk,
                "params": params,
                "device": device,
                "instance_id": self._instance_id
            }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            
            # å¤±è´¥æ—¶çš„å½»åº•æ¸…ç†
            self._emergency_cleanup()
            
            return {
                "success": False,
                "error": str(e),
                "elapsed_time": time.time() - start_time,
                "device": device,
                "instance_id": self._instance_id
            }
    
    def _emergency_cleanup(self):
        """ç´§æ€¥æ¸…ç† - åœ¨ç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨"""
        logger.warning(f"æ‰§è¡Œç´§æ€¥æ¸…ç† (å®ä¾‹: {self._instance_id})")
        
        try:
            # 1. å…ˆå°è¯•æ·±åº¦GPUæ¸…ç†
            self._deep_gpu_cleanup()
            
            # 2. æ¸…ç†pipelineç»„ä»¶
            self._safe_pipeline_cleanup()
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # 4. å†æ¬¡æ¸…ç†GPU
            self._deep_gpu_cleanup()
            
            # 5. é‡æ–°åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.is_loaded:
                logger.info(f"å°è¯•é‡æ–°åŠ è½½æ¨¡å‹ (å®ä¾‹: {self._instance_id})")
                self.load()
            
        except Exception as e:
            logger.error(f"ç´§æ€¥æ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def unload(self):
        """å¸è½½æ¨¡å‹ - å¢å¼ºç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹å¸è½½æ¨¡å‹ (å®ä¾‹: {self._instance_id})")
        
        # æ ‡è®°ä¸ºæœªåŠ è½½
        self.is_loaded = False
        
        # å®‰å…¨æ¸…ç†pipeline
        self._safe_pipeline_cleanup()
        
        # æ·±åº¦æ¸…ç†GPU
        self._deep_gpu_cleanup()
        
        logger.info(f"æ¨¡å‹ {self.model_id} å·²å®Œå…¨å¸è½½ (å®ä¾‹: {self._instance_id})")
    
    def get_default_params(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            "num_inference_steps": 50,  # Fluxæ¨èä½¿ç”¨50æ­¥
            "seed": 42,
            "cfg": 3.5,  # Fluxæ¨èä½¿ç”¨3.5
            "height": 1024,  # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "width": 1024,   # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "save_disk_path": None
        }
    
    def validate_params(self, **kwargs) -> bool:
        """éªŒè¯å‚æ•°"""
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        required_params = ['num_inference_steps', 'seed', 'cfg', 'height', 'width']
        for param in required_params:
            if param not in kwargs:
                return False
        
        # éªŒè¯å‚æ•°èŒƒå›´
        if kwargs['num_inference_steps'] < 1 or kwargs['num_inference_steps'] > 100:
            return False
        
        if kwargs['cfg'] < 0.1 or kwargs['cfg'] > 20:
            return False
        
        # éªŒè¯å›¾ç‰‡å°ºå¯¸
        if not self.validate_image_size(kwargs['height'], kwargs['width']):
            return False
        
        return True
    
    def get_supported_features(self) -> list:
        """è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨"""
        return ["text-to-image"]
    
    def get_optimization_kwargs(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å‚æ•° - Fluxä½¿ç”¨bfloat16"""
        kwargs = {}
        kwargs["torch_dtype"] = torch.bfloat16
        return kwargs 
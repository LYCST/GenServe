import base64
import io
import time
import torch
from typing import Dict, Any, Optional
from PIL import Image
from diffusers import FluxPipeline
from .base import BaseModel
from config import Config
import logging
import os
import threading
import gc

logger = logging.getLogger(__name__)

class FluxModel(BaseModel):
    """Fluxæ¨¡å‹å®ç°"""
    
    def __init__(self, gpu_device: Optional[str] = None):
        super().__init__(
            model_id="flux1-dev",
            model_name="FLUX.1-dev",
            description="Black Forest Labs FLUX.1-dev model for high-quality image generation",
            gpu_device=gpu_device
        )
        self.model_path = Config.get_model_path("flux1-dev")
        # æ·»åŠ çº¿ç¨‹é”ä¿æŠ¤
        self._generation_lock = threading.Lock()
        # ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºå”¯ä¸€æ ‡è¯†
        self._instance_id = f"flux_{gpu_device}_{id(self)}"
        logger.info(f"åˆ›å»ºFluxModelå®ä¾‹: {self._instance_id}")

        # è·å–GPU IDç”¨äºç¯å¢ƒå˜é‡è®¾ç½®
        self.gpu_id = self._get_gpu_id_from_device(self.gpu_device)

    def _get_gpu_id_from_device(self, device: str) -> str:
        """ä»è®¾å¤‡åç§°æå–GPU ID"""
        if device == "cpu":
            return "cpu"
        elif device.startswith("cuda:"):
            return device.split(":")[1]
        else:
            return "0"  # é»˜è®¤ä½¿ç”¨GPU 0
    
    def load(self) -> bool:
        """åŠ è½½Fluxæ¨¡å‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False
            
            # å°è¯•å¤šç§åŠ è½½æ–¹å¼
            load_success = False

            # åªæœ‰åœ¨ä½¿ç”¨GPUæ—¶æ‰è®¾ç½®ç¯å¢ƒå˜é‡
            if self.gpu_id != "cpu":
                logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name} åˆ°è®¾å¤‡: {self.gpu_device} (å®ä¾‹: {self._instance_id})")
                logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
            
            try:
                # ä½¿ç”¨FluxPipeline with CPU offload
                logger.info("å°è¯•ä½¿ç”¨FluxPipelineåŠ è½½æ¨¡å‹...")
                self.pipe = FluxPipeline.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True,
                    local_files_only=True
                )
                
                # æ ¹æ®è®¾å¤‡ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                if self.gpu_device.startswith("cuda:"):
                    # å¯¹äºGPUè®¾å¤‡ï¼Œä½¿ç”¨CPU offloadå¹¶æŒ‡å®šè®¾å¤‡
                    self.pipe.enable_model_cpu_offload(device=self.gpu_device)
                    logger.info(f"FluxPipeline with CPU offloadåŠ è½½æˆåŠŸï¼Œç›®æ ‡è®¾å¤‡: {self.gpu_device} (å®ä¾‹: {self._instance_id})")
                else:
                    # å¯¹äºCPUè®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨CPU
                    self.pipe = self.pipe.to("cpu")
                    logger.info(f"FluxPipelineåŠ è½½åˆ°CPUæˆåŠŸ (å®ä¾‹: {self._instance_id})")
                
                load_success = True
                
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False
                
            if not load_success:
                return False
            
            logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.gpu_device} (å®ä¾‹: {self._instance_id})")
            
            self.is_loaded = True
            logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆ (å®ä¾‹: {self._instance_id})")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} åŠ è½½å¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            self.is_loaded = False
            return False
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾ç‰‡ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªç”Ÿæˆä»»åŠ¡
        with self._generation_lock:
            return self._generate_internal(prompt, **kwargs)
    
    def _generate_internal(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """å†…éƒ¨ç”Ÿæˆæ–¹æ³•"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        # éªŒè¯æç¤ºè¯
        if not self.validate_prompt(prompt):
            raise ValueError("æç¤ºè¯éªŒè¯å¤±è´¥")
        
        # è·å–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±çš„å‚æ•°
        params = self.get_default_params()
        params.update(kwargs)
        
        # éªŒè¯å‚æ•°
        if not self.validate_params(**params):
            raise ValueError("å‚æ•°éªŒè¯å¤±è´¥")
        
        # ä½¿ç”¨æ¨¡å‹åŠ è½½æ—¶é€‰æ‹©çš„GPUè®¾å¤‡
        device = self.gpu_device
        logger.debug(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡è¿›è¡Œç”Ÿæˆ: {device} (å®ä¾‹: {self._instance_id})")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®éšæœºç§å­ - ä½¿ç”¨CPU generatorå¦‚ç¤ºä¾‹æ‰€ç¤º
            generator = torch.Generator("cpu").manual_seed(params['seed'])
            
            logger.info(f"å¼€å§‹ç”Ÿæˆå›¾ç‰‡ï¼Œæç¤ºè¯: {prompt}ï¼Œè®¾å¤‡: {device} (å®ä¾‹: {self._instance_id})")
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if device.startswith("cuda:"):
                # å¯¹äºGPUè®¾å¤‡ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„CUDAè®¾å¤‡
                torch.cuda.set_device(device)
                logger.debug(f"è®¾ç½®CUDAè®¾å¤‡ä¸º: {device}")
            
            with torch.no_grad():
                # ä½¿ç”¨ä¸å·¥ä½œç¤ºä¾‹ç›¸åŒçš„å‚æ•°
                result = self.pipe(
                    prompt=prompt,
                    height=params['height'],
                    width=params['width'],
                    guidance_scale=params['cfg'],
                    num_inference_steps=params['num_inference_steps'],
                    max_sequence_length=512,  # æ·»åŠ è¿™ä¸ªå…³é”®å‚æ•°
                    generator=generator
                )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸ºbase64
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            elapsed_time = time.time() - start_time
            
            # æ¸…ç†GPUå†…å­˜
            if device.startswith("cuda"):
                torch.cuda.empty_cache()
            
            # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜å›¾ç‰‡
            save_to_disk = False
            if params.get('save_disk_path'):
                try:
                    image.save(params['save_disk_path'])
                    save_to_disk = True
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {params['save_disk_path']}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            
            logger.info(f"å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œè®¾å¤‡: {device} (å®ä¾‹: {self._instance_id})")
            
            return {
                "success": True,
                "image": image,
                "base64": img_base64,
                "elapsed_time": elapsed_time,
                "save_to_disk": save_to_disk,
                "params": params,
                "device": device,
                "instance_id": self._instance_id
            }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            # æ¸…ç†GPUå†…å­˜ - å¢å¼ºç‰ˆæœ¬
            if device.startswith("cuda"):
                try:
                    # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
                    torch.cuda.empty_cache()
                    # é‡ç½®å†…å­˜åˆ†é…å™¨
                    torch.cuda.reset_peak_memory_stats()
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    # å†æ¬¡æ¸…ç†ç¼“å­˜
                    torch.cuda.empty_cache()
                    logger.debug(f"å·²å½»åº•æ¸…ç†GPUæ˜¾å­˜ (å®ä¾‹: {self._instance_id})")
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†GPUæ˜¾å­˜æ—¶å‡ºé”™: {cleanup_error}")
            return {
                "success": False,
                "error": str(e),
                "elapsed_time": time.time() - start_time,
                "device": device,
                "instance_id": self._instance_id
            }
    
    def get_default_params(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            "num_inference_steps": 50,  # Fluxæ¨èä½¿ç”¨50æ­¥
            "seed": 42,
            "cfg": 3.5,  # Fluxæ¨èä½¿ç”¨3.5
            "height": 1024,  # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "width": 1024,   # æ”¹ä¸º1024x1024å¦‚ç¤ºä¾‹
            "save_disk_path": None
        }
    
    def validate_params(self, **kwargs) -> bool:
        """éªŒè¯å‚æ•°"""
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        required_params = ['num_inference_steps', 'seed', 'cfg', 'height', 'width']
        for param in required_params:
            if param not in kwargs:
                return False
        
        # éªŒè¯å‚æ•°èŒƒå›´
        if kwargs['num_inference_steps'] < 1 or kwargs['num_inference_steps'] > 100:
            return False
        
        if kwargs['cfg'] < 0.1 or kwargs['cfg'] > 20:
            return False
        
        # éªŒè¯å›¾ç‰‡å°ºå¯¸
        if not self.validate_image_size(kwargs['height'], kwargs['width']):
            return False
        
        return True
    
    def get_supported_features(self) -> list:
        """è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨"""
        return ["text-to-image"]
    
    def get_optimization_kwargs(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å‚æ•° - Fluxä½¿ç”¨bfloat16"""
        kwargs = {}
        kwargs["torch_dtype"] = torch.bfloat16
        return kwargs 
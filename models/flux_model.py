import base64
import io
import time
import torch
from typing import Dict, Any, Optional
from PIL import Image
from diffusers import FluxPipeline
from .base import BaseModel
from config import Config
import logging
import os
import threading
import gc

logger = logging.getLogger(__name__)

class FluxModel(BaseModel):
    """Fluxæ¨¡å‹å®ç° - æ”¯æŒGPUéš”ç¦»"""
    
    def __init__(self, gpu_device: Optional[str] = None, isolated_gpu_id: Optional[str] = None):
        super().__init__(
            model_id="flux1-dev",
            model_name="FLUX.1-dev",
            description="Black Forest Labs FLUX.1-dev model for high-quality image generation",
            gpu_device=gpu_device
        )
        self.model_path = Config.get_model_path("flux1-dev")
        # æ·»åŠ çº¿ç¨‹é”ä¿æŠ¤
        self._generation_lock = threading.Lock()
        # ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºå”¯ä¸€æ ‡è¯†
        self._instance_id = f"flux_{gpu_device}_{id(self)}"
        # è®°å½•GPUéš”ç¦»ä¿¡æ¯
        self.isolated_gpu_id = isolated_gpu_id  # åœ¨éš”ç¦»ç¯å¢ƒä¸­çš„GPU ID (é€šå¸¸æ˜¯"0")
        self.physical_gpu_id = self._get_gpu_id_from_device(gpu_device or "cpu")  # ç‰©ç†GPU ID
        
        logger.info(f"åˆ›å»ºFluxModelå®ä¾‹: {self._instance_id}, ç‰©ç†GPU: {self.physical_gpu_id}, éš”ç¦»GPU: {self.isolated_gpu_id}")

    def _get_gpu_id_from_device(self, device: str) -> str:
        """ä»è®¾å¤‡åç§°æå–GPU ID"""
        if device == "cpu":
            return "cpu"
        elif device.startswith("cuda:"):
            return device.split(":")[1]
        else:
            return "0"  # é»˜è®¤ä½¿ç”¨GPU 0
    
    def _update_device_for_task(self, target_device: str):
        """æ›´æ–°è®¾å¤‡é…ç½®ä»¥å‡†å¤‡ä»»åŠ¡æ‰§è¡Œ"""
        # è¿™ä¸ªæ–¹æ³•ç”±å¹¶å‘ç®¡ç†å™¨è°ƒç”¨ï¼Œç¡®ä¿æ¨¡å‹ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
        if target_device.startswith("cuda:"):
            physical_gpu_id = target_device.split(":")[1]
            self.physical_gpu_id = physical_gpu_id
            logger.debug(f"æ›´æ–°ä»»åŠ¡è®¾å¤‡é…ç½®: ç‰©ç†GPU {physical_gpu_id}, é€»è¾‘GPU cuda:0")
    
    def _deep_gpu_cleanup(self, target_device: Optional[str] = None):
        """æ·±åº¦GPUæ˜¾å­˜æ¸…ç† - GPUéš”ç¦»ç‰ˆæœ¬"""
        try:
            # åœ¨GPUéš”ç¦»ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯æ¸…ç†è®¾å¤‡0
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                with torch.cuda.device(0):
                    # å¼ºåˆ¶åŒæ­¥
                    torch.cuda.synchronize()
                    # æ¸…ç†ç¼“å­˜
                    torch.cuda.empty_cache()
                    # é‡ç½®ç»Ÿè®¡
                    torch.cuda.reset_peak_memory_stats()
                    # å†æ¬¡åŒæ­¥
                    torch.cuda.synchronize()
                logger.debug(f"å·²æ¸…ç†GPUæ˜¾å­˜ (ç‰©ç†GPU: {self.physical_gpu_id}, é€»è¾‘GPU: 0)")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception as e:
            logger.warning(f"æ·±åº¦GPUæ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _safe_pipeline_cleanup(self):
        """å®‰å…¨æ¸…ç†pipelineåŠå…¶ç»„ä»¶"""
        try:
            if hasattr(self, 'pipe') and self.pipe is not None:
                logger.info(f"å¼€å§‹æ¸…ç†pipelineç»„ä»¶ (å®ä¾‹: {self._instance_id})")
                
                # è·å–pipelineä¸­çš„æ‰€æœ‰ç»„ä»¶
                components_to_cleanup = []
                
                # æ£€æŸ¥å¹¶æ”¶é›†éœ€è¦æ¸…ç†çš„ç»„ä»¶
                for attr_name in ['transformer', 'vae', 'text_encoder', 'text_encoder_2', 
                                'scheduler', 'tokenizer', 'tokenizer_2']:
                    if hasattr(self.pipe, attr_name):
                        component = getattr(self.pipe, attr_name)
                        if component is not None:
                            components_to_cleanup.append((attr_name, component))
                
                # é€ä¸ªæ¸…ç†ç»„ä»¶
                for name, component in components_to_cleanup:
                    try:
                        # å¦‚æœç»„ä»¶æœ‰å‚æ•°ï¼Œå°è¯•ç§»åŠ¨åˆ°CPU
                        if hasattr(component, 'to') and hasattr(component, 'parameters'):
                            component.to('cpu')
                            logger.debug(f"å·²å°† {name} ç§»åŠ¨åˆ°CPU")
                        
                        # å¦‚æœç»„ä»¶æœ‰cuda()æ–¹æ³•ï¼Œè¯´æ˜å¯èƒ½åœ¨GPUä¸Š
                        if hasattr(component, 'cuda'):
                            try:
                                component.cpu()
                                logger.debug(f"å·²å°† {name} ç§»åŠ¨åˆ°CPU")
                            except:
                                pass
                                
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ç»„ä»¶ {name} æ—¶å‡ºé”™: {e}")
                
                # æ¸…ç†pipelineæœ¬èº«
                try:
                    # ç¦ç”¨CPU offload
                    if hasattr(self.pipe, 'disable_model_cpu_offload'):
                        self.pipe.disable_model_cpu_offload()
                        logger.debug("å·²ç¦ç”¨æ¨¡å‹CPU offload")
                except Exception as e:
                    logger.warning(f"ç¦ç”¨CPU offloadæ—¶å‡ºé”™: {e}")
                
                # å°è¯•å°†æ•´ä¸ªpipelineç§»åŠ¨åˆ°CPU
                try:
                    self.pipe.to('cpu')
                    logger.debug("å·²å°†pipelineç§»åŠ¨åˆ°CPU")
                except Exception as e:
                    logger.warning(f"ç§»åŠ¨pipelineåˆ°CPUæ—¶å‡ºé”™: {e}")
                
                # åˆ é™¤pipelineå¼•ç”¨
                del self.pipe
                self.pipe = None
                logger.info(f"Pipelineå·²åˆ é™¤ (å®ä¾‹: {self._instance_id})")
                
        except Exception as e:
            logger.error(f"æ¸…ç†pipelineæ—¶å‡ºé”™: {e}")
    
    def load(self) -> bool:
        """åŠ è½½Fluxæ¨¡å‹ - GPUéš”ç¦»ç‰ˆæœ¬"""
        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False
            
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name} (å®ä¾‹: {self._instance_id})")
            logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
            logger.info(f"ç›®æ ‡è®¾å¤‡: {self.gpu_device} (ç‰©ç†GPU: {self.physical_gpu_id})")
            
            # è·å–Fluxç‰¹å®šé…ç½®
            flux_config = Config.get_flux_config()
            logger.info(f"Fluxé…ç½®: {flux_config}")
            
            try:
                # ä½¿ç”¨FluxPipeline
                logger.info("å°è¯•ä½¿ç”¨FluxPipelineåŠ è½½æ¨¡å‹...")
                self.pipe = FluxPipeline.from_pretrained(
                    self.model_path,
                    torch_dtype=flux_config["torch_dtype"],
                    use_safetensors=True,
                    local_files_only=True
                )
                
                # æ ¹æ®è®¾å¤‡ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                if self.gpu_device and self.gpu_device.startswith("cuda:"):
                    # å¯¹äºGPUè®¾å¤‡ï¼Œä½¿ç”¨CPU offload
                    # åœ¨GPUéš”ç¦»ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯æŒ‡å®šcuda:0
                    target_device = "cuda:0"
                    logger.info(f"å¯ç”¨æ¨¡å‹CPU offloadï¼Œç›®æ ‡è®¾å¤‡: {target_device}")
                    
                    # ä½¿ç”¨é…ç½®çš„CPU offloadè®¾ç½®
                    if flux_config["use_cpu_offload"]:
                        self.pipe.enable_model_cpu_offload(device=target_device)
                        logger.info("å·²å¯ç”¨æ¨¡å‹CPU offload")
                    else:
                        logger.info("CPU offloadå·²ç¦ç”¨")
                    
                    # éªŒè¯è®¾å¤‡è®¾ç½®
                    if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                        current_device = torch.cuda.current_device()
                        device_name = torch.cuda.get_device_name(0)
                        logger.info(f"GPUéš”ç¦»ç¯å¢ƒéªŒè¯: å½“å‰è®¾å¤‡={current_device}, è®¾å¤‡åç§°={device_name}")
                    
                    logger.info(f"FluxPipeline with CPU offloadåŠ è½½æˆåŠŸ (å®ä¾‹: {self._instance_id})")
                else:
                    # å¯¹äºCPUè®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨CPU
                    self.pipe = self.pipe.to("cpu")
                    logger.info(f"FluxPipelineåŠ è½½åˆ°CPUæˆåŠŸ (å®ä¾‹: {self._instance_id})")
                
                self.is_loaded = True
                logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆ (å®ä¾‹: {self._instance_id})")
                return True
                
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
                
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} åŠ è½½å¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            self.is_loaded = False
            return False
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾ç‰‡ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªç”Ÿæˆä»»åŠ¡
        with self._generation_lock:
            return self._generate_internal(prompt, **kwargs)
    
    def _generate_internal(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """å†…éƒ¨ç”Ÿæˆæ–¹æ³• - GPUéš”ç¦»ç‰ˆæœ¬"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        # éªŒè¯æç¤ºè¯
        if not self.validate_prompt(prompt):
            raise ValueError("æç¤ºè¯éªŒè¯å¤±è´¥")
        
        # è·å–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±çš„å‚æ•°
        params = self.get_default_params()
        params.update(kwargs)
        
        # éªŒè¯å‚æ•°
        if not self.validate_params(**params):
            raise ValueError("å‚æ•°éªŒè¯å¤±è´¥")
        
        # ä½¿ç”¨é€»è¾‘è®¾å¤‡ï¼ˆåœ¨GPUéš”ç¦»ç¯å¢ƒä¸­æ€»æ˜¯cuda:0ï¼‰
        logical_device = "cuda:0" if self.gpu_device.startswith("cuda:") else "cpu"
        logger.debug(f"ğŸ¯ ä½¿ç”¨é€»è¾‘è®¾å¤‡: {logical_device}, ç‰©ç†GPU: {self.physical_gpu_id} (å®ä¾‹: {self._instance_id})")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®éšæœºç§å­ - ä½¿ç”¨CPU generator
            generator = torch.Generator("cpu").manual_seed(params['seed'])
            
            logger.info(f"å¼€å§‹ç”Ÿæˆå›¾ç‰‡ï¼Œæç¤ºè¯: {prompt}ï¼Œé€»è¾‘è®¾å¤‡: {logical_device}, ç‰©ç†GPU: {self.physical_gpu_id} (å®ä¾‹: {self._instance_id})")
            
            # ç¡®ä¿åœ¨æ­£ç¡®çš„GPUä¸Šä¸‹æ–‡ä¸­
            if logical_device.startswith("cuda:") and torch.cuda.is_available():
                torch.cuda.set_device(0)  # åœ¨GPUéš”ç¦»ç¯å¢ƒä¸­æ€»æ˜¯ä½¿ç”¨è®¾å¤‡0
                logger.debug(f"è®¾ç½®CUDAè®¾å¤‡ä¸º: 0 (ç‰©ç†GPU: {self.physical_gpu_id})")
            
            with torch.no_grad():
                # ä½¿ç”¨CPU offloadçš„FluxPipeline
                result = self.pipe(
                    prompt=prompt,
                    height=params['height'],
                    width=params['width'],
                    guidance_scale=params['cfg'],
                    num_inference_steps=params['num_inference_steps'],
                    max_sequence_length=512,  # å…³é”®å‚æ•°
                    generator=generator
                )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸ºbase64
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            elapsed_time = time.time() - start_time
            
            # æˆåŠŸåçš„è½»é‡çº§æ¸…ç†
            self._deep_gpu_cleanup()
            
            # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜å›¾ç‰‡
            save_to_disk = False
            if params.get('save_disk_path'):
                try:
                    image.save(params['save_disk_path'])
                    save_to_disk = True
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {params['save_disk_path']}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            
            logger.info(f"å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œé€»è¾‘è®¾å¤‡: {logical_device}, ç‰©ç†GPU: {self.physical_gpu_id} (å®ä¾‹: {self._instance_id})")
            
            return {
                "success": True,
                "image": image,
                "base64": img_base64,
                "elapsed_time": elapsed_time,
                "save_to_disk": save_to_disk,
                "params": params,
                "device": self.gpu_device,  # è¿”å›åŸå§‹è®¾å¤‡å
                "logical_device": logical_device,
                "physical_gpu": self.physical_gpu_id,
                "instance_id": self._instance_id
            }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e} (å®ä¾‹: {self._instance_id})")
            
            # å¤±è´¥æ—¶çš„å½»åº•æ¸…ç†
            self._emergency_cleanup()
            
            return {
                "success": False,
                "error": str(e),
                "elapsed_time": time.time() - start_time,
                "device": self.gpu_device,
                "logical_device": logical_device,
                "physical_gpu": self.physical_gpu_id,
                "instance_id": self._instance_id
            }
    
    def _emergency_cleanup(self):
        """ç´§æ€¥æ¸…ç† - GPUéš”ç¦»ç‰ˆæœ¬"""
        logger.warning(f"æ‰§è¡Œç´§æ€¥æ¸…ç† (å®ä¾‹: {self._instance_id}, ç‰©ç†GPU: {self.physical_gpu_id})")
        
        try:
            # 1. å…ˆå°è¯•æ·±åº¦GPUæ¸…ç†
            self._deep_gpu_cleanup()
            
            # 2. æ¸…ç†pipelineç»„ä»¶
            self._safe_pipeline_cleanup()
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # 4. å†æ¬¡æ¸…ç†GPU
            self._deep_gpu_cleanup()
            
            # 5. é‡æ–°åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.is_loaded:
                logger.info(f"å°è¯•é‡æ–°åŠ è½½æ¨¡å‹ (å®ä¾‹: {self._instance_id})")
                self.load()
            
        except Exception as e:
            logger.error(f"ç´§æ€¥æ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def unload(self):
        """å¸è½½æ¨¡å‹ - å¢å¼ºç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹å¸è½½æ¨¡å‹ (å®ä¾‹: {self._instance_id}, ç‰©ç†GPU: {self.physical_gpu_id})")
        
        # æ ‡è®°ä¸ºæœªåŠ è½½
        self.is_loaded = False
        
        # å®‰å…¨æ¸…ç†pipeline
        self._safe_pipeline_cleanup()
        
        # æ·±åº¦æ¸…ç†GPU
        self._deep_gpu_cleanup()
        
        logger.info(f"æ¨¡å‹ {self.model_id} å·²å®Œå…¨å¸è½½ (å®ä¾‹: {self._instance_id})")
    
    def get_default_params(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            "num_inference_steps": 50,  # Fluxæ¨èä½¿ç”¨50æ­¥
            "seed": 42,
            "cfg": 3.5,  # Fluxæ¨èä½¿ç”¨3.5
            "height": 1024,  # 1024x1024
            "width": 1024,   # 1024x1024
            "save_disk_path": None
        }
    
    def validate_params(self, **kwargs) -> bool:
        """éªŒè¯å‚æ•°"""
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        required_params = ['num_inference_steps', 'seed', 'cfg', 'height', 'width']
        for param in required_params:
            if param not in kwargs:
                return False
        
        # éªŒè¯å‚æ•°èŒƒå›´
        if kwargs['num_inference_steps'] < 1 or kwargs['num_inference_steps'] > 100:
            return False
        
        if kwargs['cfg'] < 0.1 or kwargs['cfg'] > 20:
            return False
        
        # éªŒè¯å›¾ç‰‡å°ºå¯¸
        if not self.validate_image_size(kwargs['height'], kwargs['width']):
            return False
        
        return True
    
    def get_supported_features(self) -> list:
        """è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨"""
        return ["text-to-image"]
    
    def get_optimization_kwargs(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å‚æ•° - Fluxä½¿ç”¨bfloat16"""
        kwargs = {}
        kwargs["torch_dtype"] = torch.bfloat16
        return kwargs 
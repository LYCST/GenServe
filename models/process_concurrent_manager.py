import asyncio
import threading
import time
import os
import multiprocessing as mp
from typing import Dict, List, Optional, Any, Tuple
import logging
from queue import Queue, Empty, PriorityQueue
from dataclasses import dataclass, field
import uuid
import torch
from gpu_isolation_manager import GPUIsolationManager
from config import Config

logger = logging.getLogger(__name__)

@dataclass
class GenerationTask:
    """ç”Ÿæˆä»»åŠ¡"""
    task_id: str
    model_id: str
    prompt: str
    params: Dict[str, Any]
    result_queue: Queue
    created_at: float
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
    
    def __lt__(self, other):
        """æ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ—æ’åº"""
        if self.priority != other.priority:
            return self.priority < other.priority
        return self.created_at < other.created_at

class ProcessConcurrentModelManager:
    """è¿›ç¨‹çº§å¹¶å‘æ¨¡å‹ç®¡ç†å™¨ - ä½¿ç”¨å­è¿›ç¨‹å®ç°çœŸæ­£çš„GPUéš”ç¦»"""
    
    def __init__(self):
        self.gpu_manager = GPUIsolationManager()
        self.model_instances: Dict[str, List[str]] = {}  # model_id -> [gpu_ids]
        
        # å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å’Œè°ƒåº¦
        self.global_task_queue = PriorityQueue()
        self.task_results: Dict[str, Queue] = {}  # task_id -> result_queue
        
        # å·¥ä½œçº¿ç¨‹ç®¡ç†
        self.scheduler_thread = None
        self.is_running = False
        
        # è´Ÿè½½å‡è¡¡è®¡æ•°å™¨
        self.gpu_round_robin_counters: Dict[str, int] = {}  # model_id -> counter
        
        # ä»é…ç½®è·å–å‚æ•°
        config = Config.get_config()
        self.max_global_queue_size = config["concurrent"]["max_global_queue_size"]
        self.task_timeout = config["concurrent"]["task_timeout"]
        self.scheduler_sleep_time = config["concurrent"]["scheduler_sleep_time"]
        self.load_balance_strategy = config["concurrent"]["load_balance_strategy"]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "queue_full_rejections": 0
        }
        
        self._initialize_models()
        self._start_manager()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹è¿›ç¨‹"""
        model_gpu_config = Config.get_config()["model_management"]["model_gpu_config"]
        model_paths = Config.get_config()["model_management"]["model_paths"]
        
        for model_id, gpu_list in model_gpu_config.items():
            if model_id not in self.model_instances:
                self.model_instances[model_id] = []
            
            model_path = model_paths.get(model_id, "")
            if not model_path or not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                continue
            
            # ä¸ºæ¯ä¸ªGPUåˆ›å»ºéš”ç¦»è¿›ç¨‹
            for gpu_device in gpu_list:
                if gpu_device.startswith("cuda:"):
                    gpu_id = gpu_device.split(":")[1]
                    
                    # åˆ›å»ºGPUéš”ç¦»è¿›ç¨‹
                    if self.gpu_manager.create_gpu_process(gpu_id, model_path, model_id):
                        self.model_instances[model_id].append(gpu_id)
                        logger.info(f"âœ… GPU {gpu_id} è¿›ç¨‹åˆ›å»ºæˆåŠŸ")
                    else:
                        logger.error(f"âŒ GPU {gpu_id} è¿›ç¨‹åˆ›å»ºå¤±è´¥")
        
        # æ‰“å°åˆå§‹åŒ–ç»“æœ
        total_processes = sum(len(gpus) for gpus in self.model_instances.values())
        logger.info(f"ğŸ‰ æ¨¡å‹è¿›ç¨‹åˆå§‹åŒ–å®Œæˆï¼Œæ€»å…±åˆ›å»ºäº† {total_processes} ä¸ªGPUè¿›ç¨‹")
        for model_id, gpus in self.model_instances.items():
            logger.info(f"  {model_id}: {len(gpus)} ä¸ªGPUè¿›ç¨‹ ({gpus})")
    
    def _start_manager(self):
        """å¯åŠ¨ç®¡ç†å™¨"""
        self.is_running = True
        
        # å¯åŠ¨å…¨å±€è°ƒåº¦å™¨
        self.scheduler_thread = threading.Thread(
            target=self._global_scheduler_loop, 
            name="process-scheduler"
        )
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()
        logger.info("è¿›ç¨‹çº§å…¨å±€è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        # å¯åŠ¨è¿›ç¨‹ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(
            target=self._process_monitor_loop,
            name="process-monitor"
        )
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        logger.info("è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def _process_monitor_loop(self):
        """è¿›ç¨‹ç›‘æ§å¾ªç¯ - å®šæœŸæ£€æŸ¥æ­»äº¡è¿›ç¨‹å¹¶é‡å¯"""
        logger.info("è¿›ç¨‹ç›‘æ§çº¿ç¨‹å¼€å§‹è¿è¡Œ")
        
        while self.is_running:
            try:
                # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡è¿›ç¨‹çŠ¶æ€
                time.sleep(30)
                
                # æ£€æŸ¥å¹¶é‡å¯æ­»äº¡è¿›ç¨‹
                restart_results = self.gpu_manager.check_and_restart_dead_processes()
                
                if restart_results:
                    logger.info(f"è¿›ç¨‹ç›‘æ§: æ£€æŸ¥äº† {len(restart_results)} ä¸ªè¿›ç¨‹")
                    for process_key, success in restart_results.items():
                        if success:
                            logger.info(f"âœ… è¿›ç¨‹ {process_key} é‡å¯æˆåŠŸ")
                        else:
                            logger.warning(f"âš ï¸ è¿›ç¨‹ {process_key} é‡å¯å¤±è´¥")
                
            except Exception as e:
                logger.error(f"è¿›ç¨‹ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
    
    def _global_scheduler_loop(self):
        """å…¨å±€è°ƒåº¦å™¨å¾ªç¯ - æ™ºèƒ½ä»»åŠ¡åˆ†é…"""
        logger.info("è¿›ç¨‹çº§å…¨å±€è°ƒåº¦å™¨å¼€å§‹è¿è¡Œ")
        
        while self.is_running:
            try:
                # è·å–å…¨å±€ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                task = self.global_task_queue.get(timeout=self.scheduler_sleep_time)
                
                # æ‰¾åˆ°æœ€ä½³çš„GPUè¿›ç¨‹
                best_gpu_id = self._find_best_gpu(task.model_id)
                
                if best_gpu_id:
                    # å¼‚æ­¥åˆ†é…ä»»åŠ¡ç»™GPUè¿›ç¨‹ï¼Œä¸ç­‰å¾…å®Œæˆ
                    self._submit_task_async(task, best_gpu_id)
                    logger.info(f"ä»»åŠ¡ {task.task_id} å·²åˆ†é…ç»™GPU {best_gpu_id}")
                else:
                    # æ‰€æœ‰GPUéƒ½å¿™ç¢Œï¼Œé‡æ–°æ”¾å›é˜Ÿåˆ—
                    self.global_task_queue.put(task)
                    # æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                    time.sleep(self.scheduler_sleep_time * 5)
                    
            except Empty:
                continue
            except Exception as e:
                logger.error(f"å…¨å±€è°ƒåº¦å™¨é”™è¯¯: {e}")
    
    def _find_best_gpu(self, model_id: str) -> Optional[str]:
        """æ‰¾åˆ°æœ€ä½³çš„GPUè¿›ç¨‹ - å®ç°è½®è¯¢è´Ÿè½½å‡è¡¡"""
        if model_id not in self.model_instances:
            logger.warning(f"âš ï¸ æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
            return None
        
        available_gpus = self.model_instances[model_id]
        
        if not available_gpus:
            logger.debug(f"âš ï¸ æ¨¡å‹ {model_id} æ²¡æœ‰å¯ç”¨GPU")
            return None
        
        # åˆå§‹åŒ–è½®è¯¢è®¡æ•°å™¨
        if model_id not in self.gpu_round_robin_counters:
            self.gpu_round_robin_counters[model_id] = 0
        
        # è½®è¯¢ç­–ç•¥ï¼šä¾æ¬¡é€‰æ‹©ä¸‹ä¸€ä¸ªGPU
        gpu_index = self.gpu_round_robin_counters[model_id] % len(available_gpus)
        selected_gpu = available_gpus[gpu_index]
        
        # æ›´æ–°è®¡æ•°å™¨
        self.gpu_round_robin_counters[model_id] += 1
        
        logger.debug(f"è´Ÿè½½å‡è¡¡: æ¨¡å‹ {model_id} é€‰æ‹©GPU {selected_gpu} (ç´¢å¼•: {gpu_index}/{len(available_gpus)})")
        return selected_gpu
    
    def _submit_task_async(self, task: GenerationTask, gpu_id: str):
        """å¼‚æ­¥æäº¤ä»»åŠ¡åˆ°GPUè¿›ç¨‹"""
        # åˆ›å»ºåå°çº¿ç¨‹å¤„ç†ä»»åŠ¡ï¼Œä¸é˜»å¡è°ƒåº¦å™¨
        thread = threading.Thread(
            target=self._process_task_on_gpu_process,
            args=(task, gpu_id),
            name=f"task-{task.task_id[:8]}"
        )
        thread.daemon = True
        thread.start()
    
    def _process_task_on_gpu_process(self, task: GenerationTask, gpu_id: str):
        """åœ¨GPUè¿›ç¨‹ä¸­å¤„ç†ä»»åŠ¡"""
        logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡ {task.task_id[:8]} (GPU: {gpu_id})")
        
        try:
            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data = {
                "task_id": task.task_id,
                "prompt": task.prompt,
                "height": task.params.get('height', 1024),
                "width": task.params.get('width', 1024),
                "cfg": task.params.get('cfg', 3.5),
                "num_inference_steps": task.params.get('num_inference_steps', 50),
                "seed": task.params.get('seed', 42),
                "mode": task.params.get('mode', 'text2img'),
                "strength": task.params.get('strength', 0.8),
                "input_image": task.params.get('input_image'),
                "mask_image": task.params.get('mask_image'),
                "control_image": task.params.get('control_image')
            }
            
            # æäº¤ä»»åŠ¡åˆ°GPUè¿›ç¨‹
            result = self.gpu_manager.submit_task(gpu_id, task.model_id, task_data)
            
            if result:
                # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
                result.update({
                    "task_id": task.task_id,
                    "gpu_id": gpu_id,
                    "model_id": task.model_id,
                    "thread_name": threading.current_thread().name
                })
                
                # å‘é€ç»“æœ
                task.result_queue.put(result)
                
                # æ›´æ–°ç»Ÿè®¡
                if result.get("success", False):
                    self.stats["completed_tasks"] += 1
                    logger.info(f"âœ… ä»»åŠ¡ {task.task_id[:8]} å®Œæˆ (GPU: {gpu_id})")
                else:
                    self.stats["failed_tasks"] += 1
                    logger.error(f"âŒ ä»»åŠ¡ {task.task_id[:8]} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                # ä»»åŠ¡æäº¤å¤±è´¥
                error_result = {
                    "success": False,
                    "error": "GPUè¿›ç¨‹ä¸å¯ç”¨",
                    "task_id": task.task_id,
                    "gpu_id": gpu_id,
                    "model_id": task.model_id
                }
                task.result_queue.put(error_result)
                self.stats["failed_tasks"] += 1
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ä»»åŠ¡ {task.task_id[:8]} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            error_result = {
                "success": False,
                "error": str(e),
                "task_id": task.task_id,
                "gpu_id": gpu_id,
                "model_id": task.model_id
            }
            task.result_queue.put(error_result)
            self.stats["failed_tasks"] += 1
    
    async def generate_image_async(
        self, 
        model_id: str, 
        prompt: str, 
        priority: int = 0,
        height: int = 1024,
        width: int = 1024,
        num_inference_steps: int = 50,
        cfg: float = 3.5,
        seed: int = 42,
        mode: str = "text2img",
        strength: float = 0.8,
        input_image: Optional[str] = None,
        mask_image: Optional[str] = None,
        control_image: Optional[str] = None
    ) -> Dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡ - æ”¯æŒå›¾ç”Ÿå›¾"""
        if not self.is_running:
            return {
                "success": False,
                "error": "å¹¶å‘ç®¡ç†å™¨æœªè¿è¡Œ",
                "task_id": "",
                "gpu_id": None,
                "model_id": model_id
            }
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒ
        if model_id not in self.model_instances:
            return {
                "success": False,
                "error": f"æ¨¡å‹ {model_id} æœªåŠ è½½",
                "task_id": "",
                "gpu_id": None,
                "model_id": model_id
            }
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        result_queue = Queue()
        
        # æ„å»ºä»»åŠ¡å‚æ•°
        task_params = {
            "height": height,
            "width": width,
            "num_inference_steps": num_inference_steps,
            "cfg": cfg,
            "seed": seed,
            "mode": mode,
            "strength": strength,
            "input_image": input_image,
            "mask_image": mask_image,
            "control_image": control_image
        }
        
        task = GenerationTask(
            task_id=task_id,
            model_id=model_id,
            prompt=prompt,
            params=task_params,
            result_queue=result_queue,
            created_at=time.time(),
            priority=priority
        )
        
        logger.info(f"æäº¤{mode}ä»»åŠ¡: {task_id}, æ¨¡å‹: {model_id}, ä¼˜å…ˆçº§: {priority}")
        
        # æäº¤åˆ°å…¨å±€é˜Ÿåˆ—
        try:
            self.global_task_queue.put(task)
            logger.info(f"ä»»åŠ¡ {task_id} å·²æäº¤åˆ°å…¨å±€é˜Ÿåˆ—")
        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"æäº¤ä»»åŠ¡å¤±è´¥: {str(e)}",
                "task_id": task_id,
                "gpu_id": None,
                "model_id": model_id
            }
        
        # ç­‰å¾…ç»“æœ
        try:
            result = result_queue.get(timeout=Config.TASK_TIMEOUT)
            logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆ: {result.get('success', False)}")
            
            # æ·»åŠ ä»»åŠ¡IDåˆ°ç»“æœ
            result["task_id"] = task_id
            result["model_id"] = model_id
            result["mode"] = mode
            
            return result
            
        except Empty:
            logger.error(f"ä»»åŠ¡ {task_id} è¶…æ—¶")
            return {
                "success": False,
                "error": f"ä»»åŠ¡è¶…æ—¶ ({Config.TASK_TIMEOUT}ç§’)",
                "task_id": task_id,
                "gpu_id": None,
                "model_id": model_id
            }
        except Exception as e:
            logger.error(f"ç­‰å¾…ä»»åŠ¡ {task_id} ç»“æœæ—¶å‡ºé”™: {e}")
            return {
                "success": False,
                "error": f"ç­‰å¾…ç»“æœå¤±è´¥: {str(e)}",
                "task_id": task_id,
                "gpu_id": None,
                "model_id": model_id
            }
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çŠ¶æ€"""
        # è·å–GPUè¿›ç¨‹çŠ¶æ€
        gpu_status = self.gpu_manager.get_gpu_status()
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_processes = sum(len(gpus) for gpus in self.model_instances.values())
        alive_processes = sum(1 for status in gpu_status.values() if status["alive"])
        dead_processes = total_processes - alive_processes
        
        # ç»Ÿè®¡é‡å¯ä¿¡æ¯
        total_restarts = sum(status.get("restart_attempts", 0) for status in gpu_status.values())
        max_restarts_reached = sum(1 for status in gpu_status.values() 
                                 if status.get("restart_attempts", 0) >= status.get("max_restart_attempts", 3))
        
        return {
            "is_running": self.is_running,
            "global_queue_size": self.global_task_queue.qsize(),
            "max_global_queue_size": self.max_global_queue_size,
            "total_processes": total_processes,
            "alive_processes": alive_processes,
            "dead_processes": dead_processes,
            "total_restarts": total_restarts,
            "max_restarts_reached": max_restarts_reached,
            "load_balance_strategy": self.load_balance_strategy,
            "task_timeout": self.task_timeout,
            "stats": self.stats.copy(),
            "model_instances": {
                model_id: {
                    "gpu_count": len(gpus),
                    "gpu_ids": gpus
                }
                for model_id, gpus in self.model_instances.items()
            },
            "gpu_processes": gpu_status
        }
    
    def get_model_list(self) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        models = []
        for model_id, gpus in self.model_instances.items():
            model_info = {
                "model_id": model_id,
                "model_name": "FLUX.1-dev" if model_id == "flux1-dev" else model_id,
                "description": "Black Forest Labs FLUX.1-dev model for high-quality image generation",
                "total_gpu_processes": len(gpus),
                "available_gpu_processes": len(gpus),  # ç®€åŒ–ï¼Œå®é™…åº”è¯¥æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                "supported_features": ["text-to-image"]
            }
            models.append(model_info)
        return models
    
    def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        logger.info("æ­£åœ¨å…³é—­è¿›ç¨‹çº§å¹¶å‘æ¨¡å‹ç®¡ç†å™¨...")
        
        self.is_running = False
        
        # ç­‰å¾…è°ƒåº¦å™¨çº¿ç¨‹ç»“æŸ
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5.0)
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        if hasattr(self, 'monitor_thread') and self.monitor_thread:
            self.monitor_thread.join(timeout=5.0)
        
        # å…³é—­GPUéš”ç¦»ç®¡ç†å™¨
        self.gpu_manager.shutdown()
        
        logger.info("è¿›ç¨‹çº§å¹¶å‘æ¨¡å‹ç®¡ç†å™¨å·²å…³é—­") 
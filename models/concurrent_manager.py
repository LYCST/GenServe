import asyncio
import threading
import time
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
import logging
from queue import Queue, Empty, PriorityQueue
from dataclasses import dataclass, field
from .base import BaseModel
from .flux_model import FluxModel
from device_manager import DeviceManager
from config import Config
import uuid
import torch

logger = logging.getLogger(__name__)

@dataclass
class GenerationTask:
    """ç”Ÿæˆä»»åŠ¡"""
    task_id: str
    model_id: str
    prompt: str
    params: Dict[str, Any]
    result_queue: Queue
    created_at: float
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
    
    def __lt__(self, other):
        """æ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ—æ’åº"""
        if self.priority != other.priority:
            return self.priority < other.priority
        return self.created_at < other.created_at

class ModelInstance:
    """æ¨¡å‹å®ä¾‹ï¼Œç»‘å®šåˆ°ç‰¹å®šGPU"""
    def __init__(self, model: BaseModel, device: str, instance_id: str):
        self.model = model
        self.device = device
        self.instance_id = instance_id
        self.is_busy = False
        self.last_used = time.time()
        self.total_generations = 0
        self.current_task = None
        self.lock = threading.Lock()
        self.task_queue = PriorityQueue()  # ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—
        
        # ä»é…ç½®è·å–é˜Ÿåˆ—å¤§å°é™åˆ¶
        config = Config.get_config()
        self.max_queue_size = config["concurrent"]["max_gpu_queue_size"]
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®ä¾‹æ˜¯å¦å¯ç”¨"""
        with self.lock:
            return not self.is_busy and self.model.is_loaded
    
    def can_accept_task(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¥å—æ–°ä»»åŠ¡ï¼ˆè€ƒè™‘é˜Ÿåˆ—å¤§å°ï¼‰"""
        return self.task_queue.qsize() < self.max_queue_size
    
    def set_busy(self, busy: bool, task_id: str = None):
        """è®¾ç½®å¿™ç¢ŒçŠ¶æ€"""
        with self.lock:
            self.is_busy = busy
            self.current_task = task_id if busy else None
            if not busy:
                self.last_used = time.time()
                self.total_generations += 1
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å®ä¾‹çŠ¶æ€"""
        with self.lock:
            return {
                "instance_id": self.instance_id,
                "device": self.device,
                "is_busy": self.is_busy,
                "is_loaded": self.model.is_loaded,
                "current_task": self.current_task,
                "queue_size": self.task_queue.qsize(),
                "max_queue_size": self.max_queue_size,
                "total_generations": self.total_generations,
                "last_used": self.last_used
            }

class ConcurrentModelManager:
    """æ”¹è¿›çš„å¹¶å‘æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.device_manager = DeviceManager()
        self.model_instances: Dict[str, List[ModelInstance]] = {}  # model_id -> [instances]
        self.instance_lookup: Dict[str, ModelInstance] = {}  # instance_id -> instance
        
        # å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å’Œè°ƒåº¦
        self.global_task_queue = PriorityQueue()
        self.task_results: Dict[str, Queue] = {}  # task_id -> result_queue
        
        # å·¥ä½œçº¿ç¨‹ç®¡ç†
        self.worker_threads = []
        self.scheduler_thread = None
        self.is_running = False
        
        # ä»é…ç½®è·å–å‚æ•°
        config = Config.get_config()
        self.max_global_queue_size = config["concurrent"]["max_global_queue_size"]
        self.task_timeout = config["concurrent"]["task_timeout"]
        self.scheduler_sleep_time = config["concurrent"]["scheduler_sleep_time"]
        self.load_balance_strategy = config["concurrent"]["load_balance_strategy"]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "queue_full_rejections": 0
        }
        
        self._initialize_models()
        self._start_manager()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹å®ä¾‹"""
        model_gpu_config = Config.get_config()["model_management"]["model_gpu_config"]
        
        for model_id, gpu_list in model_gpu_config.items():
            if model_id not in self.model_instances:
                self.model_instances[model_id] = []
            
            # ä¸ºæ¯ä¸ªGPUåˆ›å»ºä¸€ä¸ªæ¨¡å‹å®ä¾‹
            for i, gpu_device in enumerate(gpu_list):
                if self.device_manager.validate_device(gpu_device):
                    try:
                        # åˆ›å»ºæ¨¡å‹å®ä¾‹
                        if model_id == "flux1-dev":
                            model = FluxModel(gpu_device=gpu_device)
                        else:
                            logger.warning(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_id}")
                            continue
                        
                        # ç”Ÿæˆå”¯ä¸€å®ä¾‹ID
                        instance_id = f"{model_id}_{gpu_device.replace(':', '_')}_{i}"
                        
                        # åŠ è½½æ¨¡å‹
                        if model.load():
                            instance = ModelInstance(model, gpu_device, instance_id)
                            self.model_instances[model_id].append(instance)
                            self.instance_lookup[instance_id] = instance
                            logger.info(f"æ¨¡å‹å®ä¾‹ {instance_id} åˆ›å»ºæˆåŠŸ")
                        else:
                            logger.error(f"æ¨¡å‹å®ä¾‹ {model_id} åœ¨ {gpu_device} ä¸ŠåŠ è½½å¤±è´¥")
                    except Exception as e:
                        logger.error(f"åˆ›å»ºæ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
    
    def _start_manager(self):
        """å¯åŠ¨ç®¡ç†å™¨"""
        self.is_running = True
        
        # å¯åŠ¨å…¨å±€è°ƒåº¦å™¨
        self.scheduler_thread = threading.Thread(
            target=self._global_scheduler_loop, 
            name="global-scheduler"
        )
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()
        logger.info("å…¨å±€è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        # ä¸ºæ¯ä¸ªGPUå®ä¾‹å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for model_id, instances in self.model_instances.items():
            for instance in instances:
                worker = threading.Thread(
                    target=self._gpu_worker_loop,
                    args=(instance,),
                    name=f"worker-{instance.instance_id}"
                )
                worker.daemon = True
                worker.start()
                self.worker_threads.append(worker)
                logger.info(f"GPUå·¥ä½œçº¿ç¨‹ {worker.name} å·²å¯åŠ¨")
    
    def _global_scheduler_loop(self):
        """å…¨å±€è°ƒåº¦å™¨å¾ªç¯ - æ™ºèƒ½ä»»åŠ¡åˆ†é…"""
        logger.info("å…¨å±€è°ƒåº¦å™¨å¼€å§‹è¿è¡Œ")
        
        while self.is_running:
            try:
                # è·å–å…¨å±€ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                task = self.global_task_queue.get(timeout=self.scheduler_sleep_time)
                
                # æ‰¾åˆ°æœ€ä½³çš„GPUå®ä¾‹
                best_instance = self._find_best_instance(task.model_id)
                
                if best_instance:
                    # åˆ†é…ä»»åŠ¡ç»™GPU
                    best_instance.task_queue.put(task)
                    logger.info(f"ä»»åŠ¡ {task.task_id} å·²åˆ†é…ç»™ {best_instance.instance_id}")
                else:
                    # æ‰€æœ‰GPUéƒ½å¿™ç¢Œæˆ–é˜Ÿåˆ—å·²æ»¡ï¼Œé‡æ–°æ”¾å›é˜Ÿåˆ—
                    self.global_task_queue.put(task)
                    # æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                    time.sleep(self.scheduler_sleep_time * 5)
                    
            except Empty:
                continue
            except Exception as e:
                logger.error(f"å…¨å±€è°ƒåº¦å™¨é”™è¯¯: {e}")
    
    def _find_best_instance(self, model_id: str) -> Optional[ModelInstance]:
        """æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹å®ä¾‹ - æ”¹è¿›çš„è´Ÿè½½å‡è¡¡ç®—æ³•"""
        if model_id not in self.model_instances:
            logger.warning(f"âš ï¸ æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
            return None
        
        instances = self.model_instances[model_id]
        
        # è¿‡æ»¤å¯æ¥å—ä»»åŠ¡çš„å®ä¾‹
        available_instances = [
            inst for inst in instances 
            if inst.model.is_loaded and inst.can_accept_task()
        ]
        
        if not available_instances:
            logger.debug(f"âš ï¸ æ¨¡å‹ {model_id} æ²¡æœ‰å¯ç”¨å®ä¾‹")
            return None
        
        # ä¼˜å…ˆé€‰æ‹©ç©ºé—²çš„å®ä¾‹
        idle_instances = [inst for inst in available_instances if not inst.is_busy]
        if idle_instances:
            # åœ¨ç©ºé—²å®ä¾‹ä¸­é€‰æ‹©é˜Ÿåˆ—æœ€çŸ­çš„
            best = min(idle_instances, key=lambda x: x.task_queue.qsize())
            logger.debug(f"âœ… é€‰æ‹©ç©ºé—²å®ä¾‹ {best.instance_id}ï¼Œé˜Ÿåˆ—å¤§å°: {best.task_queue.qsize()}")
            return best
        
        # å¦‚æœéƒ½åœ¨å¿™ç¢Œï¼Œé€‰æ‹©é˜Ÿåˆ—æœ€çŸ­çš„
        best = min(available_instances, key=lambda x: x.task_queue.qsize())
        logger.debug(f"âœ… é€‰æ‹©å¿™ç¢Œå®ä¾‹ {best.instance_id}ï¼Œé˜Ÿåˆ—å¤§å°: {best.task_queue.qsize()}")
        return best
    
    def _gpu_worker_loop(self, instance: ModelInstance):
        """GPUå·¥ä½œçº¿ç¨‹å¾ªç¯"""
        logger.info(f"GPUå·¥ä½œçº¿ç¨‹å¼€å§‹è¿è¡Œ: {instance.instance_id}")
        
        while self.is_running:
            try:
                # è·å–ä»»åŠ¡
                task = instance.task_queue.get(timeout=1.0)
                
                # å¤„ç†ä»»åŠ¡
                self._process_task_on_gpu(task, instance)
                
            except Empty:
                continue
            except Exception as e:
                logger.error(f"GPUå·¥ä½œçº¿ç¨‹ {instance.instance_id} é”™è¯¯: {e}")
    
    def _process_task_on_gpu(self, task: GenerationTask, instance: ModelInstance):
        """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†ä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ä»»åŠ¡ {task.task_id[:8]} åœ¨ {instance.device}")
        
        # æ ‡è®°GPUä¸ºå¿™ç¢Œ
        instance.set_busy(True, task.task_id)
        
        try:
            # è®¾ç½®CUDAè®¾å¤‡
            if instance.device.startswith("cuda:"):
                gpu_id = int(instance.device.split(":")[1])
                torch.cuda.set_device(gpu_id)
                torch.cuda.empty_cache()
                logger.debug(f"ğŸ¯ CUDAè®¾å¤‡å·²è®¾ç½®ä¸º: cuda:{gpu_id}")
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if hasattr(instance.model, 'gpu_device'):
                instance.model.gpu_device = instance.device
            
            # æ‰§è¡Œç”Ÿæˆ
            result = instance.model.generate(task.prompt, **task.params)
            result.update({
                "task_id": task.task_id,
                "device": instance.device,
                "instance_id": instance.instance_id,
                "queue_wait_time": time.time() - task.created_at
            })
            
            # è¿”å›ç»“æœ
            task.result_queue.put(result)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["completed_tasks"] += 1
            
            logger.info(f"âœ… ä»»åŠ¡ {task.task_id[:8]} å®Œæˆï¼Œè®¾å¤‡: {instance.device}ï¼Œè€—æ—¶: {result.get('elapsed_time', 0):.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ä»»åŠ¡ {task.task_id[:8]} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            result = {
                "success": False,
                "error": str(e),
                "task_id": task.task_id,
                "device": instance.device,
                "instance_id": instance.instance_id
            }
            task.result_queue.put(result)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["failed_tasks"] += 1
            
        finally:
            # é‡Šæ”¾GPU
            instance.set_busy(False)
            
            # æ¸…ç†GPUç¼“å­˜
            if instance.device.startswith("cuda:"):
                torch.cuda.empty_cache()
    
    async def generate_image_async(self, model_id: str, prompt: str, priority: int = 0, **kwargs) -> Dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡ - æ”¯æŒä¼˜å…ˆçº§"""
        task_id = str(uuid.uuid4())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨å®ä¾‹
        if model_id not in self.model_instances or not self.model_instances[model_id]:
            return {
                "success": False,
                "error": f"æ¨¡å‹ {model_id} ä¸å¯ç”¨",
                "task_id": task_id
            }
        
        # æ£€æŸ¥å…¨å±€é˜Ÿåˆ—æ˜¯å¦è¿‡è½½
        if self.global_task_queue.qsize() > self.max_global_queue_size:
            self.stats["queue_full_rejections"] += 1
            return {
                "success": False,
                "error": "æœåŠ¡å™¨è¿‡è½½ï¼Œè¯·ç¨åé‡è¯•",
                "task_id": task_id
            }
        
        # åˆ›å»ºç»“æœé˜Ÿåˆ—
        result_queue = Queue()
        self.task_results[task_id] = result_queue
        
        # åˆ›å»ºä»»åŠ¡
        task = GenerationTask(
            task_id=task_id,
            model_id=model_id,
            prompt=prompt,
            params=kwargs,
            result_queue=result_queue,
            created_at=time.time(),
            priority=priority
        )
        
        # æ·»åŠ åˆ°å…¨å±€ä»»åŠ¡é˜Ÿåˆ—
        self.global_task_queue.put(task)
        self.stats["total_tasks"] += 1
        
        logger.info(f"ä»»åŠ¡ {task_id} å·²åŠ å…¥é˜Ÿåˆ—ï¼Œä¼˜å…ˆçº§: {priority}")
        
        try:
            # ç­‰å¾…ç»“æœï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
            result = await asyncio.get_event_loop().run_in_executor(
                None, result_queue.get, True, self.task_timeout
            )
            return result
            
        except Exception as e:
            logger.error(f"ç­‰å¾…ä»»åŠ¡ {task_id} ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"ä»»åŠ¡è¶…æ—¶æˆ–å‘ç”Ÿé”™è¯¯: {str(e)}",
                "task_id": task_id
            }
        finally:
            # æ¸…ç†ç»“æœé˜Ÿåˆ—
            self.task_results.pop(task_id, None)
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çŠ¶æ€"""
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_queue_size = sum(
            inst.task_queue.qsize() 
            for instances in self.model_instances.values() 
            for inst in instances
        )
        
        busy_instances = sum(
            1 for instances in self.model_instances.values() 
            for inst in instances if inst.is_busy
        )
        
        total_instances = sum(
            len(instances) for instances in self.model_instances.values()
        )
        
        return {
            "is_running": self.is_running,
            "global_queue_size": self.global_task_queue.qsize(),
            "max_global_queue_size": self.max_global_queue_size,
            "total_queue_size": total_queue_size,
            "worker_threads": len(self.worker_threads),
            "busy_instances": busy_instances,
            "total_instances": total_instances,
            "load_balance_strategy": self.load_balance_strategy,
            "task_timeout": self.task_timeout,
            "stats": self.stats.copy(),
            "model_instances": {
                model_id: [inst.get_status() for inst in instances]
                for model_id, instances in self.model_instances.items()
            }
        }
    
    def get_model_list(self) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        models = []
        for model_id, instances in self.model_instances.items():
            if instances:
                first_instance = instances[0]
                model_info = first_instance.model.get_info()
                model_info.update({
                    "total_instances": len(instances),
                    "available_instances": len([inst for inst in instances if inst.is_available()]),
                    "busy_instances": len([inst for inst in instances if inst.is_busy]),
                    "total_queue_size": sum(inst.task_queue.qsize() for inst in instances)
                })
                models.append(model_info)
        return models
    
    def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        logger.info("æ­£åœ¨å…³é—­å¹¶å‘æ¨¡å‹ç®¡ç†å™¨...")
        
        self.is_running = False
        
        # ç­‰å¾…è°ƒåº¦å™¨çº¿ç¨‹ç»“æŸ
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5.0)
        
        # ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸ
        for worker in self.worker_threads:
            worker.join(timeout=5.0)
        
        # å¸è½½æ‰€æœ‰æ¨¡å‹
        for instances in self.model_instances.values():
            for instance in instances:
                instance.model.unload()
        
        logger.info("å¹¶å‘æ¨¡å‹ç®¡ç†å™¨å·²å…³é—­") 
import asyncio
import threading
import time
import os
import subprocess
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
import logging
from queue import Queue, Empty, PriorityQueue
from dataclasses import dataclass, field
from .base import BaseModel
from .flux_model import FluxModel
from device_manager import DeviceManager
from config import Config
import uuid
import torch

logger = logging.getLogger(__name__)

@dataclass
class GenerationTask:
    """ç”Ÿæˆä»»åŠ¡"""
    task_id: str
    model_id: str
    prompt: str
    params: Dict[str, Any]
    result_queue: Queue
    created_at: float
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
    
    def __lt__(self, other):
        """æ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ—æ’åº"""
        if self.priority != other.priority:
            return self.priority < other.priority
        return self.created_at < other.created_at

class ModelInstance:
    """æ¨¡å‹å®ä¾‹ï¼Œç»‘å®šåˆ°ç‰¹å®šGPU"""
    def __init__(self, model: BaseModel, device: str, instance_id: str):
        self.model = model
        self.device = device
        self.instance_id = instance_id
        self.is_busy = False
        self.last_used = time.time()
        self.total_generations = 0
        self.current_task = None
        self.lock = threading.Lock()
        self.task_queue = PriorityQueue()  # ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—
        
        # ä»é…ç½®è·å–é˜Ÿåˆ—å¤§å°é™åˆ¶
        config = Config.get_config()
        self.max_queue_size = config["concurrent"]["max_gpu_queue_size"]
        
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®ä¾‹æ˜¯å¦å¯ç”¨"""
        with self.lock:
            return not self.is_busy and self.model.is_loaded
    
    def can_accept_task(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¥å—æ–°ä»»åŠ¡ï¼ˆè€ƒè™‘é˜Ÿåˆ—å¤§å°ï¼‰"""
        return self.task_queue.qsize() < self.max_queue_size
    
    def set_busy(self, busy: bool, task_id: Optional[str] = None):
        """è®¾ç½®å¿™ç¢ŒçŠ¶æ€"""
        with self.lock:
            self.is_busy = busy
            self.current_task = task_id if busy else None
            if not busy:
                self.last_used = time.time()
                self.total_generations += 1
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å®ä¾‹çŠ¶æ€"""
        with self.lock:
            return {
                "instance_id": self.instance_id,
                "device": self.device,
                "is_busy": self.is_busy,
                "is_loaded": self.model.is_loaded,
                "current_task": self.current_task,
                "queue_size": self.task_queue.qsize(),
                "max_queue_size": self.max_queue_size,
                "total_generations": self.total_generations,
                "last_used": self.last_used,
                "cuda_visible_devices": self.device.split(":")[1]
            }

class ConcurrentModelManager:
    """æ”¹è¿›çš„å¹¶å‘æ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒGPUç¯å¢ƒéš”ç¦»"""
    
    def __init__(self):
        self.device_manager = DeviceManager()
        self.model_instances: Dict[str, List[ModelInstance]] = {}  # model_id -> [instances]
        self.instance_lookup: Dict[str, ModelInstance] = {}  # instance_id -> instance
        
        # å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å’Œè°ƒåº¦
        self.global_task_queue = PriorityQueue()
        self.task_results: Dict[str, Queue] = {}  # task_id -> result_queue
        
        # å·¥ä½œçº¿ç¨‹ç®¡ç†
        self.worker_threads = []
        self.scheduler_thread = None
        self.is_running = False
        
        # ä»é…ç½®è·å–å‚æ•°
        config = Config.get_config()
        self.max_global_queue_size = config["concurrent"]["max_global_queue_size"]
        self.task_timeout = config["concurrent"]["task_timeout"]
        self.scheduler_sleep_time = config["concurrent"]["scheduler_sleep_time"]
        self.load_balance_strategy = config["concurrent"]["load_balance_strategy"]
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "queue_full_rejections": 0
        }
        
        self._initialize_models()
        self._start_manager()
    
    def _create_model_with_gpu_isolation(self, model_id: str, gpu_device: str, instance_id: str) -> Optional[BaseModel]:
        """åˆ›å»ºå…·æœ‰GPUéš”ç¦»çš„æ¨¡å‹å®ä¾‹ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_id == "flux1-dev":
                # ä¼ é€’åŸå§‹è®¾å¤‡åï¼Œä¸åœ¨è¿™é‡Œè®¾ç½®CUDA_VISIBLE_DEVICES
                model = FluxModel(gpu_device=gpu_device)
            else:
                logger.warning(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_id}")
                return None
            
            # åŠ è½½æ¨¡å‹
            logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹ {model_id} (å®ä¾‹: {instance_id})")
            if model.load():
                logger.info(f"âœ… æ¨¡å‹å®ä¾‹ {instance_id} åˆ›å»ºæˆåŠŸï¼Œç›®æ ‡è®¾å¤‡: {gpu_device}")
                return model
            else:
                logger.error(f"âŒ æ¨¡å‹å®ä¾‹ {model_id} åŠ è½½å¤±è´¥")
                return None
                    
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹å®ä¾‹"""
        model_gpu_config = Config.get_config()["model_management"]["model_gpu_config"]
        
        for model_id, gpu_list in model_gpu_config.items():
            if model_id not in self.model_instances:
                self.model_instances[model_id] = []
            
            # ä¸ºæ¯ä¸ªGPUåˆ›å»ºä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼ˆæ¯ä¸ªGPUåªåˆ›å»ºä¸€ä¸ªï¼‰
            for gpu_device in gpu_list:
                if self.device_manager.validate_device(gpu_device):
                    try:
                        # ç”Ÿæˆå”¯ä¸€å®ä¾‹ID
                        instance_id = f"{model_id}_{gpu_device.replace(':', '_')}"
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¿™ä¸ªå®ä¾‹
                        if instance_id in self.instance_lookup:
                            logger.warning(f"å®ä¾‹ {instance_id} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                            continue
                        
                        # åˆ›å»ºæ¨¡å‹å®ä¾‹
                        model = self._create_model_with_gpu_isolation(model_id, gpu_device, instance_id)
                        
                        if model:
                            instance = ModelInstance(model, gpu_device, instance_id)
                            self.model_instances[model_id].append(instance)
                            self.instance_lookup[instance_id] = instance
                            logger.info(f"âœ… æ¨¡å‹å®ä¾‹ {instance_id} åˆ›å»ºæˆåŠŸ")
                        else:
                            logger.error(f"âŒ æ¨¡å‹å®ä¾‹ {model_id} åœ¨ {gpu_device} ä¸ŠåŠ è½½å¤±è´¥")
                    except Exception as e:
                        logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
                        import traceback
                        logger.error(traceback.format_exc())
        
        # æ‰“å°åˆå§‹åŒ–ç»“æœ
        total_instances = sum(len(instances) for instances in self.model_instances.values())
        logger.info(f"ğŸ‰ æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œæ€»å…±åˆ›å»ºäº† {total_instances} ä¸ªå®ä¾‹")
        for model_id, instances in self.model_instances.items():
            logger.info(f"  {model_id}: {len(instances)} ä¸ªå®ä¾‹")
            for inst in instances:
                logger.info(f"    - {inst.instance_id} ({inst.device})")
    
    def _start_manager(self):
        """å¯åŠ¨ç®¡ç†å™¨"""
        self.is_running = True
        
        # å¯åŠ¨å…¨å±€è°ƒåº¦å™¨
        self.scheduler_thread = threading.Thread(
            target=self._global_scheduler_loop, 
            name="global-scheduler"
        )
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()
        logger.info("å…¨å±€è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        # ä¸ºæ¯ä¸ªGPUå®ä¾‹å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for model_id, instances in self.model_instances.items():
            for instance in instances:
                worker = threading.Thread(
                    target=self._gpu_worker_loop,
                    args=(instance,),
                    name=f"worker-{instance.instance_id}"
                )
                worker.daemon = True
                worker.start()
                self.worker_threads.append(worker)
                logger.info(f"GPUå·¥ä½œçº¿ç¨‹ {worker.name} å·²å¯åŠ¨")
    
    def _global_scheduler_loop(self):
        """å…¨å±€è°ƒåº¦å™¨å¾ªç¯ - æ™ºèƒ½ä»»åŠ¡åˆ†é…"""
        logger.info("å…¨å±€è°ƒåº¦å™¨å¼€å§‹è¿è¡Œ")
        
        while self.is_running:
            try:
                # è·å–å…¨å±€ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                task = self.global_task_queue.get(timeout=self.scheduler_sleep_time)
                
                # æ‰¾åˆ°æœ€ä½³çš„GPUå®ä¾‹
                best_instance = self._find_best_instance(task.model_id)
                
                if best_instance:
                    # ç«‹å³æ ‡è®°GPUä¸ºå¿™ç¢ŒçŠ¶æ€ï¼Œé˜²æ­¢é‡å¤åˆ†é…
                    best_instance.set_busy(True, task.task_id)
                    
                    # åˆ†é…ä»»åŠ¡ç»™GPU
                    best_instance.task_queue.put(task)
                    logger.info(f"ä»»åŠ¡ {task.task_id} å·²åˆ†é…ç»™ {best_instance.instance_id} (è®¾å¤‡: {best_instance.device})")
                else:
                    # æ‰€æœ‰GPUéƒ½å¿™ç¢Œæˆ–é˜Ÿåˆ—å·²æ»¡ï¼Œé‡æ–°æ”¾å›é˜Ÿåˆ—
                    self.global_task_queue.put(task)
                    # æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                    time.sleep(self.scheduler_sleep_time * 5)
                    
            except Empty:
                continue
            except Exception as e:
                logger.error(f"å…¨å±€è°ƒåº¦å™¨é”™è¯¯: {e}")
    
    def _find_best_instance(self, model_id: str) -> Optional[ModelInstance]:
        """æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹å®ä¾‹ - æ”¹è¿›çš„è´Ÿè½½å‡è¡¡ç®—æ³•"""
        if model_id not in self.model_instances:
            logger.warning(f"âš ï¸ æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
            return None
        
        instances = self.model_instances[model_id]
        
        # è¿‡æ»¤å¯æ¥å—ä»»åŠ¡çš„å®ä¾‹
        available_instances = [
            inst for inst in instances 
            if inst.model.is_loaded and inst.can_accept_task() and not inst.is_busy
        ]
        
        if not available_instances:
            logger.debug(f"âš ï¸ æ¨¡å‹ {model_id} æ²¡æœ‰å¯ç”¨å®ä¾‹")
            return None
        
        # é€‰æ‹©é˜Ÿåˆ—æœ€çŸ­çš„ç©ºé—²å®ä¾‹
        best = min(available_instances, key=lambda x: x.task_queue.qsize())
        logger.debug(f"âœ… é€‰æ‹©ç©ºé—²å®ä¾‹ {best.instance_id}ï¼Œé˜Ÿåˆ—å¤§å°: {best.task_queue.qsize()}")
        return best
    
    def _gpu_worker_loop(self, instance: ModelInstance):
        """GPUå·¥ä½œçº¿ç¨‹å¾ªç¯"""
        logger.info(f"GPUå·¥ä½œçº¿ç¨‹å¼€å§‹è¿è¡Œ: {instance.instance_id}")
        
        while self.is_running:
            try:
                # è·å–ä»»åŠ¡
                task = instance.task_queue.get(timeout=1.0)
                
                # å¤„ç†ä»»åŠ¡
                self._process_task_on_gpu(task, instance)
                
            except Empty:
                continue
            except Exception as e:
                logger.error(f"GPUå·¥ä½œçº¿ç¨‹ {instance.instance_id} é”™è¯¯: {e}")
    
    def _check_and_cleanup_memory(self, instance: ModelInstance, force_cleanup: bool = False):
        """æ£€æŸ¥å¹¶æ¸…ç†å†…å­˜ - GPUéš”ç¦»ç‰ˆæœ¬"""
        if not instance.device.startswith("cuda:"):
            return
        
        try:
            gpu_id = instance.device.split(":")[1]
            old_cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "")
            
            # è®¾ç½®GPUéš”ç¦»ç¯å¢ƒ
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            
            try:
                if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                    # åœ¨GPUéš”ç¦»ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯ä½¿ç”¨è®¾å¤‡0
                    with torch.cuda.device(0):
                        allocated = torch.cuda.memory_allocated(0)
                        total = torch.cuda.get_device_properties(0).total_memory
                        usage_ratio = allocated / total
                        
                        logger.debug(f"GPU {gpu_id} (éš”ç¦»ç¯å¢ƒ) å†…å­˜ä½¿ç”¨ç‡: {usage_ratio:.1%}")
                        
                        # å¦‚æœå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡95%æˆ–è€…å¼ºåˆ¶æ¸…ç†ï¼Œæ‰§è¡Œæ·±åº¦æ¸…ç†
                        if usage_ratio > 0.95 or force_cleanup:
                            logger.warning(f"GPU {gpu_id} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_ratio:.1%})ï¼Œæ‰§è¡Œæ·±åº¦æ¸…ç†")
                            
                            # å…ˆå°è¯•æ¨¡å‹çš„ç´§æ€¥æ¸…ç†
                            if hasattr(instance.model, '_emergency_cleanup'):
                                instance.model._emergency_cleanup()
                            else:
                                # å¤‡ç”¨æ¸…ç†æ–¹æ³•
                                torch.cuda.empty_cache()
                                torch.cuda.reset_peak_memory_stats()
                                import gc
                                gc.collect()
                                torch.cuda.empty_cache()
                                torch.cuda.synchronize()
                            
                            # æ£€æŸ¥æ¸…ç†æ•ˆæœ
                            new_allocated = torch.cuda.memory_allocated(0)
                            new_usage_ratio = new_allocated / total
                            logger.info(f"GPU {gpu_id} æ¸…ç†åå†…å­˜ä½¿ç”¨ç‡: {new_usage_ratio:.1%}")
                            
            finally:
                # æ¢å¤ç¯å¢ƒå˜é‡
                if old_cuda_visible:
                    os.environ["CUDA_VISIBLE_DEVICES"] = old_cuda_visible
                else:
                    os.environ.pop("CUDA_VISIBLE_DEVICES", None)
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å†…å­˜æ—¶å‡ºé”™: {e}")
    
    def _process_task_on_gpu(self, task: GenerationTask, instance: ModelInstance):
        """åœ¨GPUä¸Šå¤„ç†ä»»åŠ¡ - åŠ¨æ€GPUéš”ç¦»ç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡ {task.task_id[:8]} (å®ä¾‹: {instance.instance_id})")
        
        # è®¾ç½®GPUéš”ç¦»ç¯å¢ƒå˜é‡
        old_cuda_visible = None
        gpu_id = None
        
        if instance.device.startswith("cuda:"):
            gpu_id = instance.device.split(":")[1]
            old_cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "")
            if gpu_id is not None:
                os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
                logger.debug(f"è®¾ç½®GPUéš”ç¦»ç¯å¢ƒ: CUDA_VISIBLE_DEVICES={gpu_id}")
        
        try:
            # æ›´æ–°æ¨¡å‹è®¾å¤‡é…ç½®
            if hasattr(instance.model, '_update_device_for_task'):
                instance.model._update_device_for_task(instance.device)
            
            # æ‰§è¡Œç”Ÿæˆä»»åŠ¡
            result = instance.model.generate(task.prompt, **task.params)
            
            # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
            result.update({
                "task_id": task.task_id,
                "device": instance.device,
                "instance_id": instance.instance_id,
                "cuda_visible_devices": gpu_id if instance.device.startswith("cuda:") else "cpu",
                "physical_gpu": gpu_id if instance.device.startswith("cuda:") else "cpu"
            })
            
            # å‘é€ç»“æœ
            task.result_queue.put(result)
            
            # æ›´æ–°ç»Ÿè®¡
            if result.get("success", False):
                self.stats["completed_tasks"] += 1
                instance.total_generations += 1
                logger.info(f"âœ… ä»»åŠ¡ {task.task_id[:8]} å®Œæˆ (å®ä¾‹: {instance.instance_id})")
            else:
                self.stats["failed_tasks"] += 1
                logger.error(f"âŒ ä»»åŠ¡ {task.task_id[:8]} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ä»»åŠ¡ {task.task_id[:8]} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # å¤±è´¥æ—¶æ‰§è¡Œå¼ºåˆ¶æ¸…ç†
            try:
                if instance.device.startswith("cuda:"):
                    logger.warning(f"ä»»åŠ¡å¤±è´¥ï¼Œå¯¹GPU {instance.device} æ‰§è¡Œå¼ºåˆ¶æ¸…ç†")
                    self._check_and_cleanup_memory(instance, force_cleanup=True)
            except Exception as cleanup_error:
                logger.error(f"å¼ºåˆ¶æ¸…ç†GPUæ˜¾å­˜æ—¶å‡ºé”™: {cleanup_error}")
            
            result = {
                "success": False,
                "error": str(e),
                "task_id": task.task_id,
                "device": instance.device,
                "instance_id": instance.instance_id,
                "cuda_visible_devices": gpu_id if instance.device.startswith("cuda:") else "cpu",
                "physical_gpu": gpu_id if instance.device.startswith("cuda:") else "cpu"
            }
            task.result_queue.put(result)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["failed_tasks"] += 1
            
        finally:
            # æ¢å¤GPUç¯å¢ƒå˜é‡
            if old_cuda_visible is not None:
                if old_cuda_visible:
                    os.environ["CUDA_VISIBLE_DEVICES"] = old_cuda_visible
                else:
                    os.environ.pop("CUDA_VISIBLE_DEVICES", None)
                logger.debug(f"æ¢å¤ CUDA_VISIBLE_DEVICES={old_cuda_visible}")
            
            # é‡Šæ”¾GPU
            instance.set_busy(False)
            
            # ä»»åŠ¡å®Œæˆåçš„æ ‡å‡†æ¸…ç†
            try:
                if instance.device.startswith("cuda:") and gpu_id is not None:
                    # åœ¨GPUéš”ç¦»ç¯å¢ƒä¸­è¿›è¡Œæ¸…ç†
                    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
                    if torch.cuda.is_available():
                        with torch.cuda.device(0):  # åœ¨éš”ç¦»ç¯å¢ƒä¸­æ€»æ˜¯ä½¿ç”¨è®¾å¤‡0
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                    # æ¢å¤ç¯å¢ƒå˜é‡
                    if old_cuda_visible:
                        os.environ["CUDA_VISIBLE_DEVICES"] = old_cuda_visible
                    else:
                        os.environ.pop("CUDA_VISIBLE_DEVICES", None)
                    logger.debug(f"å·²æ¸…ç†GPU {instance.device} ç¼“å­˜")
            except Exception as e:
                logger.warning(f"æ¸…ç†GPUç¼“å­˜æ—¶å‡ºé”™: {e}")
    
    async def generate_image_async(self, model_id: str, prompt: str, priority: int = 0, **kwargs) -> Dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡ - æ”¯æŒä¼˜å…ˆçº§"""
        task_id = str(uuid.uuid4())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨å®ä¾‹
        if model_id not in self.model_instances or not self.model_instances[model_id]:
            return {
                "success": False,
                "error": f"æ¨¡å‹ {model_id} ä¸å¯ç”¨",
                "task_id": task_id
            }
        
        # æ£€æŸ¥å…¨å±€é˜Ÿåˆ—æ˜¯å¦è¿‡è½½
        if self.global_task_queue.qsize() > self.max_global_queue_size:
            self.stats["queue_full_rejections"] += 1
            return {
                "success": False,
                "error": "æœåŠ¡å™¨è¿‡è½½ï¼Œè¯·ç¨åé‡è¯•",
                "task_id": task_id
            }
        
        # åˆ›å»ºç»“æœé˜Ÿåˆ—
        result_queue = Queue()
        self.task_results[task_id] = result_queue
        
        # åˆ›å»ºä»»åŠ¡
        task = GenerationTask(
            task_id=task_id,
            model_id=model_id,
            prompt=prompt,
            params=kwargs,
            result_queue=result_queue,
            created_at=time.time(),
            priority=priority
        )
        
        # æ·»åŠ åˆ°å…¨å±€ä»»åŠ¡é˜Ÿåˆ—
        self.global_task_queue.put(task)
        self.stats["total_tasks"] += 1
        
        logger.info(f"ä»»åŠ¡ {task_id} å·²åŠ å…¥é˜Ÿåˆ—ï¼Œä¼˜å…ˆçº§: {priority}")
        
        try:
            # ç­‰å¾…ç»“æœï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
            result = await asyncio.get_event_loop().run_in_executor(
                None, result_queue.get, True, self.task_timeout
            )
            return result
            
        except Exception as e:
            logger.error(f"ç­‰å¾…ä»»åŠ¡ {task_id} ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"ä»»åŠ¡è¶…æ—¶æˆ–å‘ç”Ÿé”™è¯¯: {str(e)}",
                "task_id": task_id
            }
        finally:
            # æ¸…ç†ç»“æœé˜Ÿåˆ—
            self.task_results.pop(task_id, None)
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çŠ¶æ€"""
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_queue_size = sum(
            inst.task_queue.qsize() 
            for instances in self.model_instances.values() 
            for inst in instances
        )
        
        busy_instances = sum(
            1 for instances in self.model_instances.values() 
            for inst in instances if inst.is_busy
        )
        
        total_instances = sum(
            len(instances) for instances in self.model_instances.values()
        )
        
        return {
            "is_running": self.is_running,
            "global_queue_size": self.global_task_queue.qsize(),
            "max_global_queue_size": self.max_global_queue_size,
            "total_queue_size": total_queue_size,
            "worker_threads": len(self.worker_threads),
            "busy_instances": busy_instances,
            "total_instances": total_instances,
            "load_balance_strategy": self.load_balance_strategy,
            "task_timeout": self.task_timeout,
            "stats": self.stats.copy(),
            "model_instances": {
                model_id: [inst.get_status() for inst in instances]
                for model_id, instances in self.model_instances.items()
            }
        }
    
    def get_model_list(self) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        models = []
        for model_id, instances in self.model_instances.items():
            if instances:
                first_instance = instances[0]
                model_info = first_instance.model.get_info()
                model_info.update({
                    "total_instances": len(instances),
                    "available_instances": len([inst for inst in instances if inst.is_available()]),
                    "busy_instances": len([inst for inst in instances if inst.is_busy]),
                    "total_queue_size": sum(inst.task_queue.qsize() for inst in instances)
                })
                models.append(model_info)
        return models
    
    def shutdown(self):
        """å…³é—­ç®¡ç†å™¨ - å¢å¼ºç‰ˆæœ¬"""
        logger.info("æ­£åœ¨å…³é—­å¹¶å‘æ¨¡å‹ç®¡ç†å™¨...")
        
        self.is_running = False
        
        # ç­‰å¾…è°ƒåº¦å™¨çº¿ç¨‹ç»“æŸ
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5.0)
        
        # ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸ
        for worker in self.worker_threads:
            worker.join(timeout=5.0)
        
        # å½»åº•å¸è½½æ‰€æœ‰æ¨¡å‹
        for instances in self.model_instances.values():
            for instance in instances:
                try:
                    logger.info(f"å¸è½½æ¨¡å‹å®ä¾‹: {instance.instance_id}")
                    instance.model.unload()
                    
                    # å¼ºåˆ¶æ¸…ç†è¿™ä¸ªå®ä¾‹ä½¿ç”¨çš„GPU
                    if instance.device.startswith("cuda:"):
                        gpu_id = int(instance.device.split(":")[1])
                        with torch.cuda.device(gpu_id):
                            torch.cuda.empty_cache()
                            torch.cuda.reset_peak_memory_stats()
                            torch.cuda.synchronize()
                        logger.info(f"å·²æ¸…ç†GPU {instance.device}")
                        
                except Exception as e:
                    logger.error(f"å¸è½½å®ä¾‹ {instance.instance_id} æ—¶å‡ºé”™: {e}")
        
        # æœ€ç»ˆæ¸…ç†æ‰€æœ‰GPU
        try:
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                        torch.cuda.synchronize()
                logger.info("å·²æ¸…ç†æ‰€æœ‰GPU")
        except Exception as e:
            logger.warning(f"æœ€ç»ˆGPUæ¸…ç†æ—¶å‡ºé”™: {e}")
        
        logger.info("å¹¶å‘æ¨¡å‹ç®¡ç†å™¨å·²å…³é—­") 
import os
import subprocess
import multiprocessing as mp
from typing import Dict, Any, Optional, Callable
import logging
import time
import signal
import sys
import traceback
import queue
import gc
import psutil

# mp.set_start_method('spawn', force=True)

logger = logging.getLogger(__name__)

def gpu_worker_process(gpu_id: str, model_path: str, model_id: str, task_queue, result_queue):
    """GPUå·¥ä½œè¿›ç¨‹ - åœ¨éš”ç¦»ç¯å¢ƒä¸­è¿è¡Œï¼ˆé¡¶å±‚å‡½æ•°ï¼Œæ”¯æŒspawnï¼‰"""
    import torch
    from diffusers import FluxPipeline, FluxImg2ImgPipeline, FluxFillPipeline, FluxControlPipeline
    import base64
    import io
    from PIL import Image
    import traceback
    import queue
    import logging
    import time
    import gc
    import psutil
    import os
    
    # å°è¯•å¯¼å…¥PEFTç›¸å…³åº“
    try:
        from peft import PeftModel
        PEFT_AVAILABLE = True
    except ImportError:
        PEFT_AVAILABLE = False
        logging.warning(f"PEFTåº“æœªå®‰è£…ï¼ŒLoRAåŠŸèƒ½å°†ä¸å¯ç”¨")
    
    # æ³¨æ„ï¼šPeftConfigåœ¨æ–°ç‰ˆæœ¬transformersä¸­å¯èƒ½ä¸å¯ç”¨ï¼Œæˆ‘ä»¬åªéœ€è¦PEFTåº“æœ¬èº«
    TRANSFORMERS_PEFT_AVAILABLE = True  # ç®€åŒ–æ£€æŸ¥ï¼Œä¸»è¦ä¾èµ–PEFTåº“
    
    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
    
    def load_image_from_base64(image_data: str):
        """ä»base64åŠ è½½å›¾ç‰‡"""
        if not image_data:
            raise ValueError("å›¾ç‰‡æ•°æ®ä¸ºç©º")
        
        try:
            # å¤„ç†data URLæ ¼å¼
            if image_data.startswith('data:image'):
                image_data = image_data.split(',')[1]
            
            # è§£ç base64
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            # è½¬æ¢ä¸ºRGBæ¨¡å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
        except Exception as e:
            raise ValueError(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {str(e)}")
    
    # è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§å’Œå†…å­˜é™åˆ¶
    try:
        process = psutil.Process()
        process.nice(10)  # é™ä½è¿›ç¨‹ä¼˜å…ˆçº§ï¼Œå‡å°‘è¢«OOM Killeræ€æ­»çš„æ¦‚ç‡
        logger.info(f"ğŸš€ GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()}, ä¼˜å…ˆçº§: {process.nice()})")
    except Exception as e:
        logger.warning(f"æ— æ³•è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§: {e}")
        logger.info(f"ğŸš€ GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()})")
    
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°GPU {gpu_id}...")
        
        # åŠ è½½å‰æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # æ ¹æ®æ¨¡å‹IDç¡®å®šå®é™…çš„æ¨¡å‹è·¯å¾„
        from config import Config
        model_paths = Config.get_model_paths()
        actual_model_path = model_paths.get(model_id, model_path)
        
        logger.info(f"GPU {gpu_id} ä½¿ç”¨æ¨¡å‹è·¯å¾„: {actual_model_path}")
        
        # åˆå§‹åŒ–pipelineå­—å…¸ - æ”¯æŒå¤šç§controlnetç±»å‹
        pipelines = {
            "text2img": None,
            "img2img": None,
            "fill": None,
            "controlnet_depth": None,
            "controlnet_canny": None,
            "controlnet_openpose": None
        }
        
        # åŠ è½½åŸºç¡€text2img pipeline
        pipelines["text2img"] = FluxPipeline.from_pretrained(
            actual_model_path,
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
            local_files_only=True
        )
        pipelines["text2img"].enable_model_cpu_offload(device="cuda:0")
        logger.info(f"âœ… åŸºç¡€æ¨¡å‹å·²åŠ è½½åˆ°GPU {gpu_id}")
        
        task_count = 0
        consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
        max_consecutive_failures = 3  # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
        last_cleanup_time = time.time()  # ä¸Šæ¬¡æ¸…ç†æ—¶é—´
        
        while True:
            try:
                # å®šæœŸå†…å­˜æ¸…ç†
                current_time = time.time()
                if current_time - last_cleanup_time > Config.GPU_MEMORY_CLEANUP_INTERVAL:
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated() / 1024**2
                        if allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                            logger.info(f"GPU {gpu_id} å®šæœŸæ¸…ç†å†…å­˜ (å·²åˆ†é…: {allocated:.1f}MB)")
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            gc.collect()
                    last_cleanup_time = current_time
                
                task = task_queue.get(timeout=1.0)
                if task is None:
                    logger.info(f"GPU {gpu_id} æ”¶åˆ°é€€å‡ºä¿¡å·")
                    break
                
                task_count += 1
                logger.info(f"GPU {gpu_id} å¼€å§‹å¤„ç†ä»»åŠ¡ #{task_count}: {task.get('task_id', 'unknown')}")
                
                # ä»»åŠ¡å¼€å§‹å‰æ£€æŸ¥å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    initial_allocated = torch.cuda.memory_allocated() / 1024**2
                    initial_cached = torch.cuda.memory_reserved() / 1024**2
                    logger.info(f"GPU {gpu_id} ä»»åŠ¡å¼€å§‹å‰å†…å­˜: å·²åˆ†é… {initial_allocated:.1f}MB, ç¼“å­˜ {initial_cached:.1f}MB")
                    
                    # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå¼ºåˆ¶æ¸…ç†
                    if initial_allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                        logger.warning(f"GPU {gpu_id} å†…å­˜ä½¿ç”¨è¿‡é«˜ ({initial_allocated:.1f}MB > {Config.GPU_MEMORY_THRESHOLD_MB}MB)ï¼Œå¼ºåˆ¶æ¸…ç†")
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        gc.collect()
                
                # å¤„ç†ä»»åŠ¡
                result = process_generation_task(pipelines, task, gpu_id, load_image_from_base64, actual_model_path)
                result_queue.put(result)
                
                success = result.get('success', False)
                logger.info(f"GPU {gpu_id} ä»»åŠ¡ #{task_count} å¤„ç†å®Œæˆ: {success}")
                
                # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°
                if success:
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                
                # ä»»åŠ¡å®Œæˆåè¿›è¡Œæ¸…ç†å’Œç­‰å¾…
                if success:
                    logger.info(f"GPU {gpu_id} å¼€å§‹æ¸…ç†èµ„æº...")
                    
                    # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
                    if torch.cuda.is_available():
                        # å¤šæ¬¡æ¸…ç†ç¡®ä¿å½»åº•
                        for i in range(3):
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            time.sleep(0.1)
                        
                        torch.cuda.reset_peak_memory_stats()
                        
                        # å¦‚æœå¯ç”¨æ¿€è¿›æ¸…ç†ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†
                        if Config.ENABLE_AGGRESSIVE_CLEANUP:
                            # å¼ºåˆ¶åƒåœ¾å›æ”¶å¤šæ¬¡
                            for i in range(2):
                                gc.collect()
                                time.sleep(0.05)
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ¸…ç†å®Œæˆ
                    cleanup_wait_time = Config.GPU_TASK_CLEANUP_WAIT_TIME
                    logger.info(f"GPU {gpu_id} ç­‰å¾… {cleanup_wait_time} ç§’ç¡®ä¿æ¸…ç†å®Œæˆ...")
                    time.sleep(cleanup_wait_time)
                    
                    # è®°å½•æ¸…ç†åçš„å†…å­˜çŠ¶æ€
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated() / 1024**2
                        cached = torch.cuda.memory_reserved() / 1024**2
                        logger.info(f"GPU {gpu_id} æ¸…ç†åå†…å­˜: å·²åˆ†é… {allocated:.1f}MB, ç¼“å­˜ {cached:.1f}MB")
                        
                        # å¦‚æœå†…å­˜ä»ç„¶è¿‡é«˜ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†
                        if allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                            logger.warning(f"GPU {gpu_id} æ¸…ç†åå†…å­˜ä»ç„¶è¿‡é«˜ ({allocated:.1f}MB)ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†")
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            gc.collect()
                    
                    logger.info(f"GPU {gpu_id} æ¸…ç†å®Œæˆï¼Œå‡†å¤‡æ¥æ”¶ä¸‹ä¸€ä¸ªä»»åŠ¡")
                else:
                    logger.warning(f"GPU {gpu_id} ä»»åŠ¡å¤±è´¥ï¼Œè·³è¿‡æ¸…ç†ç­‰å¾…")
                    
                    # å¤±è´¥åä¹Ÿè¦æ¸…ç†å†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        gc.collect()
                
                # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"GPU {gpu_id} è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œå‡†å¤‡é‡å¯è¿›ç¨‹")
                    break
                
            except queue.Empty:
                continue
            except Exception as e:
                error_msg = f"GPU {gpu_id} å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                
                consecutive_failures += 1
                
                try:
                    result_queue.put({
                        "success": False,
                        "error": str(e),
                        "gpu_id": gpu_id,
                        "task_id": task.get('task_id') if 'task' in locals() else 'unknown'
                    })
                except Exception as put_error:
                    logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›é”™è¯¯ç»“æœ: {put_error}")
                
                # å¼‚å¸¸åæ¸…ç†å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    gc.collect()
        
        logger.info(f"GPU {gpu_id} å¼€å§‹æœ€ç»ˆæ¸…ç†èµ„æº...")
        # æ¸…ç†æ‰€æœ‰pipeline
        for pipeline in pipelines.values():
            if pipeline is not None:
                del pipeline
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        logger.info(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹é€€å‡º")
    except Exception as e:
        error_msg = f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"å¯åŠ¨é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        try:
            result_queue.put({
                "success": False,
                "error": f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}",
                "gpu_id": gpu_id
            })
        except Exception as put_error:
            logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›å¯åŠ¨å¤±è´¥ç»“æœ: {put_error}")

def process_generation_task(pipelines, task, gpu_id: str, load_image_func, model_path: str):
    import torch
    import io
    import base64
    import time
    import traceback
    from PIL import Image
    import os
    from diffusers import FluxPipeline, FluxImg2ImgPipeline, FluxFillPipeline, FluxControlPipeline
    
    # å°è¯•å¯¼å…¥PEFTç›¸å…³åº“
    try:
        from peft import PeftModel
        PEFT_AVAILABLE = True
    except ImportError:
        PEFT_AVAILABLE = False
        logging.warning(f"PEFTåº“æœªå®‰è£…ï¼ŒLoRAåŠŸèƒ½å°†ä¸å¯ç”¨")
    
    # æ³¨æ„ï¼šPeftConfigåœ¨æ–°ç‰ˆæœ¬transformersä¸­å¯èƒ½ä¸å¯ç”¨ï¼Œæˆ‘ä»¬åªéœ€è¦PEFTåº“æœ¬èº«
    TRANSFORMERS_PEFT_AVAILABLE = True  # ç®€åŒ–æ£€æŸ¥ï¼Œä¸»è¦ä¾èµ–PEFTåº“
    
    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    start_time = time.time()
    
    def create_safe_adapter_name(lora_name: str, index: int, timestamp: int, random_suffix: int) -> str:
        """åˆ›å»ºå®‰å…¨çš„adapteråç§°"""
        # ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        import re
        safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', lora_name)
        # ç¡®ä¿ä¸ä»¥æ•°å­—å¼€å¤´
        if safe_name and safe_name[0].isdigit():
            safe_name = 'lora_' + safe_name
        # é™åˆ¶é•¿åº¦
        if len(safe_name) > 50:
            safe_name = safe_name[:50]
        return f"lora_{index}_{safe_name}_{timestamp}_{random_suffix}"
    
    # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶çš„å†…å­˜çŠ¶æ€
    if torch.cuda.is_available():
        initial_allocated = torch.cuda.memory_allocated() / 1024**2
        initial_cached = torch.cuda.memory_reserved() / 1024**2
        logger.info(f"GPU {gpu_id} ä»»åŠ¡å¼€å§‹å‰å†…å­˜: å·²åˆ†é… {initial_allocated:.1f}MB, ç¼“å­˜ {initial_cached:.1f}MB")
    
    try:
        logger.info(f"GPU {gpu_id} å¼€å§‹ç”Ÿæˆä»»åŠ¡: {task.get('task_id', 'unknown')}")
        generator = torch.Generator("cpu").manual_seed(task.get('seed', 42))
        
        # è·å–ç”Ÿæˆæ¨¡å¼å’Œæ¨¡å‹ID
        mode = task.get('mode', 'text2img')
        model_id = task.get('model_id', 'flux1-dev')
        logger.info(f"GPU {gpu_id} ä½¿ç”¨æ¨¡å¼: {mode}, æ¨¡å‹: {model_id}")
        
        # æ ¹æ®æ¨¡å‹IDç¡®å®šå®é™…çš„æ¨¡å‹è·¯å¾„
        from config import Config
        model_paths = Config.get_model_paths()
        actual_model_path = model_paths.get(model_id, model_path)
        
        # é€šç”¨å°ºå¯¸å¤„ç†é€»è¾‘
        def get_output_dimensions(input_image, mode):
            """è·å–è¾“å‡ºå°ºå¯¸ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šï¼Œå¦åˆ™ä½¿ç”¨å›¾ç‰‡åŸå§‹å°ºå¯¸ï¼Œå¹¶ç¡®ä¿æ˜¯16çš„å€æ•°"""
            user_height = task.get('height')
            user_width = task.get('width')
            original_width, original_height = input_image.size  # PILè¿”å›(width, height)
            
            # æ£€æŸ¥æ˜¯å¦ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†å°ºå¯¸ï¼ˆä¸æ˜¯é»˜è®¤å€¼ï¼‰
            if user_height == 1024 and user_width == 1024 and mode != 'text2img':
                height = original_height
                width = original_width
                logger.info(f"GPU {gpu_id} ä½¿ç”¨å›¾ç‰‡åŸå§‹å°ºå¯¸: {width}x{height}")
            else:
                height = user_height
                width = user_width
                logger.info(f"GPU {gpu_id} ä½¿ç”¨ç”¨æˆ·æŒ‡å®šå°ºå¯¸: {width}x{height}")
            
            # ç¡®ä¿å°ºå¯¸æ˜¯16çš„å€æ•°ï¼ˆControlNetè¦æ±‚ï¼‰
            if mode == 'controlnet':
                # Flux ControlNetå¯¹å°ºå¯¸æœ‰ç‰¹æ®Šè¦æ±‚
                # ä½¿ç”¨æ ‡å‡†å°ºå¯¸ï¼š512x512, 768x768, 1024x1024
                # é¿å…ä½¿ç”¨éæ ‡å‡†å°ºå¯¸ï¼Œå¯èƒ½å¯¼è‡´latent packingé”™è¯¯
                
                # è®¡ç®—æœ€æ¥è¿‘çš„æ ‡å‡†å°ºå¯¸
                target_size = max(width, height)
                if target_size <= 512:
                    width = height = 512
                elif target_size <= 768:
                    width = height = 768
                else:
                    width = height = 1024
                
                logger.info(f"GPU {gpu_id} ControlNetä½¿ç”¨æ ‡å‡†å°ºå¯¸: {width}x{height}")
            
            return width, height
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æˆ–åŠ è½½pipeline
        if mode == 'text2img':
            pipe = pipelines["text2img"]
        elif mode == 'img2img':
            if pipelines["img2img"] is None:
                logger.info(f"GPU {gpu_id} åŠ è½½img2img pipeline...")
                
                # ä½¿ç”¨ä¸“é—¨çš„æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                model_paths = Config.get_model_paths()
                img2img_model_path = model_paths.get("flux1-dev", actual_model_path)  # img2imgä½¿ç”¨åŸºç¡€æ¨¡å‹
                
                if not os.path.exists(img2img_model_path):
                    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼ å…¥çš„è·¯å¾„
                    img2img_model_path = actual_model_path
                    logger.warning(f"GPU {gpu_id} img2imgæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼ å…¥è·¯å¾„: {img2img_model_path}")
                else:
                    logger.info(f"GPU {gpu_id} ä½¿ç”¨img2imgæ¨¡å‹è·¯å¾„: {img2img_model_path}")
                
                try:
                    pipelines["img2img"] = FluxImg2ImgPipeline.from_pretrained(
                        img2img_model_path,
                        torch_dtype=torch.bfloat16,
                        use_safetensors=True,
                        local_files_only=True
                    )
                    pipelines["img2img"].enable_model_cpu_offload(device="cuda:0")
                    logger.info(f"GPU {gpu_id} img2img pipelineåŠ è½½å®Œæˆ")
                except Exception as e:
                    logger.error(f"GPU {gpu_id} åŠ è½½img2img pipelineå¤±è´¥: {e}")
                    # å¦‚æœimg2imgåŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€pipeline
                    logger.info(f"GPU {gpu_id} å°è¯•ä½¿ç”¨åŸºç¡€pipelineè¿›è¡Œimg2imgæ“ä½œ")
                    pipelines["img2img"] = pipelines["text2img"]
            pipe = pipelines["img2img"]
        elif mode == 'fill':
            if pipelines["fill"] is None:
                logger.info(f"GPU {gpu_id} åŠ è½½fill pipeline...")
                
                # ä½¿ç”¨ä¸“é—¨çš„Fillæ¨¡å‹è·¯å¾„
                model_paths = Config.get_model_paths()
                fill_model_path = model_paths.get("flux1-fill-dev", actual_model_path)
                
                if not os.path.exists(fill_model_path):
                    # å¦‚æœä¸“ç”¨Fillæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€è·¯å¾„
                    fill_model_path = actual_model_path
                    logger.warning(f"GPU {gpu_id} ä¸“ç”¨Fillæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {fill_model_path}")
                else:
                    logger.info(f"GPU {gpu_id} ä½¿ç”¨Fillæ¨¡å‹è·¯å¾„: {fill_model_path}")
                
                try:
                    pipelines["fill"] = FluxFillPipeline.from_pretrained(
                        fill_model_path,
                        torch_dtype=torch.bfloat16,
                        use_safetensors=True,
                        local_files_only=True
                    )
                    pipelines["fill"].enable_model_cpu_offload(device="cuda:0")
                    logger.info(f"GPU {gpu_id} fill pipelineåŠ è½½å®Œæˆ")
                except Exception as e:
                    logger.error(f"GPU {gpu_id} åŠ è½½fill pipelineå¤±è´¥: {e}")
                    # å¦‚æœfillåŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€pipeline
                    logger.info(f"GPU {gpu_id} å°è¯•ä½¿ç”¨åŸºç¡€pipelineè¿›è¡Œfillæ“ä½œ")
                    pipelines["fill"] = pipelines["text2img"]
            pipe = pipelines["fill"]
        elif mode == 'controlnet':
            # è·å–controlnetç±»å‹
            controlnet_type = task.get('controlnet_type', 'depth').lower()
            pipeline_key = f"controlnet_{controlnet_type}"
            
            if pipeline_key not in pipelines:
                raise ValueError(f"ä¸æ”¯æŒçš„controlnetç±»å‹: {controlnet_type}")
            
            if pipelines[pipeline_key] is None:
                logger.info(f"GPU {gpu_id} åŠ è½½{controlnet_type} controlnet pipeline...")
                
                # æ ¹æ®ç±»å‹é€‰æ‹©æ¨¡å‹è·¯å¾„
                model_paths = Config.get_model_paths()
                if controlnet_type == 'depth':
                    controlnet_model_path = model_paths.get("flux1-depth-dev", actual_model_path)
                elif controlnet_type == 'canny':
                    controlnet_model_path = model_paths.get("flux1-canny-dev", actual_model_path)
                elif controlnet_type == 'openpose':
                    controlnet_model_path = model_paths.get("flux1-openpose-dev", actual_model_path)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„controlnetç±»å‹: {controlnet_type}")
                
                if not os.path.exists(controlnet_model_path):
                    # å¦‚æœä¸“ç”¨è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€è·¯å¾„
                    controlnet_model_path = actual_model_path
                    logger.warning(f"GPU {gpu_id} ä¸“ç”¨{controlnet_type} controlnetæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {controlnet_model_path}")
                else:
                    logger.info(f"GPU {gpu_id} ä½¿ç”¨{controlnet_type} controlnetæ¨¡å‹è·¯å¾„: {controlnet_model_path}")
                
                try:
                    pipelines[pipeline_key] = FluxControlPipeline.from_pretrained(
                        controlnet_model_path,
                        torch_dtype=torch.bfloat16,
                        use_safetensors=True,
                        local_files_only=True
                    )
                    pipelines[pipeline_key].enable_model_cpu_offload(device="cuda:0")
                    logger.info(f"GPU {gpu_id} {controlnet_type} controlnet pipelineåŠ è½½å®Œæˆ")
                except Exception as e:
                    logger.error(f"GPU {gpu_id} åŠ è½½{controlnet_type} controlnet pipelineå¤±è´¥: {e}")
                    # å¦‚æœcontrolnetåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€pipeline
                    logger.info(f"GPU {gpu_id} å›é€€åˆ°åŸºç¡€pipeline")
                    pipelines[pipeline_key] = pipelines["text2img"]
            
            pipe = pipelines[pipeline_key]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç”Ÿæˆæ¨¡å¼: {mode}")
        
        # åŠ è½½å’Œåº”ç”¨LoRA
        loras = task.get('loras', [])
        if loras:
            if not PEFT_AVAILABLE:
                raise ValueError("PEFTåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨LoRAåŠŸèƒ½ã€‚è¯·å®‰è£…: pip install peft")
            
            logger.info(f"GPU {gpu_id} å¼€å§‹åŠ è½½ {len(loras)} ä¸ªLoRA...")
            
            adapter_names = []
            adapter_weights = []
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œéšæœºæ•°ç¡®ä¿adapter_nameå”¯ä¸€æ€§
            import time
            import random
            timestamp = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³
            random_suffix = random.randint(1000, 9999)  # 4ä½éšæœºæ•°
            
            for i, lora in enumerate(loras):
                lora_name = lora.get('name')
                lora_weight = lora.get('weight', 1.0)
                
                # è·å–LoRAè·¯å¾„
                lora_path = Config.get_lora_path(lora_name)
                if not lora_path:
                    raise ValueError(f"LoRA '{lora_name}' ä¸å­˜åœ¨")
                
                # ç”Ÿæˆå”¯ä¸€çš„adapter_name
                adapter_name = create_safe_adapter_name(lora_name, i, timestamp, random_suffix)
                adapter_names.append(adapter_name)
                adapter_weights.append(lora_weight)
                
                try:
                    logger.info(f"GPU {gpu_id} åŠ è½½LoRA: {lora_name} (æƒé‡: {lora_weight}) -> adapter: {adapter_name}")
                    
                    # æ£€æŸ¥LoRAæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(lora_path):
                        raise ValueError(f"LoRAæ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
                    
                    # å°è¯•åŠ è½½LoRA
                    pipe.load_lora_weights(lora_path, adapter_name=adapter_name)
                    logger.info(f"GPU {gpu_id} LoRA {lora_name} åŠ è½½æˆåŠŸ")
                    
                except Exception as e:
                    error_msg = str(e)
                    if "PEFT backend is required" in error_msg:
                        raise ValueError(f"PEFTåç«¯æœªæ­£ç¡®é…ç½®ã€‚è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„PEFTç‰ˆæœ¬: pip install peft transformers")
                    elif "not a valid LoRA" in error_msg:
                        raise ValueError(f"LoRAæ–‡ä»¶æ ¼å¼æ— æ•ˆæˆ–ä¸å½“å‰æ¨¡å‹ä¸å…¼å®¹: {lora_name}")
                    elif "already in use" in error_msg:
                        raise ValueError(f"LoRAé€‚é…å™¨åç§°å†²çªï¼Œè¯·é‡è¯•: {lora_name}")
                    else:
                        raise ValueError(f"åŠ è½½LoRA {lora_name} å¤±è´¥: {error_msg}")
            
            # è®¾ç½®LoRAé€‚é…å™¨
            if adapter_names:
                try:
                    pipe.set_adapters(adapter_names, adapter_weights=adapter_weights)
                    logger.info(f"GPU {gpu_id} è®¾ç½®LoRAé€‚é…å™¨: {adapter_names} (æƒé‡: {adapter_weights})")
                except Exception as e:
                    logger.error(f"GPU {gpu_id} è®¾ç½®LoRAé€‚é…å™¨å¤±è´¥: {e}")
                    raise ValueError(f"è®¾ç½®LoRAé€‚é…å™¨å¤±è´¥: {str(e)}")
        
        with torch.no_grad():
            if mode == 'text2img':
                result = pipe(
                    prompt=task['prompt'],
                    height=task.get('height', 1024),
                    width=task.get('width', 1024),
                    guidance_scale=task.get('cfg', 3.5),
                    num_inference_steps=task.get('num_inference_steps', 50),
                    max_sequence_length=512,
                    generator=generator
                )
            elif mode == 'img2img':
                # åŠ è½½è¾“å…¥å›¾ç‰‡
                input_image = load_image_func(task.get('input_image'))
                
                # è·å–å°ºå¯¸ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šï¼Œå¦åˆ™ä½¿ç”¨å›¾ç‰‡åŸå§‹å°ºå¯¸
                width, height = get_output_dimensions(input_image, mode)
                
                result = pipe(
                    prompt=task['prompt'],
                    image=input_image,
                    height=height,
                    width=width,
                    strength=task.get('strength', 0.8),
                    guidance_scale=task.get('cfg', 3.5),
                    num_inference_steps=task.get('num_inference_steps', 50),
                    max_sequence_length=512,
                    generator=generator
                )
            elif mode == 'fill':
                # åŠ è½½è¾“å…¥å›¾ç‰‡å’Œè’™ç‰ˆ
                input_image = load_image_func(task.get('input_image'))
                mask_image = load_image_func(task.get('mask_image'))
                
                # è·å–å°ºå¯¸ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šï¼Œå¦åˆ™ä½¿ç”¨å›¾ç‰‡åŸå§‹å°ºå¯¸
                width, height = get_output_dimensions(input_image, mode)
                
                # æ ¹æ®å®˜æ–¹ç¤ºä¾‹ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡å°ºå¯¸
                result = pipe(
                    prompt=task['prompt'],
                    image=input_image,
                    mask_image=mask_image,
                    height=height,
                    width=width,
                    guidance_scale=task.get('cfg', 30.0),  # Fillæ¨¡å¼æ¨èä½¿ç”¨30
                    num_inference_steps=task.get('num_inference_steps', 50),
                    max_sequence_length=512,
                    generator=generator
                )
            elif mode == 'controlnet':
                # åŠ è½½æ§åˆ¶å›¾ç‰‡
                control_image = load_image_func(task.get('control_image'))
                controlnet_type = task.get('controlnet_type', 'depth')
                
                # è·å–å°ºå¯¸ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šï¼Œå¦åˆ™ä½¿ç”¨æ§åˆ¶å›¾ç‰‡åŸå§‹å°ºå¯¸
                width, height = get_output_dimensions(control_image, mode)
                
                # å¦‚æœåŒæ—¶æä¾›äº†input_imageï¼Œè®°å½•ä½†ä¸ä½¿ç”¨ï¼ˆControlNetæ¨¡å¼ä¸»è¦ä½¿ç”¨control_imageï¼‰
                if task.get('input_image'):
                    logger.info(f"GPU {gpu_id} ControlNetæ¨¡å¼æ£€æµ‹åˆ°input_imageï¼Œä½†ä¸»è¦ä½¿ç”¨control_image")
                
                # æ„å»ºControlNetè°ƒç”¨å‚æ•° - FluxControlPipelineåªæ”¯æŒåŸºæœ¬å‚æ•°
                controlnet_kwargs = {
                    'prompt': task['prompt'],
                    'control_image': control_image,
                    'height': height,
                    'width': width,
                    'guidance_scale': task.get('cfg', 10.0),  # controlnetæ¨èä½¿ç”¨10.0
                    'num_inference_steps': task.get('num_inference_steps', 30),  # controlnetæ¨èä½¿ç”¨30æ­¥
                    'max_sequence_length': 512,
                    'generator': generator
                }
                
                # è®°å½•ç”¨æˆ·æä¾›çš„ControlNetå¼ºåº¦æ§åˆ¶å‚æ•°ï¼ˆä½†ä¸ä½¿ç”¨ï¼Œå› ä¸ºFluxControlPipelineä¸æ”¯æŒï¼‰
                if task.get('controlnet_conditioning_scale') is not None:
                    logger.info(f"GPU {gpu_id} ç”¨æˆ·æä¾›äº†controlnet_conditioning_scale: {task.get('controlnet_conditioning_scale')}ï¼Œä½†FluxControlPipelineä¸æ”¯æŒæ­¤å‚æ•°")
                
                if task.get('control_guidance_start') is not None:
                    logger.info(f"GPU {gpu_id} ç”¨æˆ·æä¾›äº†control_guidance_start: {task.get('control_guidance_start')}ï¼Œä½†FluxControlPipelineä¸æ”¯æŒæ­¤å‚æ•°")
                
                if task.get('control_guidance_end') is not None:
                    logger.info(f"GPU {gpu_id} ç”¨æˆ·æä¾›äº†control_guidance_end: {task.get('control_guidance_end')}ï¼Œä½†FluxControlPipelineä¸æ”¯æŒæ­¤å‚æ•°")
                
                result = pipe(**controlnet_kwargs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç”Ÿæˆæ¨¡å¼: {mode}")
        
        image = result.images[0]
        buffer = io.BytesIO()
        image.save(buffer, format="PNG")
        img_base64 = base64.b64encode(buffer.getvalue()).decode()
        elapsed_time = time.time() - start_time
        
        # è®°å½•ä»»åŠ¡å®Œæˆæ—¶çš„å†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            final_allocated = torch.cuda.memory_allocated() / 1024**2
            final_cached = torch.cuda.memory_reserved() / 1024**2
            memory_increase = final_allocated - initial_allocated
            logger.info(f"GPU {gpu_id} ä»»åŠ¡å®Œæˆåå†…å­˜: å·²åˆ†é… {final_allocated:.1f}MB (+{memory_increase:.1f}MB), ç¼“å­˜ {final_cached:.1f}MB")
        
        logger.info(f"GPU {gpu_id} {mode}ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        # å¤„ç†ä¿å­˜åˆ°ç£ç›˜
        save_to_disk = False
        save_path = task.get('save_disk_path')
        if save_path:
            try:
                image.save(save_path)
                save_to_disk = True
                logger.info(f"GPU {gpu_id} å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
            except Exception as e:
                logger.warning(f"GPU {gpu_id} ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
        
        return {
            "success": True,
            "image_base64": img_base64,
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id'),
            "save_to_disk": save_to_disk,
            "params": task,
            "mode": mode,
            "controlnet_type": task.get('controlnet_type') if mode == 'controlnet' else None
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = f"GPU {gpu_id} {mode}ç”Ÿæˆå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        # è®°å½•é”™è¯¯æ—¶çš„å†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            error_allocated = torch.cuda.memory_allocated() / 1024**2
            error_cached = torch.cuda.memory_reserved() / 1024**2
            logger.error(f"GPU {gpu_id} é”™è¯¯æ—¶å†…å­˜: å·²åˆ†é… {error_allocated:.1f}MB, ç¼“å­˜ {error_cached:.1f}MB")
        
        return {
            "success": False,
            "error": str(e),
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id'),
            "mode": mode,
            "controlnet_type": task.get('controlnet_type') if mode == 'controlnet' else None
        }

class GPUIsolationManager:
    """GPUéš”ç¦»ç®¡ç†å™¨ - ä½¿ç”¨å­è¿›ç¨‹å®ç°çœŸæ­£çš„GPUéš”ç¦»"""
    
    def __init__(self):
        self.processes: Dict[str, mp.Process] = {}
        self.result_queues: Dict[str, mp.Queue] = {}
        self.task_queues: Dict[str, mp.Queue] = {}
        self.process_configs: Dict[str, Dict[str, Any]] = {}  # å­˜å‚¨è¿›ç¨‹é…ç½®ç”¨äºé‡å¯
        self.is_running = True
        self.restart_attempts: Dict[str, int] = {}  # è®°å½•é‡å¯æ¬¡æ•°
        self.max_restart_attempts = 3  # æœ€å¤§é‡å¯æ¬¡æ•°
    
    def create_gpu_process(self, gpu_id: str, model_path: str, model_id: str) -> bool:
        """ä¸ºæŒ‡å®šGPUåˆ›å»ºéš”ç¦»è¿›ç¨‹"""
        try:
            task_queue = mp.Queue()
            result_queue = mp.Queue()
            process = mp.Process(
                target=gpu_worker_process,
                args=(gpu_id, model_path, model_id, task_queue, result_queue),
                name=f"gpu-worker-{gpu_id}"
            )
            process.start()
            process_key = f"{model_id}_{gpu_id}"
            self.processes[process_key] = process
            self.task_queues[process_key] = task_queue
            self.result_queues[process_key] = result_queue
            
            # ä¿å­˜è¿›ç¨‹é…ç½®ç”¨äºé‡å¯
            self.process_configs[process_key] = {
                "gpu_id": gpu_id,
                "model_path": model_path,
                "model_id": model_id
            }
            
            # åˆå§‹åŒ–é‡å¯è®¡æ•°
            self.restart_attempts[process_key] = 0
            
            logger.info(f"âœ… GPU {gpu_id} éš”ç¦»è¿›ç¨‹å·²åˆ›å»º (PID: {process.pid})")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºGPU {gpu_id} éš”ç¦»è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def restart_gpu_process(self, process_key: str) -> bool:
        """é‡å¯æŒ‡å®šçš„GPUè¿›ç¨‹"""
        if process_key not in self.process_configs:
            logger.error(f"æ— æ³•é‡å¯è¿›ç¨‹ {process_key}ï¼šé…ç½®ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥é‡å¯æ¬¡æ•°
        if self.restart_attempts[process_key] >= self.max_restart_attempts:
            logger.error(f"è¿›ç¨‹ {process_key} é‡å¯æ¬¡æ•°å·²è¾¾ä¸Šé™ ({self.max_restart_attempts})ï¼Œåœæ­¢é‡å¯")
            return False
        
        config = self.process_configs[process_key]
        gpu_id = config["gpu_id"]
        
        logger.warning(f"ğŸ”„ å°è¯•é‡å¯GPU {gpu_id} è¿›ç¨‹ (ç¬¬ {self.restart_attempts[process_key] + 1} æ¬¡)")
        
        try:
            # æ¸…ç†æ—§è¿›ç¨‹
            if process_key in self.processes:
                old_process = self.processes[process_key]
                if old_process.is_alive():
                    old_process.terminate()
                    old_process.join(timeout=5.0)
                    if old_process.is_alive():
                        old_process.kill()
            
            # æ¸…ç†æ—§é˜Ÿåˆ—
            if process_key in self.task_queues:
                del self.task_queues[process_key]
            if process_key in self.result_queues:
                del self.result_queues[process_key]
            
            # åˆ›å»ºæ–°è¿›ç¨‹
            task_queue = mp.Queue()
            result_queue = mp.Queue()
            process = mp.Process(
                target=gpu_worker_process,
                args=(gpu_id, config["model_path"], config["model_id"], task_queue, result_queue),
                name=f"gpu-worker-{gpu_id}-restart"
            )
            process.start()
            
            # æ›´æ–°è¿›ç¨‹è®°å½•
            self.processes[process_key] = process
            self.task_queues[process_key] = task_queue
            self.result_queues[process_key] = result_queue
            self.restart_attempts[process_key] += 1
            
            logger.info(f"âœ… GPU {gpu_id} è¿›ç¨‹é‡å¯æˆåŠŸ (PID: {process.pid})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡å¯GPU {gpu_id} è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def check_and_restart_dead_processes(self) -> Dict[str, bool]:
        """æ£€æŸ¥å¹¶é‡å¯æ­»äº¡çš„è¿›ç¨‹"""
        restart_results = {}
        
        for process_key, process in self.processes.items():
            try:
                if not process.is_alive():
                    logger.warning(f"æ£€æµ‹åˆ°æ­»äº¡è¿›ç¨‹ {process_key} (PID: {process.pid}, exitcode: {process.exitcode})")
                    
                    # å°è¯•é‡å¯
                    success = self.restart_gpu_process(process_key)
                    restart_results[process_key] = success
                    
                    if success:
                        logger.info(f"âœ… è¿›ç¨‹ {process_key} é‡å¯æˆåŠŸ")
                    else:
                        logger.error(f"âŒ è¿›ç¨‹ {process_key} é‡å¯å¤±è´¥")
                        
            except Exception as e:
                logger.error(f"æ£€æŸ¥è¿›ç¨‹ {process_key} çŠ¶æ€æ—¶å‡ºé”™: {e}")
                restart_results[process_key] = False
        
        return restart_results
    
    def submit_task(self, gpu_id: str, model_id: str, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æäº¤ä»»åŠ¡åˆ°æŒ‡å®šGPU"""
        process_key = f"{model_id}_{gpu_id}"
        
        if process_key not in self.task_queues:
            logger.error(f"GPU {gpu_id} è¿›ç¨‹ä¸å­˜åœ¨")
            return None
        
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜æ´»ç€
        if process_key not in self.processes:
            logger.error(f"GPU {gpu_id} è¿›ç¨‹è®°å½•ä¸å­˜åœ¨")
            return None
            
        process = self.processes[process_key]
        if not process.is_alive():
            logger.error(f"GPU {gpu_id} è¿›ç¨‹å·²æ­»äº¡ (PID: {process.pid}, exitcode: {process.exitcode})")
            
            # å°è¯•é‡å¯è¿›ç¨‹
            restart_success = self.restart_gpu_process(process_key)
            if restart_success:
                logger.info(f"GPU {gpu_id} è¿›ç¨‹å·²é‡å¯ï¼Œé‡æ–°æäº¤ä»»åŠ¡")
                # é‡æ–°è·å–è¿›ç¨‹å’Œé˜Ÿåˆ—
                process = self.processes[process_key]
                task_queue = self.task_queues[process_key]
                result_queue = self.result_queues[process_key]
            else:
                return {
                    "success": False,
                    "error": f"GPUè¿›ç¨‹å·²æ­»äº¡ä¸”é‡å¯å¤±è´¥ (exitcode: {process.exitcode})",
                    "gpu_id": gpu_id
                }
        else:
            task_queue = self.task_queues[process_key]
            result_queue = self.result_queues[process_key]
        
        try:
            # æäº¤ä»»åŠ¡
            logger.info(f"æäº¤ä»»åŠ¡åˆ°GPU {gpu_id} (PID: {process.pid}): {task.get('task_id', 'unknown')}")
            task_queue.put(task)
            
            # ç­‰å¾…ç»“æœ
            result = result_queue.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            logger.info(f"GPU {gpu_id} ä»»åŠ¡å®Œæˆ: {result.get('success', False)}")
            return result
            
        except queue.Empty:
            error_msg = f"GPU {gpu_id} ä»»åŠ¡è¶…æ—¶ (5åˆ†é’Ÿ)"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "gpu_id": gpu_id
            }
        except Exception as e:
            error_msg = f"æäº¤ä»»åŠ¡åˆ°GPU {gpu_id} å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "gpu_id": gpu_id
            }
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰GPUè¿›ç¨‹çŠ¶æ€"""
        status = {}
        
        for process_key, process in self.processes.items():
            try:
                is_alive = process.is_alive()
                status[process_key] = {
                    "pid": process.pid,
                    "alive": is_alive,
                    "exitcode": process.exitcode,
                    "name": process.name,
                    "daemon": process.daemon,
                    "restart_attempts": self.restart_attempts.get(process_key, 0),
                    "max_restart_attempts": self.max_restart_attempts
                }
                
                # å¦‚æœè¿›ç¨‹æ­»äº¡ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                if not is_alive:
                    logger.warning(f"è¿›ç¨‹ {process_key} å·²æ­»äº¡ (PID: {process.pid}, exitcode: {process.exitcode})")
                    
            except Exception as e:
                logger.error(f"æ£€æŸ¥è¿›ç¨‹ {process_key} çŠ¶æ€æ—¶å‡ºé”™: {e}")
                status[process_key] = {
                    "pid": "unknown",
                    "alive": False,
                    "exitcode": "unknown",
                    "error": str(e),
                    "restart_attempts": self.restart_attempts.get(process_key, 0),
                    "max_restart_attempts": self.max_restart_attempts
                }
        
        return status
    
    def shutdown(self):
        """å…³é—­æ‰€æœ‰GPUè¿›ç¨‹"""
        logger.info("æ­£åœ¨å…³é—­GPUéš”ç¦»ç®¡ç†å™¨...")
        
        # å‘é€é€€å‡ºä¿¡å·
        for process_key, task_queue in self.task_queues.items():
            try:
                task_queue.put(None)  # é€€å‡ºä¿¡å·
            except:
                pass
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for process_key, process in self.processes.items():
            try:
                process.join(timeout=10.0)
                if process.is_alive():
                    process.terminate()
                    process.join(timeout=5.0)
                    if process.is_alive():
                        process.kill()
            except Exception as e:
                logger.warning(f"å…³é—­è¿›ç¨‹ {process_key} æ—¶å‡ºé”™: {e}")
        
        logger.info("GPUéš”ç¦»ç®¡ç†å™¨å·²å…³é—­")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•GPUéš”ç¦»
    manager = GPUIsolationManager()
    
    # åˆ›å»ºGPUè¿›ç¨‹
    gpu_ids = ["0", "1", "2", "3"]
    from config import Config
    model_paths = Config.get_model_paths()
    model_path = model_paths.get("flux1-dev", "/path/to/flux1-dev")
    
    for gpu_id in gpu_ids:
        manager.create_gpu_process(gpu_id, model_path, "flux1-dev")
    
    # æäº¤æµ‹è¯•ä»»åŠ¡
    test_task = {
        "task_id": "test_001",
        "prompt": "A beautiful landscape",
        "height": 1024,
        "width": 1024,
        "seed": 42
    }
    
    result = manager.submit_task("0", "flux1-dev", test_task)
    print(f"æµ‹è¯•ç»“æœ: {result}")
    
    # å…³é—­ç®¡ç†å™¨
    manager.shutdown() 
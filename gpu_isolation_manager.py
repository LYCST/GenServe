import os
import subprocess
import multiprocessing as mp
from typing import Dict, Any, Optional, Callable
import logging
import time
import signal
import sys
import traceback
import queue
import gc
import psutil

# mp.set_start_method('spawn', force=True)

logger = logging.getLogger(__name__)

def gpu_worker_process(gpu_id: str, model_path: str, model_id: str, task_queue, result_queue):
    """GPUå·¥ä½œè¿›ç¨‹ - åœ¨éš”ç¦»ç¯å¢ƒä¸­è¿è¡Œï¼ˆé¡¶å±‚å‡½æ•°ï¼Œæ”¯æŒspawnï¼‰"""
    import torch
    from diffusers import FluxPipeline
    import base64
    import io
    from PIL import Image
    import traceback
    import queue
    import logging
    import time
    import gc
    import psutil

    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
    
    # è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§å’Œå†…å­˜é™åˆ¶
    try:
        process = psutil.Process()
        process.nice(10)  # é™ä½è¿›ç¨‹ä¼˜å…ˆçº§ï¼Œå‡å°‘è¢«OOM Killeræ€æ­»çš„æ¦‚ç‡
        logger.info(f"ğŸš€ GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()}, ä¼˜å…ˆçº§: {process.nice()})")
    except Exception as e:
        logger.warning(f"æ— æ³•è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§: {e}")
        logger.info(f"ğŸš€ GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()})")
    
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°GPU {gpu_id}...")
        
        # åŠ è½½å‰æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        pipe = FluxPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
            local_files_only=True
        )
        pipe.enable_model_cpu_offload(device="cuda:0")
        logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU {gpu_id}")
        
        task_count = 0
        consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
        max_consecutive_failures = 3  # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
        last_cleanup_time = time.time()  # ä¸Šæ¬¡æ¸…ç†æ—¶é—´
        
        while True:
            try:
                # å®šæœŸå†…å­˜æ¸…ç†
                current_time = time.time()
                from config import Config
                if current_time - last_cleanup_time > Config.GPU_MEMORY_CLEANUP_INTERVAL:
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated() / 1024**2
                        if allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                            logger.info(f"GPU {gpu_id} å®šæœŸæ¸…ç†å†…å­˜ (å·²åˆ†é…: {allocated:.1f}MB)")
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            gc.collect()
                    last_cleanup_time = current_time
                
                task = task_queue.get(timeout=1.0)
                if task is None:
                    logger.info(f"GPU {gpu_id} æ”¶åˆ°é€€å‡ºä¿¡å·")
                    break
                
                task_count += 1
                logger.info(f"GPU {gpu_id} å¼€å§‹å¤„ç†ä»»åŠ¡ #{task_count}: {task.get('task_id', 'unknown')}")
                
                # ä»»åŠ¡å¼€å§‹å‰æ£€æŸ¥å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    initial_allocated = torch.cuda.memory_allocated() / 1024**2
                    initial_cached = torch.cuda.memory_reserved() / 1024**2
                    logger.info(f"GPU {gpu_id} ä»»åŠ¡å¼€å§‹å‰å†…å­˜: å·²åˆ†é… {initial_allocated:.1f}MB, ç¼“å­˜ {initial_cached:.1f}MB")
                    
                    # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå¼ºåˆ¶æ¸…ç†
                    if initial_allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                        logger.warning(f"GPU {gpu_id} å†…å­˜ä½¿ç”¨è¿‡é«˜ ({initial_allocated:.1f}MB > {Config.GPU_MEMORY_THRESHOLD_MB}MB)ï¼Œå¼ºåˆ¶æ¸…ç†")
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        gc.collect()
                
                # å¤„ç†ä»»åŠ¡
                result = process_generation_task(pipe, task, gpu_id)
                result_queue.put(result)
                
                success = result.get('success', False)
                logger.info(f"GPU {gpu_id} ä»»åŠ¡ #{task_count} å¤„ç†å®Œæˆ: {success}")
                
                # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°
                if success:
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                
                # ä»»åŠ¡å®Œæˆåè¿›è¡Œæ¸…ç†å’Œç­‰å¾…
                if success:
                    logger.info(f"GPU {gpu_id} å¼€å§‹æ¸…ç†èµ„æº...")
                    
                    # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
                    if torch.cuda.is_available():
                        # å¤šæ¬¡æ¸…ç†ç¡®ä¿å½»åº•
                        for i in range(3):
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            time.sleep(0.1)
                        
                        torch.cuda.reset_peak_memory_stats()
                        
                        # å¦‚æœå¯ç”¨æ¿€è¿›æ¸…ç†ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†
                        if Config.ENABLE_AGGRESSIVE_CLEANUP:
                            # å¼ºåˆ¶åƒåœ¾å›æ”¶å¤šæ¬¡
                            for i in range(2):
                                gc.collect()
                                time.sleep(0.05)
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ¸…ç†å®Œæˆ
                    cleanup_wait_time = Config.GPU_TASK_CLEANUP_WAIT_TIME
                    logger.info(f"GPU {gpu_id} ç­‰å¾… {cleanup_wait_time} ç§’ç¡®ä¿æ¸…ç†å®Œæˆ...")
                    time.sleep(cleanup_wait_time)
                    
                    # è®°å½•æ¸…ç†åçš„å†…å­˜çŠ¶æ€
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated() / 1024**2
                        cached = torch.cuda.memory_reserved() / 1024**2
                        logger.info(f"GPU {gpu_id} æ¸…ç†åå†…å­˜: å·²åˆ†é… {allocated:.1f}MB, ç¼“å­˜ {cached:.1f}MB")
                        
                        # å¦‚æœå†…å­˜ä»ç„¶è¿‡é«˜ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†
                        if allocated > Config.GPU_MEMORY_THRESHOLD_MB:
                            logger.warning(f"GPU {gpu_id} æ¸…ç†åå†…å­˜ä»ç„¶è¿‡é«˜ ({allocated:.1f}MB)ï¼Œè¿›è¡Œé¢å¤–æ¸…ç†")
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            gc.collect()
                    
                    logger.info(f"GPU {gpu_id} æ¸…ç†å®Œæˆï¼Œå‡†å¤‡æ¥æ”¶ä¸‹ä¸€ä¸ªä»»åŠ¡")
                else:
                    logger.warning(f"GPU {gpu_id} ä»»åŠ¡å¤±è´¥ï¼Œè·³è¿‡æ¸…ç†ç­‰å¾…")
                    
                    # å¤±è´¥åä¹Ÿè¦æ¸…ç†å†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        gc.collect()
                
                # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"GPU {gpu_id} è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œå‡†å¤‡é‡å¯è¿›ç¨‹")
                    break
                
            except queue.Empty:
                continue
            except Exception as e:
                error_msg = f"GPU {gpu_id} å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                
                consecutive_failures += 1
                
                try:
                    result_queue.put({
                        "success": False,
                        "error": str(e),
                        "gpu_id": gpu_id,
                        "task_id": task.get('task_id') if 'task' in locals() else 'unknown'
                    })
                except Exception as put_error:
                    logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›é”™è¯¯ç»“æœ: {put_error}")
                
                # å¼‚å¸¸åæ¸…ç†å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    gc.collect()
        
        logger.info(f"GPU {gpu_id} å¼€å§‹æœ€ç»ˆæ¸…ç†èµ„æº...")
        del pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        logger.info(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹é€€å‡º")
    except Exception as e:
        error_msg = f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"å¯åŠ¨é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        try:
            result_queue.put({
                "success": False,
                "error": f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}",
                "gpu_id": gpu_id
            })
        except Exception as put_error:
            logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›å¯åŠ¨å¤±è´¥ç»“æœ: {put_error}")

def process_generation_task(pipe, task, gpu_id: str):
    import torch
    import io
    import base64
    import time
    import traceback
    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    start_time = time.time()
    
    # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶çš„å†…å­˜çŠ¶æ€
    if torch.cuda.is_available():
        initial_allocated = torch.cuda.memory_allocated() / 1024**2
        initial_cached = torch.cuda.memory_reserved() / 1024**2
        logger.info(f"GPU {gpu_id} ä»»åŠ¡å¼€å§‹å‰å†…å­˜: å·²åˆ†é… {initial_allocated:.1f}MB, ç¼“å­˜ {initial_cached:.1f}MB")
    
    try:
        logger.info(f"GPU {gpu_id} å¼€å§‹ç”Ÿæˆä»»åŠ¡: {task.get('task_id', 'unknown')}")
        generator = torch.Generator("cpu").manual_seed(task.get('seed', 42))
        
        with torch.no_grad():
            result = pipe(
                prompt=task['prompt'],
                height=task.get('height', 1024),
                width=task.get('width', 1024),
                guidance_scale=task.get('cfg', 3.5),
                num_inference_steps=task.get('num_inference_steps', 50),
                max_sequence_length=512,
                generator=generator
            )
        
        image = result.images[0]
        buffer = io.BytesIO()
        image.save(buffer, format="PNG")
        img_base64 = base64.b64encode(buffer.getvalue()).decode()
        elapsed_time = time.time() - start_time
        
        # è®°å½•ä»»åŠ¡å®Œæˆæ—¶çš„å†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            final_allocated = torch.cuda.memory_allocated() / 1024**2
            final_cached = torch.cuda.memory_reserved() / 1024**2
            memory_increase = final_allocated - initial_allocated
            logger.info(f"GPU {gpu_id} ä»»åŠ¡å®Œæˆåå†…å­˜: å·²åˆ†é… {final_allocated:.1f}MB (+{memory_increase:.1f}MB), ç¼“å­˜ {final_cached:.1f}MB")
        
        logger.info(f"GPU {gpu_id} ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        # å¤„ç†ä¿å­˜åˆ°ç£ç›˜
        save_to_disk = False
        save_path = task.get('save_disk_path')
        if save_path:
            try:
                image.save(save_path)
                save_to_disk = True
                logger.info(f"GPU {gpu_id} å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
            except Exception as e:
                logger.warning(f"GPU {gpu_id} ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
        
        return {
            "success": True,
            "image_base64": img_base64,
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id'),
            "save_to_disk": save_to_disk,
            "params": task
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = f"GPU {gpu_id} ç”Ÿæˆå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        # è®°å½•é”™è¯¯æ—¶çš„å†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            error_allocated = torch.cuda.memory_allocated() / 1024**2
            error_cached = torch.cuda.memory_reserved() / 1024**2
            logger.error(f"GPU {gpu_id} é”™è¯¯æ—¶å†…å­˜: å·²åˆ†é… {error_allocated:.1f}MB, ç¼“å­˜ {error_cached:.1f}MB")
        
        return {
            "success": False,
            "error": str(e),
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id')
        }

class GPUIsolationManager:
    """GPUéš”ç¦»ç®¡ç†å™¨ - ä½¿ç”¨å­è¿›ç¨‹å®ç°çœŸæ­£çš„GPUéš”ç¦»"""
    
    def __init__(self):
        self.processes: Dict[str, mp.Process] = {}
        self.result_queues: Dict[str, mp.Queue] = {}
        self.task_queues: Dict[str, mp.Queue] = {}
        self.process_configs: Dict[str, Dict[str, Any]] = {}  # å­˜å‚¨è¿›ç¨‹é…ç½®ç”¨äºé‡å¯
        self.is_running = True
        self.restart_attempts: Dict[str, int] = {}  # è®°å½•é‡å¯æ¬¡æ•°
        self.max_restart_attempts = 3  # æœ€å¤§é‡å¯æ¬¡æ•°
    
    def create_gpu_process(self, gpu_id: str, model_path: str, model_id: str) -> bool:
        """ä¸ºæŒ‡å®šGPUåˆ›å»ºéš”ç¦»è¿›ç¨‹"""
        try:
            task_queue = mp.Queue()
            result_queue = mp.Queue()
            process = mp.Process(
                target=gpu_worker_process,
                args=(gpu_id, model_path, model_id, task_queue, result_queue),
                name=f"gpu-worker-{gpu_id}"
            )
            process.start()
            process_key = f"{model_id}_{gpu_id}"
            self.processes[process_key] = process
            self.task_queues[process_key] = task_queue
            self.result_queues[process_key] = result_queue
            
            # ä¿å­˜è¿›ç¨‹é…ç½®ç”¨äºé‡å¯
            self.process_configs[process_key] = {
                "gpu_id": gpu_id,
                "model_path": model_path,
                "model_id": model_id
            }
            
            # åˆå§‹åŒ–é‡å¯è®¡æ•°
            self.restart_attempts[process_key] = 0
            
            logger.info(f"âœ… GPU {gpu_id} éš”ç¦»è¿›ç¨‹å·²åˆ›å»º (PID: {process.pid})")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºGPU {gpu_id} éš”ç¦»è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def restart_gpu_process(self, process_key: str) -> bool:
        """é‡å¯æŒ‡å®šçš„GPUè¿›ç¨‹"""
        if process_key not in self.process_configs:
            logger.error(f"æ— æ³•é‡å¯è¿›ç¨‹ {process_key}ï¼šé…ç½®ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥é‡å¯æ¬¡æ•°
        if self.restart_attempts[process_key] >= self.max_restart_attempts:
            logger.error(f"è¿›ç¨‹ {process_key} é‡å¯æ¬¡æ•°å·²è¾¾ä¸Šé™ ({self.max_restart_attempts})ï¼Œåœæ­¢é‡å¯")
            return False
        
        config = self.process_configs[process_key]
        gpu_id = config["gpu_id"]
        
        logger.warning(f"ğŸ”„ å°è¯•é‡å¯GPU {gpu_id} è¿›ç¨‹ (ç¬¬ {self.restart_attempts[process_key] + 1} æ¬¡)")
        
        try:
            # æ¸…ç†æ—§è¿›ç¨‹
            if process_key in self.processes:
                old_process = self.processes[process_key]
                if old_process.is_alive():
                    old_process.terminate()
                    old_process.join(timeout=5.0)
                    if old_process.is_alive():
                        old_process.kill()
            
            # æ¸…ç†æ—§é˜Ÿåˆ—
            if process_key in self.task_queues:
                del self.task_queues[process_key]
            if process_key in self.result_queues:
                del self.result_queues[process_key]
            
            # åˆ›å»ºæ–°è¿›ç¨‹
            task_queue = mp.Queue()
            result_queue = mp.Queue()
            process = mp.Process(
                target=gpu_worker_process,
                args=(gpu_id, config["model_path"], config["model_id"], task_queue, result_queue),
                name=f"gpu-worker-{gpu_id}-restart"
            )
            process.start()
            
            # æ›´æ–°è¿›ç¨‹è®°å½•
            self.processes[process_key] = process
            self.task_queues[process_key] = task_queue
            self.result_queues[process_key] = result_queue
            self.restart_attempts[process_key] += 1
            
            logger.info(f"âœ… GPU {gpu_id} è¿›ç¨‹é‡å¯æˆåŠŸ (PID: {process.pid})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡å¯GPU {gpu_id} è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def check_and_restart_dead_processes(self) -> Dict[str, bool]:
        """æ£€æŸ¥å¹¶é‡å¯æ­»äº¡çš„è¿›ç¨‹"""
        restart_results = {}
        
        for process_key, process in self.processes.items():
            try:
                if not process.is_alive():
                    logger.warning(f"æ£€æµ‹åˆ°æ­»äº¡è¿›ç¨‹ {process_key} (PID: {process.pid}, exitcode: {process.exitcode})")
                    
                    # å°è¯•é‡å¯
                    success = self.restart_gpu_process(process_key)
                    restart_results[process_key] = success
                    
                    if success:
                        logger.info(f"âœ… è¿›ç¨‹ {process_key} é‡å¯æˆåŠŸ")
                    else:
                        logger.error(f"âŒ è¿›ç¨‹ {process_key} é‡å¯å¤±è´¥")
                        
            except Exception as e:
                logger.error(f"æ£€æŸ¥è¿›ç¨‹ {process_key} çŠ¶æ€æ—¶å‡ºé”™: {e}")
                restart_results[process_key] = False
        
        return restart_results
    
    def submit_task(self, gpu_id: str, model_id: str, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æäº¤ä»»åŠ¡åˆ°æŒ‡å®šGPU"""
        process_key = f"{model_id}_{gpu_id}"
        
        if process_key not in self.task_queues:
            logger.error(f"GPU {gpu_id} è¿›ç¨‹ä¸å­˜åœ¨")
            return None
        
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜æ´»ç€
        if process_key not in self.processes:
            logger.error(f"GPU {gpu_id} è¿›ç¨‹è®°å½•ä¸å­˜åœ¨")
            return None
            
        process = self.processes[process_key]
        if not process.is_alive():
            logger.error(f"GPU {gpu_id} è¿›ç¨‹å·²æ­»äº¡ (PID: {process.pid}, exitcode: {process.exitcode})")
            
            # å°è¯•é‡å¯è¿›ç¨‹
            restart_success = self.restart_gpu_process(process_key)
            if restart_success:
                logger.info(f"GPU {gpu_id} è¿›ç¨‹å·²é‡å¯ï¼Œé‡æ–°æäº¤ä»»åŠ¡")
                # é‡æ–°è·å–è¿›ç¨‹å’Œé˜Ÿåˆ—
                process = self.processes[process_key]
                task_queue = self.task_queues[process_key]
                result_queue = self.result_queues[process_key]
            else:
                return {
                    "success": False,
                    "error": f"GPUè¿›ç¨‹å·²æ­»äº¡ä¸”é‡å¯å¤±è´¥ (exitcode: {process.exitcode})",
                    "gpu_id": gpu_id
                }
        else:
            task_queue = self.task_queues[process_key]
            result_queue = self.result_queues[process_key]
        
        try:
            # æäº¤ä»»åŠ¡
            logger.info(f"æäº¤ä»»åŠ¡åˆ°GPU {gpu_id} (PID: {process.pid}): {task.get('task_id', 'unknown')}")
            task_queue.put(task)
            
            # ç­‰å¾…ç»“æœ
            result = result_queue.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            logger.info(f"GPU {gpu_id} ä»»åŠ¡å®Œæˆ: {result.get('success', False)}")
            return result
            
        except queue.Empty:
            error_msg = f"GPU {gpu_id} ä»»åŠ¡è¶…æ—¶ (5åˆ†é’Ÿ)"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "gpu_id": gpu_id
            }
        except Exception as e:
            error_msg = f"æäº¤ä»»åŠ¡åˆ°GPU {gpu_id} å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "gpu_id": gpu_id
            }
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰GPUè¿›ç¨‹çŠ¶æ€"""
        status = {}
        
        for process_key, process in self.processes.items():
            try:
                is_alive = process.is_alive()
                status[process_key] = {
                    "pid": process.pid,
                    "alive": is_alive,
                    "exitcode": process.exitcode,
                    "name": process.name,
                    "daemon": process.daemon,
                    "restart_attempts": self.restart_attempts.get(process_key, 0),
                    "max_restart_attempts": self.max_restart_attempts
                }
                
                # å¦‚æœè¿›ç¨‹æ­»äº¡ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                if not is_alive:
                    logger.warning(f"è¿›ç¨‹ {process_key} å·²æ­»äº¡ (PID: {process.pid}, exitcode: {process.exitcode})")
                    
            except Exception as e:
                logger.error(f"æ£€æŸ¥è¿›ç¨‹ {process_key} çŠ¶æ€æ—¶å‡ºé”™: {e}")
                status[process_key] = {
                    "pid": "unknown",
                    "alive": False,
                    "exitcode": "unknown",
                    "error": str(e),
                    "restart_attempts": self.restart_attempts.get(process_key, 0),
                    "max_restart_attempts": self.max_restart_attempts
                }
        
        return status
    
    def shutdown(self):
        """å…³é—­æ‰€æœ‰GPUè¿›ç¨‹"""
        logger.info("æ­£åœ¨å…³é—­GPUéš”ç¦»ç®¡ç†å™¨...")
        
        # å‘é€é€€å‡ºä¿¡å·
        for process_key, task_queue in self.task_queues.items():
            try:
                task_queue.put(None)  # é€€å‡ºä¿¡å·
            except:
                pass
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for process_key, process in self.processes.items():
            try:
                process.join(timeout=10.0)
                if process.is_alive():
                    process.terminate()
                    process.join(timeout=5.0)
                    if process.is_alive():
                        process.kill()
            except Exception as e:
                logger.warning(f"å…³é—­è¿›ç¨‹ {process_key} æ—¶å‡ºé”™: {e}")
        
        logger.info("GPUéš”ç¦»ç®¡ç†å™¨å·²å…³é—­")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•GPUéš”ç¦»
    manager = GPUIsolationManager()
    
    # åˆ›å»ºGPUè¿›ç¨‹
    gpu_ids = ["0", "1", "2", "3"]
    model_path = "/home/shuzuan/prj/models/flux1-dev"
    
    for gpu_id in gpu_ids:
        manager.create_gpu_process(gpu_id, model_path, "flux1-dev")
    
    # æäº¤æµ‹è¯•ä»»åŠ¡
    test_task = {
        "task_id": "test_001",
        "prompt": "A beautiful landscape",
        "height": 1024,
        "width": 1024,
        "seed": 42
    }
    
    result = manager.submit_task("0", "flux1-dev", test_task)
    print(f"æµ‹è¯•ç»“æœ: {result}")
    
    # å…³é—­ç®¡ç†å™¨
    manager.shutdown() 
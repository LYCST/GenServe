import os
import subprocess
import multiprocessing as mp
from typing import Dict, Any, Optional, Callable
import logging
import time
import signal
import sys
import traceback
import queue

# mp.set_start_method('spawn', force=True)

logger = logging.getLogger(__name__)

def gpu_worker_process(gpu_id: str, model_path: str, model_id: str, task_queue, result_queue):
    """GPUå·¥ä½œè¿›ç¨‹ - åœ¨éš”ç¦»ç¯å¢ƒä¸­è¿è¡Œï¼ˆé¡¶å±‚å‡½æ•°ï¼Œæ”¯æŒspawnï¼‰"""
    import torch
    from diffusers import FluxPipeline
    import base64
    import io
    from PIL import Image
    import traceback
    import queue
    import logging
    import time

    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
    logger.info(f"ğŸš€ GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()})")
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°GPU {gpu_id}...")
        pipe = FluxPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
            local_files_only=True
        )
        pipe.enable_model_cpu_offload(device="cuda:0")
        logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU {gpu_id}")
        while True:
            try:
                task = task_queue.get(timeout=1.0)
                if task is None:
                    logger.info(f"GPU {gpu_id} æ”¶åˆ°é€€å‡ºä¿¡å·")
                    break
                logger.info(f"GPU {gpu_id} å¼€å§‹å¤„ç†ä»»åŠ¡: {task.get('task_id', 'unknown')}")
                result = process_generation_task(pipe, task, gpu_id)
                result_queue.put(result)
                logger.info(f"GPU {gpu_id} ä»»åŠ¡å¤„ç†å®Œæˆ: {result.get('success', False)}")
            except queue.Empty:
                continue
            except Exception as e:
                error_msg = f"GPU {gpu_id} å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                try:
                    result_queue.put({
                        "success": False,
                        "error": str(e),
                        "gpu_id": gpu_id,
                        "task_id": task.get('task_id') if 'task' in locals() else 'unknown'
                    })
                except Exception as put_error:
                    logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›é”™è¯¯ç»“æœ: {put_error}")
        logger.info(f"GPU {gpu_id} å¼€å§‹æ¸…ç†èµ„æº...")
        del pipe
        torch.cuda.empty_cache()
        logger.info(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹é€€å‡º")
    except Exception as e:
        error_msg = f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"å¯åŠ¨é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        try:
            result_queue.put({
                "success": False,
                "error": f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: {str(e)}",
                "gpu_id": gpu_id
            })
        except Exception as put_error:
            logger.error(f"GPU {gpu_id} æ— æ³•è¿”å›å¯åŠ¨å¤±è´¥ç»“æœ: {put_error}")

def process_generation_task(pipe, task, gpu_id: str):
    import torch
    import io
    import base64
    import time
    import traceback
    logger = logging.getLogger(f"gpu_worker_{gpu_id}")
    start_time = time.time()
    try:
        logger.info(f"GPU {gpu_id} å¼€å§‹ç”Ÿæˆä»»åŠ¡: {task.get('task_id', 'unknown')}")
        generator = torch.Generator("cpu").manual_seed(task.get('seed', 42))
        with torch.no_grad():
            result = pipe(
                prompt=task['prompt'],
                height=task.get('height', 1024),
                width=task.get('width', 1024),
                guidance_scale=task.get('cfg', 3.5),
                num_inference_steps=task.get('num_inference_steps', 50),
                max_sequence_length=512,
                generator=generator
            )
        image = result.images[0]
        buffer = io.BytesIO()
        image.save(buffer, format="PNG")
        img_base64 = base64.b64encode(buffer.getvalue()).decode()
        elapsed_time = time.time() - start_time
        logger.info(f"GPU {gpu_id} ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        return {
            "success": True,
            "image_base64": img_base64,
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id'),
            "params": task
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = f"GPU {gpu_id} ç”Ÿæˆå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(f"ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return {
            "success": False,
            "error": str(e),
            "elapsed_time": elapsed_time,
            "gpu_id": gpu_id,
            "task_id": task.get('task_id')
        }

class GPUIsolationManager:
    """GPUéš”ç¦»ç®¡ç†å™¨ - ä½¿ç”¨å­è¿›ç¨‹å®ç°çœŸæ­£çš„GPUéš”ç¦»"""
    
    def __init__(self):
        self.processes: Dict[str, mp.Process] = {}
        self.result_queues: Dict[str, mp.Queue] = {}
        self.task_queues: Dict[str, mp.Queue] = {}
        self.is_running = True
    
    def create_gpu_process(self, gpu_id: str, model_path: str, model_id: str) -> bool:
        """ä¸ºæŒ‡å®šGPUåˆ›å»ºéš”ç¦»è¿›ç¨‹"""
        try:
            task_queue = mp.Queue()
            result_queue = mp.Queue()
            process = mp.Process(
                target=gpu_worker_process,
                args=(gpu_id, model_path, model_id, task_queue, result_queue),
                name=f"gpu-worker-{gpu_id}"
            )
            process.start()
            process_key = f"{model_id}_{gpu_id}"
            self.processes[process_key] = process
            self.task_queues[process_key] = task_queue
            self.result_queues[process_key] = result_queue
            logger.info(f"âœ… GPU {gpu_id} éš”ç¦»è¿›ç¨‹å·²åˆ›å»º (PID: {process.pid})")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºGPU {gpu_id} éš”ç¦»è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def submit_task(self, gpu_id: str, model_id: str, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æäº¤ä»»åŠ¡åˆ°æŒ‡å®šGPU"""
        process_key = f"{model_id}_{gpu_id}"
        
        if process_key not in self.task_queues:
            logger.error(f"GPU {gpu_id} è¿›ç¨‹ä¸å­˜åœ¨")
            return None
        
        try:
            # æäº¤ä»»åŠ¡
            self.task_queues[process_key].put(task)
            
            # ç­‰å¾…ç»“æœ
            result = self.result_queues[process_key].get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            return result
            
        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡åˆ°GPU {gpu_id} å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "gpu_id": gpu_id
            }
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰GPUè¿›ç¨‹çŠ¶æ€"""
        status = {}
        
        for process_key, process in self.processes.items():
            status[process_key] = {
                "pid": process.pid,
                "alive": process.is_alive(),
                "exitcode": process.exitcode
            }
        
        return status
    
    def shutdown(self):
        """å…³é—­æ‰€æœ‰GPUè¿›ç¨‹"""
        logger.info("æ­£åœ¨å…³é—­GPUéš”ç¦»ç®¡ç†å™¨...")
        
        # å‘é€é€€å‡ºä¿¡å·
        for process_key, task_queue in self.task_queues.items():
            try:
                task_queue.put(None)  # é€€å‡ºä¿¡å·
            except:
                pass
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for process_key, process in self.processes.items():
            try:
                process.join(timeout=10.0)
                if process.is_alive():
                    process.terminate()
                    process.join(timeout=5.0)
                    if process.is_alive():
                        process.kill()
            except Exception as e:
                logger.warning(f"å…³é—­è¿›ç¨‹ {process_key} æ—¶å‡ºé”™: {e}")
        
        logger.info("GPUéš”ç¦»ç®¡ç†å™¨å·²å…³é—­")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•GPUéš”ç¦»
    manager = GPUIsolationManager()
    
    # åˆ›å»ºGPUè¿›ç¨‹
    gpu_ids = ["0", "1", "2", "3"]
    model_path = "/home/shuzuan/prj/models/flux1-dev"
    
    for gpu_id in gpu_ids:
        manager.create_gpu_process(gpu_id, model_path, "flux1-dev")
    
    # æäº¤æµ‹è¯•ä»»åŠ¡
    test_task = {
        "task_id": "test_001",
        "prompt": "A beautiful landscape",
        "height": 1024,
        "width": 1024,
        "seed": 42
    }
    
    result = manager.submit_task("0", "flux1-dev", test_task)
    print(f"æµ‹è¯•ç»“æœ: {result}")
    
    # å…³é—­ç®¡ç†å™¨
    manager.shutdown() 
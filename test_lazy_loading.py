#!/usr/bin/env python3
"""
æ‡’åŠ è½½å’ŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢æµ‹è¯•è„šæœ¬
éªŒè¯æ¨¡å‹æ˜¯å¦æŒ‰éœ€åŠ è½½å’ŒåŠ¨æ€åˆ‡æ¢
"""

import asyncio
import aiohttp
import time
import json
import random
from typing import List, Dict, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_lazy_loading_and_model_switching(base_url: str = "http://localhost:12411"):
    """æµ‹è¯•æ‡’åŠ è½½å’ŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢"""
    logger.info("ğŸ§ª å¼€å§‹æ‡’åŠ è½½å’ŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢æµ‹è¯•")
    logger.info(f"   æœåŠ¡åœ°å€: {base_url}")
    
    # æµ‹è¯•é…ç½®
    test_cases = [
        {
            "name": "é¦–æ¬¡è¯·æ±‚ - flux1-dev",
            "model_id": "flux1-dev",
            "prompt": "A beautiful landscape with mountains, first request",
            "expected_gpu": None  # é¦–æ¬¡è¯·æ±‚ï¼Œä¸çŸ¥é“ä¼šåˆ†é…åˆ°å“ªä¸ªGPU
        },
        {
            "name": "ç›¸åŒæ¨¡å‹ - flux1-dev",
            "model_id": "flux1-dev", 
            "prompt": "A futuristic city, same model request",
            "expected_gpu": None  # åº”è¯¥å¤ç”¨å·²åŠ è½½çš„æ¨¡å‹
        },
        {
            "name": "ä¸åŒæ¨¡å‹ - flux1-depth-dev",
            "model_id": "flux1-depth-dev",
            "prompt": "A portrait of a person, different model request",
            "expected_gpu": None  # åº”è¯¥å¸è½½æ—§æ¨¡å‹ï¼ŒåŠ è½½æ–°æ¨¡å‹
        },
        {
            "name": "å›åˆ°åŸæ¨¡å‹ - flux1-dev",
            "model_id": "flux1-dev",
            "prompt": "A cute cat, back to original model",
            "expected_gpu": None  # åº”è¯¥å†æ¬¡å¸è½½å’ŒåŠ è½½
        }
    ]
    
    async with aiohttp.ClientSession() as session:
        for i, test_case in enumerate(test_cases):
            logger.info(f"\n{'='*60}")
            logger.info(f"æµ‹è¯• {i+1}/{len(test_cases)}: {test_case['name']}")
            logger.info(f"{'='*60}")
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "prompt": test_case["prompt"],
                "model_id": test_case["model_id"],
                "height": 512,
                "width": 512,
                "num_inference_steps": 10,  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«æµ‹è¯•
                "cfg": 3.5,
                "seed": random.randint(1, 1000000),
                "priority": 0,
                "mode": "text2img"
            }
            
            # å‘é€è¯·æ±‚
            start_time = time.time()
            try:
                async with session.post(f"{base_url}/generate", json=data) as response:
                    response_time = time.time() - start_time
                    response_text = await response.text()
                    
                    if response.status == 200:
                        try:
                            response_json = await response.json()
                            task_id = response_json.get("task_id", "")
                            gpu_id = response_json.get("gpu_id", "")
                            model_id = response_json.get("model_id", "")
                            success = response_json.get("success", False)
                            
                            logger.info(f"âœ… è¯·æ±‚æˆåŠŸ")
                            logger.info(f"   ä»»åŠ¡ID: {task_id[:8]}")
                            logger.info(f"   ç‰©ç†GPU: {gpu_id}")
                            logger.info(f"   æ¨¡å‹ID: {model_id}")
                            logger.info(f"   å“åº”æ—¶é—´: {response_time:.2f}s")
                            logger.info(f"   æˆåŠŸçŠ¶æ€: {success}")
                            
                            # åˆ†æå“åº”æ—¶é—´
                            if i == 0:
                                logger.info(f"   ğŸ“Š é¦–æ¬¡åŠ è½½æ—¶é—´: {response_time:.2f}s (åŒ…å«æ¨¡å‹åŠ è½½)")
                            else:
                                logger.info(f"   ğŸ“Š åç»­è¯·æ±‚æ—¶é—´: {response_time:.2f}s")
                            
                        except Exception as e:
                            logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
                    else:
                        logger.error(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        logger.error(f"   å“åº”å†…å®¹: {response_text}")
                        
            except Exception as e:
                logger.error(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
            if i < len(test_cases) - 1:
                logger.info("â³ ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                await asyncio.sleep(5)

async def test_concurrent_model_switching(base_url: str = "http://localhost:12411"):
    """æµ‹è¯•å¹¶å‘æ¨¡å‹åˆ‡æ¢"""
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ§ª å¼€å§‹å¹¶å‘æ¨¡å‹åˆ‡æ¢æµ‹è¯•")
    logger.info(f"{'='*60}")
    
    # å¹¶å‘è¯·æ±‚ä¸åŒæ¨¡å‹
    models = ["flux1-dev", "flux1-depth-dev", "flux1-fill-dev"]
    prompts = [
        "A beautiful sunset",
        "A portrait of a person", 
        "A futuristic city"
    ]
    
    async def single_request(session: aiohttp.ClientSession, model_id: str, prompt: str, request_id: int):
        """å•ä¸ªè¯·æ±‚å‡½æ•°"""
        data = {
            "prompt": f"{prompt}, request {request_id}",
            "model_id": model_id,
            "height": 512,
            "width": 512,
            "num_inference_steps": 10,
            "cfg": 3.5,
            "seed": random.randint(1, 1000000),
            "priority": 0,
            "mode": "text2img"
        }
        
        start_time = time.time()
        try:
            async with session.post(f"{base_url}/generate", json=data) as response:
                response_time = time.time() - start_time
                
                if response.status == 200:
                    response_json = await response.json()
                    gpu_id = response_json.get("gpu_id", "")
                    success = response_json.get("success", False)
                    
                    logger.info(f"âœ… å¹¶å‘è¯·æ±‚ {request_id} æˆåŠŸ (æ¨¡å‹: {model_id}, GPU: {gpu_id}, è€—æ—¶: {response_time:.2f}s)")
                    return {
                        "success": True,
                        "model_id": model_id,
                        "gpu_id": gpu_id,
                        "response_time": response_time
                    }
                else:
                    logger.error(f"âŒ å¹¶å‘è¯·æ±‚ {request_id} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return {"success": False, "error": f"HTTP {response.status}"}
                    
        except Exception as e:
            logger.error(f"âŒ å¹¶å‘è¯·æ±‚ {request_id} å¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(6):  # 6ä¸ªå¹¶å‘è¯·æ±‚
            model_id = models[i % len(models)]
            prompt = prompts[i % len(prompts)]
            task = single_request(session, model_id, prompt, i + 1)
            tasks.append(task)
        
        # åŒæ—¶å‘é€æ‰€æœ‰è¯·æ±‚
        logger.info(f"ğŸ“¤ åŒæ—¶å‘é€ {len(tasks)} ä¸ªå¹¶å‘è¯·æ±‚...")
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        failed_results = [r for r in results if isinstance(r, dict) and not r.get("success", False)]
        exception_results = [r for r in results if isinstance(r, Exception)]
        
        logger.info(f"\nğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
        logger.info(f"   æ€»è¯·æ±‚æ•°: {len(results)}")
        logger.info(f"   æˆåŠŸè¯·æ±‚: {len(successful_results)}")
        logger.info(f"   å¤±è´¥è¯·æ±‚: {len(failed_results)}")
        logger.info(f"   å¼‚å¸¸è¯·æ±‚: {len(exception_results)}")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        
        # åˆ†æGPUä½¿ç”¨æƒ…å†µ
        gpu_usage = {}
        for result in successful_results:
            gpu_id = result.get("gpu_id", "unknown")
            gpu_usage[gpu_id] = gpu_usage.get(gpu_id, 0) + 1
        
        logger.info(f"\nç‰©ç†GPUä½¿ç”¨æƒ…å†µ:")
        for gpu_id, count in sorted(gpu_usage.items()):
            logger.info(f"   ç‰©ç†GPU {gpu_id}: {count} ä¸ªè¯·æ±‚")

async def main():
    """ä¸»å‡½æ•°"""
    BASE_URL = "http://localhost:12411"
    
    try:
        # æµ‹è¯•æ‡’åŠ è½½å’Œæ¨¡å‹åˆ‡æ¢
        await test_lazy_loading_and_model_switching(BASE_URL)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        logger.info("\nâ³ ç­‰å¾…10ç§’åè¿›è¡Œå¹¶å‘æµ‹è¯•...")
        await asyncio.sleep(10)
        
        # æµ‹è¯•å¹¶å‘æ¨¡å‹åˆ‡æ¢
        await test_concurrent_model_switching(BASE_URL)
        
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        logger.info(f"{'='*60}")
        
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main()) 
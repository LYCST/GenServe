#!/usr/bin/env python3
"""
å¹¶è¡Œæ€§èƒ½ä¼˜åŒ–è„šæœ¬
åˆ†æå½“å‰å¹¶è¡Œæ•ˆæœå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import asyncio
import aiohttp
import time
import json
import random
import statistics
from typing import List, Dict, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ParallelPerformanceOptimizer:
    """å¹¶è¡Œæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:12411"):
        self.base_url = base_url
        self.results = []
    
    async def test_parallel_performance(self, num_requests: int = 20, batch_size: int = 5):
        """æµ‹è¯•å¹¶è¡Œæ€§èƒ½"""
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ€§èƒ½æµ‹è¯•: {num_requests}ä¸ªè¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # ç”Ÿæˆæµ‹è¯•æç¤ºè¯
        prompts = [
            "A beautiful sunset over mountains, digital art",
            "A futuristic city with flying cars, sci-fi style",
            "A peaceful forest with ancient trees, fantasy art",
            "A steampunk mechanical robot, detailed illustration",
            "A magical crystal cave with glowing crystals",
            "A cyberpunk street scene with neon lights",
            "A medieval castle on a hill, fantasy landscape",
            "A space station orbiting Earth, sci-fi scene",
            "A underwater city with mermaids, fantasy art",
            "A desert oasis with palm trees, realistic style"
        ]
        
        # åˆ†æ‰¹å‘é€è¯·æ±‚
        all_tasks = []
        start_time = time.time()
        
        for i in range(0, num_requests, batch_size):
            batch_end = min(i + batch_size, num_requests)
            logger.info(f"ğŸ“¤ å‘é€æ‰¹æ¬¡ {i//batch_size + 1}: è¯·æ±‚ {i+1}-{batch_end}")
            
            # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
            batch_tasks = []
            for j in range(i, batch_end):
                prompt = prompts[j % len(prompts)]
                task = self._create_single_request(j + 1, prompt)
                batch_tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            all_tasks.extend(batch_results)
            
            # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…è¿‡è½½
            if batch_end < num_requests:
                await asyncio.sleep(0.5)
        
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        self._analyze_performance(all_tasks, total_time)
    
    async def _create_single_request(self, request_id: int, prompt: str) -> Dict[str, Any]:
        """åˆ›å»ºå•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        try:
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "prompt": f"{prompt}, request {request_id}",
                "model_id": "flux1-dev",
                "height": 512,
                "width": 512,
                "num_inference_steps": 20,  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«æµ‹è¯•
                "cfg": 3.5,
                "seed": random.randint(1, 1000000),
                "priority": random.randint(0, 2),  # éšæœºä¼˜å…ˆçº§
                "mode": "text2img"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(f"{self.base_url}/generate", json=data) as response:
                    response_time = time.time() - start_time
                    
                    result = {
                        "request_id": request_id,
                        "status_code": response.status,
                        "response_time": response_time,
                        "success": response.status == 200,
                        "timestamp": start_time
                    }
                    
                    if response.status == 200:
                        try:
                            response_json = await response.json()
                            result["task_id"] = response_json.get("task_id", "")
                            result["gpu_id"] = response_json.get("gpu_id", "")
                            result["model_id"] = response_json.get("model_id", "")
                            result["elapsed_time"] = response_json.get("elapsed_time", 0)
                            logger.info(f"è¯·æ±‚ {request_id}: âœ… æˆåŠŸï¼ŒGPU: {result['gpu_id']}, è€—æ—¶: {response_time:.2f}s")
                        except:
                            logger.warning(f"è¯·æ±‚ {request_id}: å“åº”è§£æå¤±è´¥")
                    else:
                        logger.error(f"è¯·æ±‚ {request_id}: âŒ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    
                    return result
                    
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"è¯·æ±‚ {request_id}: âŒ å¼‚å¸¸: {e}")
            return {
                "request_id": request_id,
                "status_code": 0,
                "response_time": response_time,
                "success": False,
                "error": str(e),
                "timestamp": start_time
            }
    
    def _analyze_performance(self, results: List[Dict[str, Any]], total_time: float):
        """åˆ†ææ€§èƒ½ç»“æœ"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š å¹¶è¡Œæ€§èƒ½åˆ†ææŠ¥å‘Š")
        logger.info("=" * 80)
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        failed_results = [r for r in results if isinstance(r, dict) and not r.get("success", False)]
        exception_results = [r for r in results if isinstance(r, Exception)]
        
        logger.info(f"æ€»è¯·æ±‚æ•°: {len(results)}")
        logger.info(f"æˆåŠŸè¯·æ±‚: {len(valid_results)} ({len(valid_results)/len(results)*100:.1f}%)")
        logger.info(f"å¤±è´¥è¯·æ±‚: {len(failed_results)} ({len(failed_results)/len(results)*100:.1f}%)")
        logger.info(f"å¼‚å¸¸è¯·æ±‚: {len(exception_results)} ({len(exception_results)/len(results)*100:.1f}%)")
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}s")
        
        if valid_results:
            # å“åº”æ—¶é—´åˆ†æ
            response_times = [r["response_time"] for r in valid_results]
            elapsed_times = [r.get("elapsed_time", 0) for r in valid_results if r.get("elapsed_time")]
            
            logger.info(f"\nâ±ï¸ å“åº”æ—¶é—´åˆ†æ:")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.2f}s")
            logger.info(f"  ä¸­ä½æ•°å“åº”æ—¶é—´: {statistics.median(response_times):.2f}s")
            logger.info(f"  æœ€å¿«å“åº”æ—¶é—´: {min(response_times):.2f}s")
            logger.info(f"  æœ€æ…¢å“åº”æ—¶é—´: {max(response_times):.2f}s")
            logger.info(f"  å“åº”æ—¶é—´æ ‡å‡†å·®: {statistics.stdev(response_times):.2f}s")
            
            if elapsed_times:
                logger.info(f"\nâš¡ å®é™…ç”Ÿæˆæ—¶é—´åˆ†æ:")
                logger.info(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {statistics.mean(elapsed_times):.2f}s")
                logger.info(f"  ä¸­ä½æ•°ç”Ÿæˆæ—¶é—´: {statistics.median(elapsed_times):.2f}s")
                logger.info(f"  æœ€å¿«ç”Ÿæˆæ—¶é—´: {min(elapsed_times):.2f}s")
                logger.info(f"  æœ€æ…¢ç”Ÿæˆæ—¶é—´: {max(elapsed_times):.2f}s")
            
            # GPUä½¿ç”¨åˆ†æ
            gpu_usage = {}
            for r in valid_results:
                gpu_id = r.get("gpu_id", "unknown")
                gpu_usage[gpu_id] = gpu_usage.get(gpu_id, 0) + 1
            
            logger.info(f"\nğŸ® GPUä½¿ç”¨æƒ…å†µ:")
            for gpu_id, count in sorted(gpu_usage.items()):
                percentage = count / len(valid_results) * 100
                logger.info(f"  GPU {gpu_id}: {count} ä¸ªè¯·æ±‚ ({percentage:.1f}%)")
            
            # è´Ÿè½½å‡è¡¡è¯„ä¼°
            if len(gpu_usage) > 1:
                counts = list(gpu_usage.values())
                mean_count = statistics.mean(counts)
                variance = statistics.variance(counts)
                balance_score = 1.0 / (1.0 + variance / mean_count) if mean_count > 0 else 0
                
                logger.info(f"\nâš–ï¸ è´Ÿè½½å‡è¡¡è¯„ä¼°:")
                logger.info(f"  å‡è¡¡åº¦è¯„åˆ†: {balance_score:.3f} (1.0ä¸ºå®Œç¾å‡è¡¡)")
                if balance_score > 0.8:
                    logger.info(f"  âœ… è´Ÿè½½å‡è¡¡æ•ˆæœä¼˜ç§€")
                elif balance_score > 0.6:
                    logger.info(f"  âš ï¸ è´Ÿè½½å‡è¡¡æ•ˆæœè‰¯å¥½")
                else:
                    logger.info(f"  âŒ è´Ÿè½½å‡è¡¡æ•ˆæœéœ€è¦æ”¹è¿›")
            
            # å¹¶å‘æ•ˆç‡è¯„ä¼°
            avg_response_time = statistics.mean(response_times)
            concurrent_efficiency = total_time / avg_response_time if avg_response_time > 0 else 0
            
            logger.info(f"\nğŸš€ å¹¶å‘æ•ˆç‡è¯„ä¼°:")
            logger.info(f"  å¹¶å‘æ•ˆç‡: {concurrent_efficiency:.2f} (ç†æƒ³å€¼æ¥è¿‘è¯·æ±‚æ•°)")
            logger.info(f"  ç†è®ºæœ€å¤§å¹¶å‘: {len(valid_results)}")
            logger.info(f"  å®é™…å¹¶å‘åº¦: {concurrent_efficiency:.1f}")
            
            if concurrent_efficiency > len(valid_results) * 0.8:
                logger.info(f"  âœ… å¹¶å‘æ•ˆç‡ä¼˜ç§€")
            elif concurrent_efficiency > len(valid_results) * 0.6:
                logger.info(f"  âš ï¸ å¹¶å‘æ•ˆç‡è‰¯å¥½")
            else:
                logger.info(f"  âŒ å¹¶å‘æ•ˆç‡éœ€è¦æ”¹è¿›")
        
        # ä¼˜åŒ–å»ºè®®
        self._provide_optimization_suggestions(valid_results, total_time)
    
    def _provide_optimization_suggestions(self, valid_results: List[Dict[str, Any]], total_time: float):
        """æä¾›ä¼˜åŒ–å»ºè®®"""
        logger.info(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        if not valid_results:
            logger.info("  âŒ æ²¡æœ‰æˆåŠŸè¯·æ±‚ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
            return
        
        # åˆ†æå“åº”æ—¶é—´åˆ†å¸ƒ
        response_times = [r["response_time"] for r in valid_results]
        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        
        # æ£€æŸ¥å“åº”æ—¶é—´å·®å¼‚
        time_variance = statistics.variance(response_times)
        if time_variance > avg_response_time * 0.5:
            logger.info("  ğŸ”§ å“åº”æ—¶é—´å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®:")
            logger.info("    - æ£€æŸ¥GPUè´Ÿè½½å‡è¡¡ç®—æ³•")
            logger.info("    - ä¼˜åŒ–ä»»åŠ¡è°ƒåº¦ç­–ç•¥")
            logger.info("    - è€ƒè™‘GPUæ€§èƒ½å·®å¼‚")
        
        # æ£€æŸ¥å¹¶å‘åº¦
        concurrent_efficiency = total_time / avg_response_time if avg_response_time > 0 else 0
        if concurrent_efficiency < len(valid_results) * 0.7:
            logger.info("  ğŸ”§ å¹¶å‘åº¦è¾ƒä½ï¼Œå»ºè®®:")
            logger.info("    - å¢åŠ GPUæ•°é‡")
            logger.info("    - ä¼˜åŒ–é˜Ÿåˆ—ç®¡ç†")
            logger.info("    - å‡å°‘ä»»åŠ¡å¤„ç†æ—¶é—´")
        
        # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
        gpu_usage = {}
        for r in valid_results:
            gpu_id = r.get("gpu_id", "unknown")
            gpu_usage[gpu_id] = gpu_usage.get(gpu_id, 0) + 1
        
        if len(gpu_usage) > 1:
            counts = list(gpu_usage.values())
            max_count = max(counts)
            min_count = min(counts)
            if max_count > min_count * 2:
                logger.info("  ğŸ”§ GPUä½¿ç”¨ä¸å‡è¡¡ï¼Œå»ºè®®:")
                logger.info("    - æ”¹è¿›è´Ÿè½½å‡è¡¡ç®—æ³•")
                logger.info("    - æ£€æŸ¥GPUæ€§èƒ½å·®å¼‚")
                logger.info("    - è°ƒæ•´ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # æ€»ä½“å»ºè®®
        logger.info("  ğŸ“ˆ æ€»ä½“ä¼˜åŒ–æ–¹å‘:")
        logger.info("    - ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
        logger.info("    - ä¼˜åŒ–æ¨¡å‹åŠ è½½å’Œæ¨ç†é€Ÿåº¦")
        logger.info("    - è°ƒæ•´é˜Ÿåˆ—å¤§å°å’Œè¶…æ—¶è®¾ç½®")
        logger.info("    - è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨è®¾å¤‡")

async def main():
    """ä¸»å‡½æ•°"""
    optimizer = ParallelPerformanceOptimizer()
    
    # æµ‹è¯•ä¸åŒè§„æ¨¡çš„å¹¶å‘
    test_scenarios = [
        (10, 5),   # 10ä¸ªè¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°5
        (20, 5),   # 20ä¸ªè¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°5
        (30, 10),  # 30ä¸ªè¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°10
    ]
    
    for num_requests, batch_size in test_scenarios:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ§ª æµ‹è¯•åœºæ™¯: {num_requests}ä¸ªè¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°{batch_size}")
        logger.info(f"{'='*60}")
        
        await optimizer.test_parallel_performance(num_requests, batch_size)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
        if num_requests != test_scenarios[-1][0]:
            logger.info("â³ ç­‰å¾…30ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
            await asyncio.sleep(30)

if __name__ == "__main__":
    asyncio.run(main()) 